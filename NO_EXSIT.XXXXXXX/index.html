<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/7.0.0/css/all.min.css" integrity="sha256-VHqXKFhhMxcpubYf9xiWdCiojEbY9NexQ4jh8AxbvcM=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"e-x.top","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.27.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":false,"codeblock":{"theme":{"light":"default","dark":"stackoverflow-dark"},"prism":{"light":"prism","dark":"prism-dark"},"copy_button":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"language":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js" defer></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="Sky&#39;s Blogs">
<meta property="og:url" content="https://e-x.top/NO_EXSIT.XXXXXXX/index.html">
<meta property="og:site_name" content="Sky&#39;s Blogs">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Sky">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://e-x.top/NO_EXSIT.XXXXXXX/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"NO_EXSIT.XXXXXXX/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Sky's Blogs</title>
  








  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
<script src="/js/utils.js" defer></script><script src="/js/motion.js" defer></script><script src="/js/sidebar.js" defer></script><script src="/js/next-boot.js" defer></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.5.0/search.js" integrity="sha256-xFC6PJ82SL9b3WkGjFavNiA9gm5z6UBxWPiu4CYjptg=" crossorigin="anonymous" defer></script>
<script src="/js/third-party/search/local-search.js" defer></script>







  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8677552300382028"
     crossorigin="anonymous"></script>


  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Sky's Blogs</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">A normal star</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>
<div class="custom-middle-nav animated fadeInDown">
  <div class="custom-nav-item">
    <a href="/archives/"><i class="fa fa-archive"></i> 日志历程</a>
  </div>
  <div class="custom-nav-item">
    <a href="/categories/"><i class="fa fa-th"></i> 全部分类</a>
  </div>
  <div class="custom-nav-item">
    <a href="/tags/"><i class="fa fa-tags"></i> 热门标签</a>
  </div>
  <div class="custom-nav-item">
    <a href="javascript:;" class="my-search-btn"><i class="fa fa-search"></i> 本站搜索</a>
  </div>
</div>


<script>
// 使用脚本手动触发 NexT 的搜索弹窗
document.addEventListener('DOMContentLoaded', () => {
  const customBtn = document.querySelector('.my-search-btn');
  customBtn.addEventListener('click', (e) => {
    e.preventDefault();
    // 找到 NexT 原生的那个搜索触发按钮并模拟点击
    const originalBtn = document.querySelector('.site-nav-right .popup-trigger');
    if (originalBtn) {
      originalBtn.click();
    } else {
      // 如果原生的没找到，尝试直接调用 NexT 的 Search 内部变量（针对某些版本）
      if (typeof CONFIG.local_search !== 'undefined') {
        // 尝试手动激活弹窗
        document.querySelector('.search-pop-overlay').style.display = 'block';
        document.body.style.overflow = 'hidden';
        document.querySelector('.search-input').focus();
      }
    }
  });
});
</script>
</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Sky"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Sky</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">139</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">92</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">222</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/flyinskyin2013" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;flyinskyin2013" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>
<div id="days" style="margin-top: 10px; font-size: 13px; color: #555;"></div>
<script>
  function show_date_time(){
    window.setTimeout("show_date_time()", 1000);
    BirthDay=new Date("01/07/2014 00:00:00"); // 这里改成你建站的时间
    today=new Date();
    timeold=(today.getTime()-BirthDay.getTime());
    sectimeold=timeold/1000
    secondsold=Math.floor(sectimeold);
    msPerDay=24*60*60*1000
    e_daysold=timeold/msPerDay
    daysold=Math.floor(e_daysold);
    document.getElementById("days").innerHTML="本站已运行 "+daysold+" 天";
  }
  show_date_time();
</script>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://e-x.top/2025/12/28/blog_idx_146/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Sky">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sky's Blogs">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | Sky's Blogs">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/12/28/blog_idx_146/" class="post-title-link" itemprop="url">一次酣畅淋漓的问题排查（c++标准库异常实现原理）</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-12-28 17:24:00 / 修改时间：17:28:32" itemprop="dateCreated datePublished" datetime="2025-12-28T17:24:00+08:00">2025-12-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Linux/" itemprop="url" rel="index"><span itemprop="name">Linux</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Linux/Android/" itemprop="url" rel="index"><span itemprop="name">Android</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Linux/Android/C/" itemprop="url" rel="index"><span itemprop="name">C++</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Linux/Android/C/Exception/" itemprop="url" rel="index"><span itemprop="name">Exception</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>6.3k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>23 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <script src="\assets\js\APlayer.min.js"> </script><!--
 * @Description: 
 * @Author: Sky
 * @Date: 2020-08-24 16:37:34
 * @LastEditors: Sky
 * @LastEditTime: 2021-06-29 15:04:48
 * @Github: https://github.com/flyinskyin2013/
-->
<p><font color="red" size="7">PS：要转载请注明出处，本人版权所有。</font><br/><br>
<font color="red" size="7">PS: 这个只是基于《我自己》的理解，</font><br/><font color="red" size="7">如果和你的原则及想法相冲突，请谅解，勿喷。</font><br/></p>
<!-- ###### 前置说明
&emsp;&emsp;本文作为本人github blog/cnblog的主站的备份。（BlogID=146） 
&emsp;&emsp;本文发布于 2025-12-28 17:24:00             （BlogID=146） 
-->
<h6 id="环境说明">环境说明</h6>
<p>  无</p>
<h3 id="前言">前言</h3>
<hr>
<p>  在集成和定制llama.cpp工程的时候，做了许多工作，也遇到了很多问题，但是绝大部分问题都是很快就能解决的，少部分问题花一些时间也能解决掉，其中有两个关联问题是让我最印象深刻的。为了整理和探究这两个问题的根源，特在此编写本文。且在写本文这段时间内，也整理和提了一个关联的pr给llama.cpp（<a target="_blank" rel="noopener" href="https://github.com/ggml-org/llama.cpp/pull/17653%EF%BC%89%E3%80%82">https://github.com/ggml-org/llama.cpp/pull/17653）。</a></p>
<p>  首先我们有如下的代码示例：</p>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">try&#123;
    &#123;
        &#x2F;&#x2F; ... ...
        if (!std::filesystem::exists(&quot;&#x2F;bugreports&quot;))

        &#x2F;&#x2F; ... ...
    &#125;

    &#123;
        std::filesystem::directory_iterator dir_it(&quot;&#x2F;&quot;, fs::directory_options::skip_permission_denied);
        for (const auto &amp; entry : dir_it) &#123;
            &#x2F;&#x2F; ... ...
        &#125;
        &#x2F;&#x2F; ... ...
    &#125;

    return ;
&#125;
catch (const std::exception&amp; e)&#123;
    printf(&quot;exception: %s\n&quot;, e.what());
    return ;
&#125;
catch(...)&#123;
    printf(&quot;Fatal Error, Unkown exception\n&quot;);
    return ;
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>  根据上面的代码示例，在不同的编译条件、同一个执行环境(软、硬件)下它3个code-block分支都会走，这让我简直头大。下面是两个catch-code-block部分的输出：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">exception: filesystem error: <span class="token keyword">in</span> posix_stat: failed to determine attributes <span class="token keyword">for</span> the specified path: Permission denied <span class="token punctuation">[</span><span class="token string">"/bugreports"</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">Fatal Error, Unkown exception<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>  当然，上面的3个code-block其实对应这几个问题：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>为什么同一个设备，同一段代码在不同条件下执行3个不同的分支，尤其是什么情况下正常执行，什么情况下抛出异常？</p>
</li>
<li class="lvl-2">
<p>std::filesystem::exists/std::filesystem::directory_iterator 什么情况下会抛出异常？</p>
</li>
<li class="lvl-2">
<p>对于std::filesystem::exists/std::filesystem::directory_iterator抛出的异常来说，为什么捕获路径不一样（是否能抓到filesystem error）？</p>
</li>
</ul>
<p>  下面我们分别对这几个问题进行分析(以std::filesystem::exists为例)。</p>
<br/>
<br/>
<br/>
<br/>
<h3 id="问题初步分析">问题初步分析</h3>
<hr>
<p>  </p>
<br/>
<br/>
<h5 id="为什么同一设备，同一代码，不同编译条件可以正常或者异常运行">为什么同一设备，同一代码，不同编译条件可以正常或者异常运行?</h5>
<p>  在我的例子里面，根据我的实际测试反馈来看，在build.gradle里面，【 compileSdk = 34，minSdk = 34，ndk=26】【 compileSdk = 34，minSdk = 34，ndk=26】两种不同配置，导致运行结果不一样，当minSdk=26时，代码会抛出异常，当minSdk=34时，代码正常运行。</p>
<p>  经过上面的分析和测试，我们可以得到一个猜（可能性极大）的原因：因为ndk版本是一样的，意味着上面的标准库实现是一样的，因此这个现象的主要原因还是不同的编译条件，让我们使用posix api访问/bugreports目录时，posix api有不同的返回。</p>
<p>  更底层的原因导致posix api有不同的返回，我不是很了解、不熟悉android的底层系统细节，因此就不继续排查了，有缘再说，下次一定。</p>
<p>  接着我们排查一下c++标准库的std::filesystem::exists实现，看看异常从哪里来？</p>
<br/>
<br/>
<h5 id="什么情况下std-filesystem-exists会抛出异常？">什么情况下std::filesystem::exists会抛出异常？</h5>
<p>  我们先查看<a target="_blank" rel="noopener" href="https://en.cppreference.com/w/cpp/filesystem/exists.html%EF%BC%8C%E5%85%B6%E5%AE%9A%E4%B9%89%E5%A6%82%E4%B8%8B%EF%BC%9A">https://en.cppreference.com/w/cpp/filesystem/exists.html，其定义如下：</a></p>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">bool exists( std::filesystem::file_status s ) noexcept; (1)	(since C++17)
bool exists( const std::filesystem::path&amp; p ); (2)	(since C++17)
bool exists( const std::filesystem::path&amp; p, std::error_code&amp; ec ) noexcept; (3)	(since C++17)

&#x2F;*
    Exceptions
        Any overload not marked noexcept may throw std::bad_alloc if memory allocation fails.

        2) Throws std::filesystem::filesystem_error on underlying OS API errors, constructed with p as the first path argument and the OS error code as the error code argument.
*&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>  因此，对于我们上文的用法，如果底层OS的API出现问题，那么会抛出异常，这个现象是符合标准定义的。</p>
<p>   下面我们来看看exists的源码具体实现（libcxx）：</p>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">inline _LIBCPP_HIDE_FROM_ABI bool exists(const path&amp; __p) &#123; return exists(__status(__p)); &#125;

_LIBCPP_EXPORTED_FROM_ABI file_status __status(const path&amp;, error_code* __ec &#x3D; nullptr); 

file_status __status(const path&amp; p, error_code* ec) &#123; return detail::posix_stat(p, ec); &#125;


inline file_status posix_stat(path const&amp; p, error_code* ec) &#123;
  StatT path_stat;
  return posix_stat(p, path_stat, ec);
&#125;

inline file_status posix_stat(path const&amp; p, StatT&amp; path_stat, error_code* ec) &#123;
  error_code m_ec;
  if (detail::stat(p.c_str(), &amp;path_stat) &#x3D;&#x3D; -1)
    m_ec &#x3D; detail::capture_errno();
  return create_file_status(m_ec, p, path_stat, ec);
&#125;

namespace detail &#123;
using ::stat; &#x2F;&#x2F;&lt;sys&#x2F;stat.h&gt;
&#125; &#x2F;&#x2F; end namespace detail

inline file_status create_file_status(error_code&amp; m_ec, path const&amp; p, const StatT&amp; path_stat, error_code* ec) &#123;
  if (ec)
    *ec &#x3D; m_ec;
  if (m_ec &amp;&amp; (m_ec.value() &#x3D;&#x3D; ENOENT || m_ec.value() &#x3D;&#x3D; ENOTDIR)) &#123;
    return file_status(file_type::not_found);
  &#125; else if (m_ec) &#123;
    ErrorHandler&lt;void&gt; err(&quot;posix_stat&quot;, ec, &amp;p);
    err.report(m_ec, &quot;failed to determine attributes for the specified path&quot;);
    return file_status(file_type::none);
  &#125;

  &#x2F;&#x2F; ... ... other code
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>  因此exists()抛异常的根本原因就是，调用detail::stat的时候，产生了Permission denied 错误，然后在create_file_status中抛出了异常。</p>
<br/>
<br/>
<h5 id="对于std-filesystem-filesystem-error异常，在不同位置捕获的原因？">对于std::filesystem::filesystem_error异常，在不同位置捕获的原因？</h5>
<p>  根据上面的最小化测试代码，再一次对整体构建过程进行排查后，有如下发现：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>当上面的代码在一个so中，如果启用了-Wl,–version-script功能，导致未导出vtable和typeinfo对象的符号(Android)。</p>
</li>
<li class="lvl-2">
<p>在x86里面构建上面同样的实例时，发现启用了-Wl,–version-script功能，默认也能导出了vtable和typeinfo对象的符号。</p>
</li>
</ul>
<p>  上面的现象把我搞郁闷了，经过编译器、链接器、编译参数、链接参数和符号等相关的排查，终于在一个位置发现了一些奇怪的东西：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token comment">#  readelf -sW build/libnativelib.so|grep fs10filesystem16filesystem_errorE</span>
<span class="token comment"># 下面的so能在catch (const std::exception&amp; e)中捕获异常，nm -CD 也有fs10filesystem16filesystem_errorE相关的符号</span>
  <span class="token number">12</span>: 0000000000000000     <span class="token number">0</span> OBJECT  GLOBAL DEFAULT  UND _ZTINSt6__ndk14__fs10filesystem16filesystem_errorE
  <span class="token number">18</span>: 0000000000000000     <span class="token number">0</span> OBJECT  GLOBAL DEFAULT  UND _ZTVNSt6__ndk14__fs10filesystem16filesystem_errorE
 <span class="token number">235</span>: 0000000000000000     <span class="token number">0</span> OBJECT  GLOBAL DEFAULT  UND _ZTINSt6__ndk14__fs10filesystem16filesystem_errorE
 <span class="token number">241</span>: 0000000000000000     <span class="token number">0</span> OBJECT  GLOBAL DEFAULT  UND _ZTVNSt6__ndk14__fs10filesystem16filesystem_errorE

<span class="token comment"># 下面的so只能在catch(...)捕获异常，nm -CD 没有fs10filesystem16filesystem_errorE相关的符号</span>
 <span class="token number">393</span>: 0000000000036340    <span class="token number">24</span> OBJECT  LOCAL  DEFAULT   <span class="token number">17</span> _ZTINSt6__ndk14__fs10filesystem16filesystem_errorE
 <span class="token number">395</span>: 0000000000036318    <span class="token number">40</span> OBJECT  LOCAL  DEFAULT   <span class="token number">17</span> _ZTVNSt6__ndk14__fs10filesystem16filesystem_errorE
 <span class="token number">410</span>: 000000000000ad5a    <span class="token number">47</span> OBJECT  LOCAL  DEFAULT   <span class="token number">11</span> _ZTSNSt6__ndk14__fs10filesystem16filesystem_errorE<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>  上面我们可以知道，正常的so，其相关的typeinfo/vtable是GLOBAL 且未定义的，其定义应该在libc++.so或者libstdc++.so的。而异常的so相关的typeinfo/vtable的符号是LOCAL且已经定义了。</p>
<p>  经过一系列查询，上面问题的差异出在ANDROID_STL在cmake中默认是c++_static的(<a target="_blank" rel="noopener" href="https://developer.android.com/ndk/guides/cpp-support?hl=zh-cn#selecting_a_c_runtime">https://developer.android.com/ndk/guides/cpp-support?hl=zh-cn#selecting_a_c_runtime</a>)，这个时候c<ins>标准库的实现是以静态库的方式链接到我的so，因此相关的实现是local的，现在只需要改为c</ins>_shared就解决了上面的异常路径不一致的情况。</p>
<p>  此外，当我还是用c++_static继续编译，只是手动把typeinfo/vtable的符号都导出为依赖libc++.so或者libstdc++.so时，发现也能够正常捕获异常了。</p>
<p>  上面我们只是找到了引起问题的地方，但是没有回答，为什么nm -CD 没有fs10filesystem16filesystem_errorE相关的typeinfo/vtable符号的时候，只有catch(…)能捕获异常。要回答这个问题，我们得去初步看一下c++异常机制是怎么实现的，下面我们继续分析。</p>
<br/>
<br/>
<br/>
<br/>
<h3 id="c-标准库异常实现原理简单分析">c++标准库异常实现原理简单分析</h3>
<p>  为了尽可能的贴近我的遇到问题的场景和方便调试，且不同ABI的异常实现可能不一致，下面基于clang，x64，来分析c<ins>异常实现的基本原理（Itanium C</ins> ABI）。</p>
<p>  首先我们来看看我们throw一个异常的时候调用的汇编代码是什么？</p>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">extern &quot;C&quot; __attribute__((visibility(&quot;default&quot;))) void pp()
&#123;
  throw std::runtime_error(&quot;test_exception&quot;);
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">   0x00007ffff7f9a380 <span class="token operator">&lt;</span>+<span class="token operator"><span class="token file-descriptor important">0</span>></span>:     push   %rbp
   0x00007ffff7f9a381 <span class="token operator">&lt;</span>+<span class="token operator"><span class="token file-descriptor important">1</span>></span>:     mov    %rsp,%rbp
   0x00007ffff7f9a384 <span class="token operator">&lt;</span>+<span class="token operator"><span class="token file-descriptor important">4</span>></span>:     sub    <span class="token variable">$0x20</span>,%rsp
   0x00007ffff7f9a388 <span class="token operator">&lt;</span>+<span class="token operator"><span class="token file-descriptor important">8</span>></span>:     mov    <span class="token variable">$0x10</span>,%edi
<span class="token operator">=</span><span class="token operator">></span> 0x00007ffff7f9a38d <span class="token operator">&lt;</span>+1<span class="token operator"><span class="token file-descriptor important">3</span>></span>:    call   0x7ffff7fb48e0 <span class="token operator">&lt;</span>__cxa_allocate_exception<span class="token operator">></span>
   0x00007ffff7f9a392 <span class="token operator">&lt;</span>+1<span class="token operator"><span class="token file-descriptor important">8</span>></span>:    mov    %rax,%rdi
   0x00007ffff7f9a395 <span class="token operator">&lt;</span>+2<span class="token operator"><span class="token file-descriptor important">1</span>></span>:    mov    %rdi,%rax
   0x00007ffff7f9a398 <span class="token operator">&lt;</span>+2<span class="token operator"><span class="token file-descriptor important">4</span>></span>:    mov    %rax,-0x18<span class="token punctuation">(</span>%rbp<span class="token punctuation">)</span>
   0x00007ffff7f9a39c <span class="token operator">&lt;</span>+2<span class="token operator"><span class="token file-descriptor important">8</span>></span>:    lea    -0x902d<span class="token punctuation">(</span>%rip<span class="token punctuation">)</span>,%rsi        <span class="token comment"># 0x7ffff7f91376</span>
   0x00007ffff7f9a3a3 <span class="token operator">&lt;</span>+3<span class="token operator"><span class="token file-descriptor important">5</span>></span>:    call   0x7ffff7fb5e80 <span class="token operator">&lt;</span>_ZNSt13runtime_errorC2EPKc<span class="token operator">></span>
   0x00007ffff7f9a3a8 <span class="token operator">&lt;</span>+4<span class="token operator"><span class="token file-descriptor important">0</span>></span>:    jmp    0x7ffff7f9a3ad <span class="token operator">&lt;</span>pp<span class="token punctuation">(</span><span class="token punctuation">)</span>+4<span class="token operator"><span class="token file-descriptor important">5</span>></span>
   0x00007ffff7f9a3ad <span class="token operator">&lt;</span>+4<span class="token operator"><span class="token file-descriptor important">5</span>></span>:    mov    -0x18<span class="token punctuation">(</span>%rbp<span class="token punctuation">)</span>,%rdi
   0x00007ffff7f9a3b1 <span class="token operator">&lt;</span>+4<span class="token operator"><span class="token file-descriptor important">9</span>></span>:    lea    0x1d158<span class="token punctuation">(</span>%rip<span class="token punctuation">)</span>,%rsi        <span class="token comment"># 0x7ffff7fb7510 &lt;_ZTISt13runtime_error></span>
   0x00007ffff7f9a3b8 <span class="token operator">&lt;</span>+5<span class="token operator"><span class="token file-descriptor important">6</span>></span>:    lea    0xb1<span class="token punctuation">(</span>%rip<span class="token punctuation">)</span>,%rdx        <span class="token comment"># 0x7ffff7f9a470 &lt;_ZNSt15underflow_errorD2Ev></span>
   0x00007ffff7f9a3bf <span class="token operator">&lt;</span>+6<span class="token operator"><span class="token file-descriptor important">3</span>></span>:    call   0x7ffff7fb4b00 <span class="token operator">&lt;</span>__cxa_throw<span class="token operator">></span>
   0x00007ffff7f9a3c4 <span class="token operator">&lt;</span>+6<span class="token operator"><span class="token file-descriptor important">8</span>></span>:    mov    -0x18<span class="token punctuation">(</span>%rbp<span class="token punctuation">)</span>,%rdi
   0x00007ffff7f9a3c8 <span class="token operator">&lt;</span>+7<span class="token operator"><span class="token file-descriptor important">2</span>></span>:    mov    %rax,%rcx
   0x00007ffff7f9a3cb <span class="token operator">&lt;</span>+7<span class="token operator"><span class="token file-descriptor important">5</span>></span>:    mov    %edx,%eax
   0x00007ffff7f9a3cd <span class="token operator">&lt;</span>+7<span class="token operator"><span class="token file-descriptor important">7</span>></span>:    mov    %rcx,-0x8<span class="token punctuation">(</span>%rbp<span class="token punctuation">)</span>
   0x00007ffff7f9a3d1 <span class="token operator">&lt;</span>+8<span class="token operator"><span class="token file-descriptor important">1</span>></span>:    mov    %eax,-0xc<span class="token punctuation">(</span>%rbp<span class="token punctuation">)</span>
   0x00007ffff7f9a3d4 <span class="token operator">&lt;</span>+8<span class="token operator"><span class="token file-descriptor important">4</span>></span>:    call   0x7ffff7fb49c0 <span class="token operator">&lt;</span>__cxa_free_exception<span class="token operator">></span>
   0x00007ffff7f9a3d9 <span class="token operator">&lt;</span>+8<span class="token operator"><span class="token file-descriptor important">9</span>></span>:    mov    -0x8<span class="token punctuation">(</span>%rbp<span class="token punctuation">)</span>,%rdi
   0x00007ffff7f9a3dd <span class="token operator">&lt;</span>+9<span class="token operator"><span class="token file-descriptor important">3</span>></span>:    call   0x7ffff7fb6160 <span class="token operator">&lt;</span>_Unwind_Resume@plt<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>   从上面的代码可以知道，先调用__cxa_allocate_exception在特定空间分配内存（不是一般的堆栈空间，避免干扰堆栈），然后调用placement new 在前面的空间上面构造std::runtime_error对象，然后执行__cxa_throw开始堆栈展开，查找异常链。这个链接介绍了cpp标准里面对异常展开流程的描述（<a target="_blank" rel="noopener" href="https://en.cppreference.com/w/cpp/language/throw.html%EF%BC%89%E3%80%82">https://en.cppreference.com/w/cpp/language/throw.html）。</a></p>
<p>  下面我们通过查看__cxa_throw的源码，看看libc++对异常展开是怎么实现的。</p>
<p>libcxxabi\src\cxa_exception.cpp</p>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void
__cxa_throw(void *thrown_object, std::type_info *tinfo, void (_LIBCXXABI_DTOR_FUNC *dest)(void *)) &#123;
    __cxa_eh_globals *globals &#x3D; __cxa_get_globals();
    __cxa_exception* exception_header &#x3D; cxa_exception_from_thrown_object(thrown_object);

    exception_header-&gt;unexpectedHandler &#x3D; std::get_unexpected();
    exception_header-&gt;terminateHandler  &#x3D; std::get_terminate();
    exception_header-&gt;exceptionType &#x3D; tinfo;
    exception_header-&gt;exceptionDestructor &#x3D; dest;
    setOurExceptionClass(&amp;exception_header-&gt;unwindHeader);
    exception_header-&gt;referenceCount &#x3D; 1;  &#x2F;&#x2F; This is a newly allocated exception, no need for thread safety.
    globals-&gt;uncaughtExceptions +&#x3D; 1;   &#x2F;&#x2F; Not atomically, since globals are thread-local

    exception_header-&gt;unwindHeader.exception_cleanup &#x3D; exception_cleanup_func;

#if __has_feature(address_sanitizer)
    &#x2F;&#x2F; Inform the ASan runtime that now might be a good time to clean stuff up.
    __asan_handle_no_return();
#endif

#ifdef __USING_SJLJ_EXCEPTIONS__
    _Unwind_SjLj_RaiseException(&amp;exception_header-&gt;unwindHeader);
#else
    _Unwind_RaiseException(&amp;exception_header-&gt;unwindHeader);
#endif
    &#x2F;&#x2F;  This only happens when there is no handler, or some unexpected unwinding
    &#x2F;&#x2F;     error happens.
    failed_throw(exception_header);
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>  这里可以看到，首先函数3个参数分别是：刚刚的std::runtime_error对象，异常对象的typeinfo，std::runtime_error对应的析构函数。然后就开始根据不同的异常实现，开始展开堆栈。此外，这里有个地方可以值得注意：exceptionType 很明显就是我们本文的问题有关系，如果没有导出对应的typeinfo，很有可能在其他地方无法匹配这个异常。</p>
<p>  还有这里补充一个细节：现在常见的异常模型大概有3类，SJLJ（setjump-longjump），DWARF，SEH (Windows)，当前类linux用的异常模型是DWARF中的定义。</p>
<p>  根据上面的执行流，我们接着来看_Unwind_RaiseException的实现。<br>
libunwind\src\UnwindLevel1.c</p>
<pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token comment">/// Called by __cxa_throw.  Only returns if there is a fatal error.</span>
_LIBUNWIND_EXPORT _Unwind_Reason_Code
<span class="token function">_Unwind_RaiseException</span><span class="token punctuation">(</span>_Unwind_Exception <span class="token operator">*</span>exception_object<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token function">_LIBUNWIND_TRACE_API</span><span class="token punctuation">(</span><span class="token string">"_Unwind_RaiseException(ex_obj=%p)"</span><span class="token punctuation">,</span>
                       static_cast<span class="token operator">&lt;</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token operator">></span><span class="token punctuation">(</span>exception_object<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token class-name">unw_context_t</span> uc<span class="token punctuation">;</span>
  <span class="token class-name">unw_cursor_t</span> cursor<span class="token punctuation">;</span>
  <span class="token function">__unw_getcontext</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>uc<span class="token punctuation">)</span><span class="token punctuation">;</span>

  <span class="token comment">// This field for is for compatibility with GCC to say this isn't a forced</span>
  <span class="token comment">// unwind. EHABI #7.2</span>
  exception_object<span class="token operator">-></span>unwinder_cache<span class="token punctuation">.</span>reserved1 <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>

  <span class="token comment">// phase 1: the search phase</span>
  _Unwind_Reason_Code phase1 <span class="token operator">=</span> <span class="token function">unwind_phase1</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>uc<span class="token punctuation">,</span> <span class="token operator">&amp;</span>cursor<span class="token punctuation">,</span> exception_object<span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token keyword">if</span> <span class="token punctuation">(</span>phase1 <span class="token operator">!=</span> _URC_NO_REASON<span class="token punctuation">)</span>
    <span class="token keyword">return</span> phase1<span class="token punctuation">;</span>

  <span class="token comment">// phase 2: the clean up phase</span>
  <span class="token keyword">return</span> <span class="token function">unwind_phase2</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>uc<span class="token punctuation">,</span> <span class="token operator">&amp;</span>cursor<span class="token punctuation">,</span> exception_object<span class="token punctuation">,</span> false<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>  从这里来看，异常展开分为了两个阶段，phase1和phase2，从备注来看就是搜索、清理。下面我们先来看unwind_phase1的做了什么。</p>
<p>libunwind\src\UnwindLevel1.c</p>
<pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token keyword">static</span> _Unwind_Reason_Code
<span class="token function">unwind_phase1</span><span class="token punctuation">(</span><span class="token class-name">unw_context_t</span> <span class="token operator">*</span>uc<span class="token punctuation">,</span> <span class="token class-name">unw_cursor_t</span> <span class="token operator">*</span>cursor<span class="token punctuation">,</span> _Unwind_Exception <span class="token operator">*</span>exception_object<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token function">__unw_init_local</span><span class="token punctuation">(</span>cursor<span class="token punctuation">,</span> uc<span class="token punctuation">)</span><span class="token punctuation">;</span>

  <span class="token comment">// Walk each frame looking for a place to stop.</span>
  <span class="token keyword">while</span> <span class="token punctuation">(</span>true<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
    <span class="token comment">// Ask libunwind to get next frame (skip over first which is</span>
    <span class="token comment">// _Unwind_RaiseException).</span>
    <span class="token keyword">int</span> stepResult <span class="token operator">=</span> <span class="token function">__unw_step</span><span class="token punctuation">(</span>cursor<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">// ... ...</span>

    <span class="token comment">// See if frame has code to run (has personality routine).</span>
    <span class="token class-name">unw_proc_info_t</span> frameInfo<span class="token punctuation">;</span>
    <span class="token class-name">unw_word_t</span> sp<span class="token punctuation">;</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">__unw_get_proc_info</span><span class="token punctuation">(</span>cursor<span class="token punctuation">,</span> <span class="token operator">&amp;</span>frameInfo<span class="token punctuation">)</span> <span class="token operator">!=</span> UNW_ESUCCESS<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
        <span class="token comment">// ... ...</span>
    <span class="token punctuation">&#125;</span>

    <span class="token comment">// ... ...</span>

    <span class="token comment">// If there is a personality routine, ask it if it will want to stop at</span>
    <span class="token comment">// this frame.</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>frameInfo<span class="token punctuation">.</span>handler <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
      _Unwind_Personality_Fn p <span class="token operator">=</span>
          <span class="token punctuation">(</span>_Unwind_Personality_Fn<span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token class-name">uintptr_t</span><span class="token punctuation">)</span><span class="token punctuation">(</span>frameInfo<span class="token punctuation">.</span>handler<span class="token punctuation">)</span><span class="token punctuation">;</span>
      <span class="token function">_LIBUNWIND_TRACE_UNWINDING</span><span class="token punctuation">(</span>
          <span class="token string">"unwind_phase1(ex_ojb=%p): calling personality function %p"</span><span class="token punctuation">,</span>
          <span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token punctuation">)</span>exception_object<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token class-name">uintptr_t</span><span class="token punctuation">)</span>p<span class="token punctuation">)</span><span class="token punctuation">;</span>
      _Unwind_Reason_Code personalityResult <span class="token operator">=</span>
          <span class="token punctuation">(</span><span class="token operator">*</span>p<span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> _UA_SEARCH_PHASE<span class="token punctuation">,</span> exception_object<span class="token operator">-></span>exception_class<span class="token punctuation">,</span>
               exception_object<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token keyword">struct</span> <span class="token class-name">_Unwind_Context</span> <span class="token operator">*</span><span class="token punctuation">)</span><span class="token punctuation">(</span>cursor<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
      <span class="token keyword">switch</span> <span class="token punctuation">(</span>personalityResult<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
      <span class="token keyword">case</span> _URC_HANDLER_FOUND<span class="token operator">:</span>
        <span class="token comment">// found a catch clause or locals that need destructing in this frame</span>
        <span class="token comment">// stop search and remember stack pointer at the frame</span>
        <span class="token function">__unw_get_reg</span><span class="token punctuation">(</span>cursor<span class="token punctuation">,</span> UNW_REG_SP<span class="token punctuation">,</span> <span class="token operator">&amp;</span>sp<span class="token punctuation">)</span><span class="token punctuation">;</span>
        exception_object<span class="token operator">-></span>private_2 <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token class-name">uintptr_t</span><span class="token punctuation">)</span>sp<span class="token punctuation">;</span>
        <span class="token function">_LIBUNWIND_TRACE_UNWINDING</span><span class="token punctuation">(</span>
            <span class="token string">"unwind_phase1(ex_ojb=%p): _URC_HANDLER_FOUND"</span><span class="token punctuation">,</span>
            <span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token punctuation">)</span>exception_object<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">return</span> _URC_NO_REASON<span class="token punctuation">;</span>

      <span class="token keyword">case</span> _URC_CONTINUE_UNWIND<span class="token operator">:</span>
        <span class="token function">_LIBUNWIND_TRACE_UNWINDING</span><span class="token punctuation">(</span>
            <span class="token string">"unwind_phase1(ex_ojb=%p): _URC_CONTINUE_UNWIND"</span><span class="token punctuation">,</span>
            <span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token punctuation">)</span>exception_object<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">// continue unwinding</span>
        <span class="token keyword">break</span><span class="token punctuation">;</span>

      <span class="token keyword">default</span><span class="token operator">:</span>
        <span class="token comment">// something went wrong</span>
        <span class="token function">_LIBUNWIND_TRACE_UNWINDING</span><span class="token punctuation">(</span>
            <span class="token string">"unwind_phase1(ex_ojb=%p): _URC_FATAL_PHASE1_ERROR"</span><span class="token punctuation">,</span>
            <span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token punctuation">)</span>exception_object<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">return</span> _URC_FATAL_PHASE1_ERROR<span class="token punctuation">;</span>
      <span class="token punctuation">&#125;</span>
    <span class="token punctuation">&#125;</span>
  <span class="token punctuation">&#125;</span>
  <span class="token keyword">return</span> _URC_NO_REASON<span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">static _Unwind_Reason_Code
unwind_phase2(unw_context_t *uc, unw_cursor_t *cursor, _Unwind_Exception *exception_object) &#123;
  __unw_init_local(cursor, uc);

  _LIBUNWIND_TRACE_UNWINDING(&quot;unwind_phase2(ex_ojb&#x3D;%p)&quot;,
                             (void *)exception_object);

  &#x2F;&#x2F; uc is initialized by __unw_getcontext in the parent frame. The first stack
  &#x2F;&#x2F; frame walked is unwind_phase2.
  unsigned framesWalked &#x3D; 1;
  &#x2F;&#x2F; Walk each frame until we reach where search phase said to stop.
  while (true) &#123;

    &#x2F;&#x2F; Ask libunwind to get next frame (skip over first which is
    &#x2F;&#x2F; _Unwind_RaiseException).
    int stepResult &#x3D; __unw_step(cursor);
    &#x2F;&#x2F; ... ...

    &#x2F;&#x2F; Get info about this frame.
    unw_word_t sp;
    unw_proc_info_t frameInfo;
    __unw_get_reg(cursor, UNW_REG_SP, &amp;sp);
    if (__unw_get_proc_info(cursor, &amp;frameInfo) !&#x3D; UNW_ESUCCESS) &#123;
        &#x2F;&#x2F; ... ...
    &#125;

    &#x2F;&#x2F; ... ...

    ++framesWalked;
    &#x2F;&#x2F; If there is a personality routine, tell it we are unwinding.
    if (frameInfo.handler !&#x3D; 0) &#123;
      _Unwind_Personality_Fn p &#x3D;
          (_Unwind_Personality_Fn)(uintptr_t)(frameInfo.handler);
      _Unwind_Action action &#x3D; _UA_CLEANUP_PHASE;
      if (sp &#x3D;&#x3D; exception_object-&gt;private_2) &#123;
        &#x2F;&#x2F; Tell personality this was the frame it marked in phase 1.
        action &#x3D; (_Unwind_Action)(_UA_CLEANUP_PHASE | _UA_HANDLER_FRAME);
      &#125;
       _Unwind_Reason_Code personalityResult &#x3D;
          (*p)(1, action, exception_object-&gt;exception_class, exception_object,
               (struct _Unwind_Context *)(cursor));
      switch (personalityResult) &#123;
      case _URC_CONTINUE_UNWIND:
        &#x2F;&#x2F; Continue unwinding
        _LIBUNWIND_TRACE_UNWINDING(
            &quot;unwind_phase2(ex_ojb&#x3D;%p): _URC_CONTINUE_UNWIND&quot;,
            (void *)exception_object);
        if (sp &#x3D;&#x3D; exception_object-&gt;private_2) &#123;
          &#x2F;&#x2F; Phase 1 said we would stop at this frame, but we did not...
          _LIBUNWIND_ABORT(&quot;during phase1 personality function said it would &quot;
                           &quot;stop here, but now in phase2 it did not stop here&quot;);
        &#125;
        break;
      case _URC_INSTALL_CONTEXT:
        _LIBUNWIND_TRACE_UNWINDING(
            &quot;unwind_phase2(ex_ojb&#x3D;%p): _URC_INSTALL_CONTEXT&quot;,
            (void *)exception_object);
        &#x2F;&#x2F; Personality routine says to transfer control to landing pad.
        &#x2F;&#x2F; We may get control back if landing pad calls _Unwind_Resume().
        if (_LIBUNWIND_TRACING_UNWINDING) &#123;
          unw_word_t pc;
          __unw_get_reg(cursor, UNW_REG_IP, &amp;pc);
          __unw_get_reg(cursor, UNW_REG_SP, &amp;sp);
          _LIBUNWIND_TRACE_UNWINDING(&quot;unwind_phase2(ex_ojb&#x3D;%p): re-entering &quot;
                                     &quot;user code with ip&#x3D;0x%&quot; PRIxPTR
                                     &quot;, sp&#x3D;0x%&quot; PRIxPTR,
                                     (void *)exception_object, pc, sp);
        &#125;

        __unw_phase2_resume(cursor, framesWalked);
        &#x2F;&#x2F; __unw_phase2_resume() only returns if there was an error.
        return _URC_FATAL_PHASE2_ERROR;
      default:
        &#x2F;&#x2F; Personality routine returned an unknown result code.
        _LIBUNWIND_DEBUG_LOG(&quot;personality function returned unknown result %d&quot;,
                             personalityResult);
        return _URC_FATAL_PHASE2_ERROR;
      &#125;
    &#125;
  &#125;

  &#x2F;&#x2F; Clean up phase did not resume at the frame that the search phase
  &#x2F;&#x2F; said it would...
  return _URC_FATAL_PHASE2_ERROR;
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>  这里的代码也很明晰，首先获取了当前栈帧的信息，然后将frameInfo.handler转换为_Unwind_Personality_Fn处理函数，然后调用这个函数进行处理。这里有两种情况：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>unwind_phase1，当action=_UA_SEARCH_PHASE时，代码我们当前阶段是通过_Unwind_Personality_Fn搜索catch代码块，当找到处理块时，返回_URC_HANDLER_FOUND，并给exception_object-&gt;private_2赋值，方便在第二阶段进行执行。</p>
</li>
<li class="lvl-2">
<p>unwind_phase2，exception_object-&gt;private_2 == sp时，当action=(_UA_CLEANUP_PHASE | _UA_HANDLER_FRAME)时，我们开始调用_Unwind_Personality_Fn安装对应的catch-block，然后返回_URC_INSTALL_CONTEXT，最后执行__unw_phase2_resume开始执行异常处理。</p>
</li>
</ul>
<p>  此外，这里的 __unw_init_local执行了一个非常重要的操作，那就是找到了.eh_frame的位置，下面简单看一下代码流程:</p>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">inline bool LocalAddressSpace::findUnwindSections(pint_t targetAddr,
                                                  UnwindInfoSections &amp;info) &#123;

    &#x2F;&#x2F; ... ...

    info.dso_base &#x3D; 0;
    &#x2F;&#x2F; Bare metal is statically linked, so no need to ask the dynamic loader
    info.dwarf_section_length &#x3D; (size_t)(&amp;__eh_frame_end - &amp;__eh_frame_start);
    info.dwarf_section &#x3D;        (uintptr_t)(&amp;__eh_frame_start);

    &#x2F;&#x2F; ... ...
&#125;

template &lt;typename A, typename R&gt;
void UnwindCursor&lt;A, R&gt;::setInfoBasedOnIPRegister(bool isReturnAddress) &#123;

  &#x2F;&#x2F; ... ...
  &#x2F;&#x2F; Ask address space object to find unwind sections for this pc.
  UnwindInfoSections sects;
  if (_addressSpace.findUnwindSections(pc, sects)) 
  &#x2F;&#x2F; ... ...
&#125;

&#x2F;&#x2F; template &lt;typename A, typename R&gt;
&#x2F;&#x2F; int UnwindCursor&lt;A, R&gt;::step() &#123;
&#x2F;&#x2F;     &#x2F;&#x2F; ... ...
&#x2F;&#x2F;     this-&gt;setInfoBasedOnIPRegister(true);
&#x2F;&#x2F;     &#x2F;&#x2F; ... ...
&#x2F;&#x2F; &#125;

_LIBUNWIND_HIDDEN int __unw_init_local(unw_cursor_t *cursor,
                                       unw_context_t *context) &#123;
  &#x2F;&#x2F; ... ...
  &#x2F;&#x2F; Use &quot;placement new&quot; to allocate UnwindCursor in the cursor buffer.
  new (reinterpret_cast&lt;UnwindCursor&lt;LocalAddressSpace, REGISTER_KIND&gt; *&gt;(cursor))
      UnwindCursor&lt;LocalAddressSpace, REGISTER_KIND&gt;(
          context, LocalAddressSpace::sThisAddressSpace);
#undef REGISTER_KIND
  AbstractUnwindCursor *co &#x3D; (AbstractUnwindCursor *)cursor;
  co-&gt;setInfoBasedOnIPRegister();

  return UNW_ESUCCESS;
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>  这里的_Unwind_Personality_Fn函数是itanium-cxx-abi 定义的，定义文档在这个位置<a target="_blank" rel="noopener" href="https://itanium-cxx-abi.github.io/cxx-abi/abi-eh.html#cxx-throw%E3%80%82%E4%B8%BB%E8%A6%81%E4%BD%9C%E7%94%A8%E5%B0%B1%E6%98%AF%E5%92%8Cc++%E7%89%B9%E6%80%A7%E7%9B%B8%E5%85%B3%E7%9A%84%E5%A0%86%E6%A0%88%E5%B1%95%E5%BC%80%E7%89%B9%E5%AE%9A%E4%BB%A3%E7%A0%81%EF%BC%8C%E8%BF%99%E4%B8%AA%E5%87%BD%E6%95%B0%E5%9C%A8gcc/clang%E9%87%8C%E9%9D%A2%E5%8F%AB%E5%81%9A%EF%BC%9A__gxx_personality_v0%EF%BC%8C%E6%88%91%E4%BB%AC%E7%9B%B4%E6%8E%A5%E5%8E%BB%E7%9C%8B%E4%BB%96%E7%9A%84%E6%BA%90%E7%A0%81%E3%80%82">https://itanium-cxx-abi.github.io/cxx-abi/abi-eh.html#cxx-throw。主要作用就是和c++特性相关的堆栈展开特定代码，这个函数在gcc/clang里面叫做：__gxx_personality_v0，我们直接去看他的源码。</a></p>
<p>libcxxabi\src\cxa_personality.cpp</p>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#if !defined(_LIBCXXABI_ARM_EHABI)
#if defined(__SEH__) &amp;&amp; !defined(__USING_SJLJ_EXCEPTIONS__)
static _Unwind_Reason_Code __gxx_personality_imp
#else
_LIBCXXABI_FUNC_VIS _Unwind_Reason_Code
#ifdef __USING_SJLJ_EXCEPTIONS__
__gxx_personality_sj0
#elif defined(__MVS__)
__zos_cxx_personality_v2
#else
__gxx_personality_v0
#endif
#endif
                    (int version, _Unwind_Action actions, uint64_t exceptionClass,
                     _Unwind_Exception* unwind_exception, _Unwind_Context* context)
&#123;
    if (version !&#x3D; 1 || unwind_exception &#x3D;&#x3D; 0 || context &#x3D;&#x3D; 0)
        return _URC_FATAL_PHASE1_ERROR;

    bool native_exception &#x3D; (exceptionClass     &amp; get_vendor_and_language) &#x3D;&#x3D;
                            (kOurExceptionClass &amp; get_vendor_and_language);
    scan_results results;
    &#x2F;&#x2F; Process a catch handler for a native exception first.
    if (actions &#x3D;&#x3D; (_UA_CLEANUP_PHASE | _UA_HANDLER_FRAME) &amp;&amp;
        native_exception) &#123;
        &#x2F;&#x2F; Reload the results from the phase 1 cache.
        __cxa_exception* exception_header &#x3D;
            (__cxa_exception*)(unwind_exception + 1) - 1;
        results.ttypeIndex &#x3D; exception_header-&gt;handlerSwitchValue;
        results.actionRecord &#x3D; exception_header-&gt;actionRecord;
        results.languageSpecificData &#x3D; exception_header-&gt;languageSpecificData;
        results.landingPad &#x3D;
            reinterpret_cast&lt;uintptr_t&gt;(exception_header-&gt;catchTemp);
        results.adjustedPtr &#x3D; exception_header-&gt;adjustedPtr;

        &#x2F;&#x2F; Jump to the handler.
        set_registers(unwind_exception, context, results);
        &#x2F;&#x2F; Cache base for calculating the address of ttype in
        &#x2F;&#x2F; __cxa_call_unexpected.
        if (results.ttypeIndex &lt; 0) &#123;
#if defined(_AIX)
          exception_header-&gt;catchTemp &#x3D; (void *)_Unwind_GetDataRelBase(context);
#else
          exception_header-&gt;catchTemp &#x3D; 0;
#endif
        &#125;
        return _URC_INSTALL_CONTEXT;
    &#125;

    &#x2F;&#x2F; In other cases we need to scan LSDA.
    scan_eh_tab(results, actions, native_exception, unwind_exception, context);
    if (results.reason &#x3D;&#x3D; _URC_CONTINUE_UNWIND ||
        results.reason &#x3D;&#x3D; _URC_FATAL_PHASE1_ERROR)
        return results.reason;

    if (actions &amp; _UA_SEARCH_PHASE)
    &#123;
        &#x2F;&#x2F; Phase 1 search:  All we&#39;re looking for in phase 1 is a handler that
        &#x2F;&#x2F;   halts unwinding
        assert(results.reason &#x3D;&#x3D; _URC_HANDLER_FOUND);
        if (native_exception) &#123;
            &#x2F;&#x2F; For a native exception, cache the LSDA result.
            __cxa_exception* exc &#x3D; (__cxa_exception*)(unwind_exception + 1) - 1;
            exc-&gt;handlerSwitchValue &#x3D; static_cast&lt;int&gt;(results.ttypeIndex);
            exc-&gt;actionRecord &#x3D; results.actionRecord;
            exc-&gt;languageSpecificData &#x3D; results.languageSpecificData;
            exc-&gt;catchTemp &#x3D; reinterpret_cast&lt;void*&gt;(results.landingPad);
            exc-&gt;adjustedPtr &#x3D; results.adjustedPtr;
        &#125;
        return _URC_HANDLER_FOUND;
    &#125;

    assert(actions &amp; _UA_CLEANUP_PHASE);
    assert(results.reason &#x3D;&#x3D; _URC_HANDLER_FOUND);
    set_registers(unwind_exception, context, results);
    &#x2F;&#x2F; Cache base for calculating the address of ttype in __cxa_call_unexpected.
    if (results.ttypeIndex &lt; 0) &#123;
      __cxa_exception* exception_header &#x3D;
            (__cxa_exception*)(unwind_exception + 1) - 1;
#if defined(_AIX)
      exception_header-&gt;catchTemp &#x3D; (void *)_Unwind_GetDataRelBase(context);
#else
      exception_header-&gt;catchTemp &#x3D; 0;
#endif
    &#125;
    return _URC_INSTALL_CONTEXT;
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>  我们从整体来看这段代码，从上面可以知道，phase1，phase2都会调用到这里来：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>phase1, action=_UA_SEARCH_PHASE, 调用scan_eh_tab查找catch-block，并返回_URC_HANDLER_FOUND</p>
</li>
<li class="lvl-2">
<p>phase2, action=(_UA_CLEANUP_PHASE | _UA_HANDLER_FRAME)，通过set_registers设置对应的catch-block，然后返回_URC_INSTALL_CONTEXT，然后在__unw_phase2_resume执行对应的catch-block。</p>
</li>
</ul>
<p>  从上面的实现来看，scan_eh_tab是核心，其正是展开异常搜索和匹配的关键。其源码如下</p>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">static void scan_eh_tab(scan_results &amp;results, _Unwind_Action actions,
                        bool native_exception,
                        _Unwind_Exception *unwind_exception,
                        _Unwind_Context *context) &#123;
    &#x2F;&#x2F; Initialize results to found nothing but an error
    results.ttypeIndex &#x3D; 0;
    results.actionRecord &#x3D; 0;
    results.languageSpecificData &#x3D; 0;
    results.landingPad &#x3D; 0;
    results.adjustedPtr &#x3D; 0;
    results.reason &#x3D; _URC_FATAL_PHASE1_ERROR;
    &#x2F;&#x2F; Check for consistent actions
    &#x2F;&#x2F; ... ...

    &#x2F;&#x2F; Start scan by getting exception table address.
    const uint8_t *lsda &#x3D; (const uint8_t *)_Unwind_GetLanguageSpecificData(context);
    if (lsda &#x3D;&#x3D; 0)
    &#123;
        &#x2F;&#x2F; There is no exception table
        results.reason &#x3D; _URC_CONTINUE_UNWIND;
        return;
    &#125;
    results.languageSpecificData &#x3D; lsda;
#if defined(_AIX)
    uintptr_t base &#x3D; _Unwind_GetDataRelBase(context);
#else
    uintptr_t base &#x3D; 0;
#endif
    &#x2F;&#x2F; Get the current instruction pointer and offset it before next
    &#x2F;&#x2F; instruction in the current frame which threw the exception.
    uintptr_t ip &#x3D; _Unwind_GetIP(context) - 1;
    &#x2F;&#x2F; Get beginning current frame&#39;s code (as defined by the
    &#x2F;&#x2F; emitted dwarf code)
    uintptr_t funcStart &#x3D; _Unwind_GetRegionStart(context);
#ifdef __USING_SJLJ_EXCEPTIONS__
    if (ip &#x3D;&#x3D; uintptr_t(-1))
    &#123;
        &#x2F;&#x2F; no action
        results.reason &#x3D; _URC_CONTINUE_UNWIND;
        return;
    &#125;
    else if (ip &#x3D;&#x3D; 0)
        call_terminate(native_exception, unwind_exception);
    &#x2F;&#x2F; ip is 1-based index into call site table
#else  &#x2F;&#x2F; !__USING_SJLJ_EXCEPTIONS__
    uintptr_t ipOffset &#x3D; ip - funcStart;
#endif &#x2F;&#x2F; !defined(_USING_SLJL_EXCEPTIONS__)
    const uint8_t* classInfo &#x3D; NULL;
    &#x2F;&#x2F; Note: See JITDwarfEmitter::EmitExceptionTable(...) for corresponding
    &#x2F;&#x2F;       dwarf emission
    &#x2F;&#x2F; Parse LSDA header.
    uint8_t lpStartEncoding &#x3D; *lsda++;
    const uint8_t* lpStart &#x3D;
        (const uint8_t*)readEncodedPointer(&amp;lsda, lpStartEncoding, base);
    if (lpStart &#x3D;&#x3D; 0)
        lpStart &#x3D; (const uint8_t*)funcStart;
    uint8_t ttypeEncoding &#x3D; *lsda++;
    if (ttypeEncoding !&#x3D; DW_EH_PE_omit)
    &#123;
        &#x2F;&#x2F; Calculate type info locations in emitted dwarf code which
        &#x2F;&#x2F; were flagged by type info arguments to llvm.eh.selector
        &#x2F;&#x2F; intrinsic
        uintptr_t classInfoOffset &#x3D; readULEB128(&amp;lsda);
        classInfo &#x3D; lsda + classInfoOffset;
    &#125;
    &#x2F;&#x2F; Walk call-site table looking for range that
    &#x2F;&#x2F; includes current PC.
    uint8_t callSiteEncoding &#x3D; *lsda++;
#ifdef __USING_SJLJ_EXCEPTIONS__
    (void)callSiteEncoding;  &#x2F;&#x2F; When using SjLj exceptions, callSiteEncoding is never used
#endif
    uint32_t callSiteTableLength &#x3D; static_cast&lt;uint32_t&gt;(readULEB128(&amp;lsda));
    const uint8_t* callSiteTableStart &#x3D; lsda;
    const uint8_t* callSiteTableEnd &#x3D; callSiteTableStart + callSiteTableLength;
    const uint8_t* actionTableStart &#x3D; callSiteTableEnd;
    const uint8_t* callSitePtr &#x3D; callSiteTableStart;
    while (callSitePtr &lt; callSiteTableEnd)
    &#123;
        &#x2F;&#x2F; There is one entry per call site.
#ifndef __USING_SJLJ_EXCEPTIONS__
        &#x2F;&#x2F; The call sites are non-overlapping in [start, start+length)
        &#x2F;&#x2F; The call sites are ordered in increasing value of start
        uintptr_t start &#x3D; readEncodedPointer(&amp;callSitePtr, callSiteEncoding);
        uintptr_t length &#x3D; readEncodedPointer(&amp;callSitePtr, callSiteEncoding);
        uintptr_t landingPad &#x3D; readEncodedPointer(&amp;callSitePtr, callSiteEncoding);
        uintptr_t actionEntry &#x3D; readULEB128(&amp;callSitePtr);
        if ((start &lt;&#x3D; ipOffset) &amp;&amp; (ipOffset &lt; (start + length)))
#else  &#x2F;&#x2F; __USING_SJLJ_EXCEPTIONS__
        &#x2F;&#x2F; ip is 1-based index into this table
        uintptr_t landingPad &#x3D; readULEB128(&amp;callSitePtr);
        uintptr_t actionEntry &#x3D; readULEB128(&amp;callSitePtr);
        if (--ip &#x3D;&#x3D; 0)
#endif &#x2F;&#x2F; __USING_SJLJ_EXCEPTIONS__
        &#123;
            &#x2F;&#x2F; Found the call site containing ip.
#ifndef __USING_SJLJ_EXCEPTIONS__
            if (landingPad &#x3D;&#x3D; 0)
            &#123;
                &#x2F;&#x2F; No handler here
                results.reason &#x3D; _URC_CONTINUE_UNWIND;
                return;
            &#125;
            landingPad &#x3D; (uintptr_t)lpStart + landingPad;
#else  &#x2F;&#x2F; __USING_SJLJ_EXCEPTIONS__
            ++landingPad;
#endif &#x2F;&#x2F; __USING_SJLJ_EXCEPTIONS__
            results.landingPad &#x3D; landingPad;
            if (actionEntry &#x3D;&#x3D; 0)
            &#123;
                &#x2F;&#x2F; Found a cleanup
                results.reason &#x3D; actions &amp; _UA_SEARCH_PHASE
                                     ? _URC_CONTINUE_UNWIND
                                     : _URC_HANDLER_FOUND;
                return;
            &#125;
            &#x2F;&#x2F; Convert 1-based byte offset into
            const uint8_t* action &#x3D; actionTableStart + (actionEntry - 1);
            bool hasCleanup &#x3D; false;
            &#x2F;&#x2F; Scan action entries until you find a matching handler, cleanup, or the end of action list
            while (true)
            &#123;
                const uint8_t* actionRecord &#x3D; action;
                int64_t ttypeIndex &#x3D; readSLEB128(&amp;action);
                if (ttypeIndex &gt; 0)
                &#123;
                    &#x2F;&#x2F; Found a catch, does it actually catch?
                    &#x2F;&#x2F; First check for catch (...)
                    const __shim_type_info* catchType &#x3D;
                        get_shim_type_info(static_cast&lt;uint64_t&gt;(ttypeIndex),
                                           classInfo, ttypeEncoding,
                                           native_exception, unwind_exception,
                                           base);
                    if (catchType &#x3D;&#x3D; 0)
                    &#123;
                        &#x2F;&#x2F; Found catch (...) catches everything, including
                        &#x2F;&#x2F; foreign exceptions. This is search phase, cleanup
                        &#x2F;&#x2F; phase with foreign exception, or forced unwinding.
                        assert(actions &amp; (_UA_SEARCH_PHASE | _UA_HANDLER_FRAME |
                                          _UA_FORCE_UNWIND));
                        results.ttypeIndex &#x3D; ttypeIndex;
                        results.actionRecord &#x3D; actionRecord;
                        results.adjustedPtr &#x3D;
                            get_thrown_object_ptr(unwind_exception);
                        results.reason &#x3D; _URC_HANDLER_FOUND;
                        return;
                    &#125;
                    &#x2F;&#x2F; Else this is a catch (T) clause and will never
                    &#x2F;&#x2F;    catch a foreign exception
                    else if (native_exception)
                    &#123;
                        __cxa_exception* exception_header &#x3D; (__cxa_exception*)(unwind_exception+1) - 1;
                        void* adjustedPtr &#x3D; get_thrown_object_ptr(unwind_exception);
                        const __shim_type_info* excpType &#x3D;
                            static_cast&lt;const __shim_type_info*&gt;(exception_header-&gt;exceptionType);
                        if (adjustedPtr &#x3D;&#x3D; 0 || excpType &#x3D;&#x3D; 0)
                        &#123;
                            &#x2F;&#x2F; Something very bad happened
                            call_terminate(native_exception, unwind_exception);
                        &#125;
                        if (catchType-&gt;can_catch(excpType, adjustedPtr))
                        &#123;
                            &#x2F;&#x2F; Found a matching handler. This is either search
                            &#x2F;&#x2F; phase or forced unwinding.
                            assert(actions &amp;
                                   (_UA_SEARCH_PHASE | _UA_FORCE_UNWIND));
                            results.ttypeIndex &#x3D; ttypeIndex;
                            results.actionRecord &#x3D; actionRecord;
                            results.adjustedPtr &#x3D; adjustedPtr;
                            results.reason &#x3D; _URC_HANDLER_FOUND;
                            return;
                        &#125;
                    &#125;
                    &#x2F;&#x2F; Scan next action ...
                &#125;
                else if (ttypeIndex &lt; 0)
                &#123;
                    &#x2F;&#x2F; Found an exception specification.
                    if (actions &amp; _UA_FORCE_UNWIND) &#123;
                        &#x2F;&#x2F; Skip if forced unwinding.
                    &#125; else if (native_exception) &#123;
                        &#x2F;&#x2F; Does the exception spec catch this native exception?
                        __cxa_exception* exception_header &#x3D; (__cxa_exception*)(unwind_exception+1) - 1;
                        void* adjustedPtr &#x3D; get_thrown_object_ptr(unwind_exception);
                        const __shim_type_info* excpType &#x3D;
                            static_cast&lt;const __shim_type_info*&gt;(exception_header-&gt;exceptionType);
                        if (adjustedPtr &#x3D;&#x3D; 0 || excpType &#x3D;&#x3D; 0)
                        &#123;
                            &#x2F;&#x2F; Something very bad happened
                            call_terminate(native_exception, unwind_exception);
                        &#125;
                        if (exception_spec_can_catch(ttypeIndex, classInfo,
                                                     ttypeEncoding, excpType,
                                                     adjustedPtr,
                                                     unwind_exception, base))
                        &#123;
                            &#x2F;&#x2F; Native exception caught by exception
                            &#x2F;&#x2F; specification.
                            assert(actions &amp; _UA_SEARCH_PHASE);
                            results.ttypeIndex &#x3D; ttypeIndex;
                            results.actionRecord &#x3D; actionRecord;
                            results.adjustedPtr &#x3D; adjustedPtr;
                            results.reason &#x3D; _URC_HANDLER_FOUND;
                            return;
                        &#125;
                    &#125; else &#123;
                        &#x2F;&#x2F; foreign exception caught by exception spec
                        results.ttypeIndex &#x3D; ttypeIndex;
                        results.actionRecord &#x3D; actionRecord;
                        results.adjustedPtr &#x3D;
                            get_thrown_object_ptr(unwind_exception);
                        results.reason &#x3D; _URC_HANDLER_FOUND;
                        return;
                    &#125;
                    &#x2F;&#x2F; Scan next action ...
                &#125; else &#123;
                    hasCleanup &#x3D; true;
                &#125;
                const uint8_t* temp &#x3D; action;
                int64_t actionOffset &#x3D; readSLEB128(&amp;temp);
                if (actionOffset &#x3D;&#x3D; 0)
                &#123;
                    &#x2F;&#x2F; End of action list. If this is phase 2 and we have found
                    &#x2F;&#x2F; a cleanup (ttypeIndex&#x3D;0), return _URC_HANDLER_FOUND;
                    &#x2F;&#x2F; otherwise return _URC_CONTINUE_UNWIND.
                    results.reason &#x3D; hasCleanup &amp;&amp; actions &amp; _UA_CLEANUP_PHASE
                                         ? _URC_HANDLER_FOUND
                                         : _URC_CONTINUE_UNWIND;
                    return;
                &#125;
                &#x2F;&#x2F; Go to next action
                action +&#x3D; actionOffset;
            &#125;  &#x2F;&#x2F; there is no break out of this loop, only return
        &#125;
#ifndef __USING_SJLJ_EXCEPTIONS__
        else if (ipOffset &lt; start)
        &#123;
            &#x2F;&#x2F; There is no call site for this ip
            &#x2F;&#x2F; Something bad has happened.  We should never get here.
            &#x2F;&#x2F; Possible stack corruption.
            call_terminate(native_exception, unwind_exception);
        &#125;
#endif &#x2F;&#x2F; !__USING_SJLJ_EXCEPTIONS__
    &#125;  &#x2F;&#x2F; there might be some tricky cases which break out of this loop

    &#x2F;&#x2F; It is possible that no eh table entry specify how to handle
    &#x2F;&#x2F; this exception. By spec, terminate it immediately.
    call_terminate(native_exception, unwind_exception);
&#125;
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>  从这里可以看到，这里的核心就是获取lsda数据（_Unwind_GetLanguageSpecificData, .gcc_except_table段），然后用上下文传过来的抛出的异常信息来匹配，如果匹配上，就找到了对应的catch字段，我们就返回并执行，如果没有匹配上，就只有调用std::terminate了。</p>
<p>  其实这里的解析lsda，就能找到对应的catch-block，因此我们需要了解一下lsda的大致结构：</p>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;*
    Exception Handling Table Layout:

+-----------------+--------+
| lpStartEncoding | (char) |
+---------+-------+--------+---------------+-----------------------+
| lpStart | (encoded with lpStartEncoding) | defaults to funcStart |
+---------+-----+--------+-----------------+---------------+-------+
| ttypeEncoding | (char) | Encoding of the type_info table |
+---------------+-+------+----+----------------------------+----------------+
| classInfoOffset | (ULEB128) | Offset to type_info table, defaults to null |
+-----------------++--------+-+----------------------------+----------------+
| callSiteEncoding | (char) | Encoding for Call Site Table |
+------------------+--+-----+-----+------------------------+--------------------------+
| callSiteTableLength | (ULEB128) | Call Site Table length, used to find Action table |
+---------------------+-----------+---------------------------------------------------+
+---------------------+-----------+------------------------------------------------+
| Beginning of Call Site Table            The current ip is a 1-based index into   |
| ...                                     this table.  Or it is -1 meaning no      |
|                                         action is needed.  Or it is 0 meaning    |
|                                         terminate.                               |
| +-------------+---------------------------------+------------------------------+ |
| | landingPad  | (ULEB128)                       | offset relative to lpStart   | |
| | actionEntry | (ULEB128)                       | Action Table Index 1-based   | |
| |             |                                 | actionEntry &#x3D;&#x3D; 0 -&gt; cleanup  | |
| +-------------+---------------------------------+------------------------------+ |
| ...                                                                              |
+----------------------------------------------------------------------------------+
+---------------------------------------------------------------------+
| Beginning of Action Table       ttypeIndex &#x3D;&#x3D; 0 : cleanup           |
| ...                             ttypeIndex  &gt; 0 : catch             |
|                                 ttypeIndex  &lt; 0 : exception spec    |
| +--------------+-----------+--------------------------------------+ |
| | ttypeIndex   | (SLEB128) | Index into type_info Table (1-based) | |
| | actionOffset | (SLEB128) | Offset into next Action Table entry  | |
| +--------------+-----------+--------------------------------------+ |
| ...                                                                 |
+---------------------------------------------------------------------+-----------------+
| type_info Table, but classInfoOffset does *not* point here!                           |
| +----------------+------------------------------------------------+-----------------+ |
| | Nth type_info* | Encoded with ttypeEncoding, 0 means catch(...) | ttypeIndex &#x3D;&#x3D; N | |
| +----------------+------------------------------------------------+-----------------+ |
| ...                                                                                   |
| +----------------+------------------------------------------------+-----------------+ |
| | 1st type_info* | Encoded with ttypeEncoding, 0 means catch(...) | ttypeIndex &#x3D;&#x3D; 1 | |
| +----------------+------------------------------------------------+-----------------+ |
| +---------------------------------------+-----------+------------------------------+  |
| | 1st ttypeIndex for 1st exception spec | (ULEB128) | classInfoOffset points here! |  |
| | ...                                   | (ULEB128) |                              |  |
| | Mth ttypeIndex for 1st exception spec | (ULEB128) |                              |  |
| | 0                                     | (ULEB128) |                              |  |
| +---------------------------------------+------------------------------------------+  |
| ...                                                                                   |
| +---------------------------------------+------------------------------------------+  |
| | 0                                     | (ULEB128) | throw()                      |  |
| +---------------------------------------+------------------------------------------+  |
| ...                                                                                   |
| +---------------------------------------+------------------------------------------+  |
| | 1st ttypeIndex for Nth exception spec | (ULEB128) |                              |  |
| | ...                                   | (ULEB128) |                              |  |
| | Mth ttypeIndex for Nth exception spec | (ULEB128) |                              |  |
| | 0                                     | (ULEB128) |                              |  |
| +---------------------------------------+------------------------------------------+  |
+---------------------------------------------------------------------------------------+
*&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>  从这里可以知道，其实lsda的核心，就是遍历 Call Site Table，获取到Action Table Index，然后在Action Table中获取到ttypeIndex，然后根据ttypeIndex在type_info Table中开始搜索和匹配异常对象和catch对象是否匹配。如果匹配，返回，如果不匹配，循环遍历Action Table中的action链表，直到处理完。</p>
<br/>
<br/>
<br/>
<br/>
<h3 id="本文不同异常捕获的原因分析">本文不同异常捕获的原因分析</h3>
<p>  根据上文的分析，本文的问题肯定出在lsda的Action Table和type_info Table上面。</p>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">int main(int argc, char* argv[])
&#123;
        try&#123;
                p();
        &#125;
        catch(std::exception&amp; e)&#123;
                printf(&quot;std::exception: %s\n&quot;, e.what());
        &#125;
        catch(...)&#123;
                printf(&quot;unkown exception\n&quot;);
        &#125;
        return 0;
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token comment"># objdump -d --disassemble=main ./build/test</span>
<span class="token comment"># 此时是正常捕获std异常</span>
0000000000001a70 <span class="token operator">&lt;</span>main<span class="token operator">></span>:
    1a70:       <span class="token number">55</span>                      push   %rbp
    1a71:       <span class="token number">48</span> <span class="token number">89</span> e5                mov    %rsp,%rbp
    1a74:       <span class="token number">48</span> <span class="token number">83</span> ec <span class="token number">30</span>             sub    <span class="token variable">$0x30</span>,%rsp
    1a78:       c7 <span class="token number">45</span> fc 00 00 00 00    movl   <span class="token variable">$0x0</span>,-0x4<span class="token punctuation">(</span>%rbp<span class="token punctuation">)</span>
    1a7f:       <span class="token number">89</span> 7d f8                mov    %edi,-0x8<span class="token punctuation">(</span>%rbp<span class="token punctuation">)</span>
    1a82:       <span class="token number">48</span> <span class="token number">89</span> <span class="token number">75</span> f0             mov    %rsi,-0x10<span class="token punctuation">(</span>%rbp<span class="token punctuation">)</span>
    1a86:       e8 <span class="token number">35</span> 01 00 00          call   1bc0 <span class="token operator">&lt;</span>p@plt<span class="token operator">></span>
    1a8b:       e9 00 00 00 00          jmp    1a90 <span class="token operator">&lt;</span>main+0x2<span class="token operator"><span class="token file-descriptor important">0</span>></span>
    1a90:       e9 <span class="token number">51</span> 00 00 00          jmp    1ae6 <span class="token operator">&lt;</span>main+0x7<span class="token operator"><span class="token file-descriptor important">6</span>></span>
    1a95:       <span class="token number">48</span> <span class="token number">89</span> c1                mov    %rax,%rcx
    1a98:       <span class="token number">89</span> d0                   mov    %edx,%eax
    1a9a:       <span class="token number">48</span> <span class="token number">89</span> 4d e8             mov    %rcx,-0x18<span class="token punctuation">(</span>%rbp<span class="token punctuation">)</span>
    1a9e:       <span class="token number">89</span> <span class="token number">45</span> e4                mov    %eax,-0x1c<span class="token punctuation">(</span>%rbp<span class="token punctuation">)</span>
    1aa1:       8b <span class="token number">45</span> e4                mov    -0x1c<span class="token punctuation">(</span>%rbp<span class="token punctuation">)</span>,%eax
    1aa4:       b9 02 00 00 00          mov    <span class="token variable">$0x2</span>,%ecx
    1aa9:       <span class="token number">39</span> c8                   <span class="token function">cmp</span>    %ecx,%eax
    1aab:       0f <span class="token number">85</span> 3d 00 00 00       jne    1aee <span class="token operator">&lt;</span>main+0x7e<span class="token operator">></span>
    1ab1:       <span class="token number">48</span> 8b 7d e8             mov    -0x18<span class="token punctuation">(</span>%rbp<span class="token punctuation">)</span>,%rdi
    1ab5:       e8 <span class="token number">16</span> 01 00 00          call   1bd0 <span class="token operator">&lt;</span>__cxa_begin_catch@plt<span class="token operator">></span>
    1aba:       <span class="token number">48</span> <span class="token number">89</span> <span class="token number">45</span> d8             mov    %rax,-0x28<span class="token punctuation">(</span>%rbp<span class="token punctuation">)</span>
    1abe:       <span class="token number">48</span> 8b 7d d8             mov    -0x28<span class="token punctuation">(</span>%rbp<span class="token punctuation">)</span>,%rdi
    1ac2:       <span class="token number">48</span> 8b 07                mov    <span class="token punctuation">(</span>%rdi<span class="token punctuation">)</span>,%rax
    1ac5:       <span class="token number">48</span> 8b <span class="token number">40</span> <span class="token number">10</span>             mov    0x10<span class="token punctuation">(</span>%rax<span class="token punctuation">)</span>,%rax
    1ac9:       ff d0                   call   *%rax
    1acb:       <span class="token number">48</span> <span class="token number">89</span> c6                mov    %rax,%rsi
    1ace:       <span class="token number">48</span> 8d 3d a1 ed ff ff    lea    -0x125f<span class="token punctuation">(</span>%rip<span class="token punctuation">)</span>,%rdi        <span class="token comment"># 876 &lt;_IO_stdin_used+0x16></span>
    1ad5:       <span class="token number">31</span> c0                   xor    %eax,%eax
    1ad7:       e8 04 01 00 00          call   1be0 <span class="token operator">&lt;</span>printf@plt<span class="token operator">></span>
    1adc:       e9 00 00 00 00          jmp    1ae1 <span class="token operator">&lt;</span>main+0x7<span class="token operator"><span class="token file-descriptor important">1</span>></span>
    1ae1:       e8 0a 01 00 00          call   1bf0 <span class="token operator">&lt;</span>__cxa_end_catch@plt<span class="token operator">></span>
    1ae6:       <span class="token number">31</span> c0                   xor    %eax,%eax
    1ae8:       <span class="token number">48</span> <span class="token number">83</span> c4 <span class="token number">30</span>             <span class="token function">add</span>    <span class="token variable">$0x30</span>,%rsp
    1aec:       5d                      pop    %rbp
    1aed:       c3                      ret
    1aee:       <span class="token number">48</span> 8b 7d e8             mov    -0x18<span class="token punctuation">(</span>%rbp<span class="token punctuation">)</span>,%rdi
    1af2:       e8 d9 00 00 00          call   1bd0 <span class="token operator">&lt;</span>__cxa_begin_catch@plt<span class="token operator">></span>
    1af7:       <span class="token number">48</span> 8d 3d <span class="token number">66</span> ed ff ff    lea    -0x129a<span class="token punctuation">(</span>%rip<span class="token punctuation">)</span>,%rdi        <span class="token comment"># 864 &lt;_IO_stdin_used+0x4></span>
    1afe:       <span class="token number">31</span> c0                   xor    %eax,%eax
    1b00:       e8 db 00 00 00          call   1be0 <span class="token operator">&lt;</span>printf@plt<span class="token operator">></span>
    1b05:       e9 00 00 00 00          jmp    1b0a <span class="token operator">&lt;</span>main+0x9a<span class="token operator">></span>
    1b0a:       e8 e1 00 00 00          call   1bf0 <span class="token operator">&lt;</span>__cxa_end_catch@plt<span class="token operator">></span>
    1b0f:       e9 d2 ff ff ff          jmp    1ae6 <span class="token operator">&lt;</span>main+0x7<span class="token operator"><span class="token file-descriptor important">6</span>></span>
    1b14:       <span class="token number">48</span> <span class="token number">89</span> c1                mov    %rax,%rcx
    1b17:       <span class="token number">89</span> d0                   mov    %edx,%eax
    1b19:       <span class="token number">48</span> <span class="token number">89</span> 4d e8             mov    %rcx,-0x18<span class="token punctuation">(</span>%rbp<span class="token punctuation">)</span>
    1b1d:       <span class="token number">89</span> <span class="token number">45</span> e4                mov    %eax,-0x1c<span class="token punctuation">(</span>%rbp<span class="token punctuation">)</span>
    1b20:       e8 cb 00 00 00          call   1bf0 <span class="token operator">&lt;</span>__cxa_end_catch@plt<span class="token operator">></span>
    1b25:       e9 00 00 00 00          jmp    1b2a <span class="token operator">&lt;</span>main+0xba<span class="token operator">></span>
    1b2a:       e9 1b 00 00 00          jmp    1b4a <span class="token operator">&lt;</span>main+0xda<span class="token operator">></span>
    1b2f:       <span class="token number">48</span> <span class="token number">89</span> c1                mov    %rax,%rcx
    1b32:       <span class="token number">89</span> d0                   mov    %edx,%eax
    1b34:       <span class="token number">48</span> <span class="token number">89</span> 4d e8             mov    %rcx,-0x18<span class="token punctuation">(</span>%rbp<span class="token punctuation">)</span>
    1b38:       <span class="token number">89</span> <span class="token number">45</span> e4                mov    %eax,-0x1c<span class="token punctuation">(</span>%rbp<span class="token punctuation">)</span>
    1b3b:       e8 b0 00 00 00          call   1bf0 <span class="token operator">&lt;</span>__cxa_end_catch@plt<span class="token operator">></span>
    1b40:       e9 00 00 00 00          jmp    1b45 <span class="token operator">&lt;</span>main+0xd<span class="token operator"><span class="token file-descriptor important">5</span>></span>
    1b45:       e9 00 00 00 00          jmp    1b4a <span class="token operator">&lt;</span>main+0xda<span class="token operator">></span>
    1b4a:       <span class="token number">48</span> 8b 7d e8             mov    -0x18<span class="token punctuation">(</span>%rbp<span class="token punctuation">)</span>,%rdi
    1b4e:       e8 ad 00 00 00          call   1c00 <span class="token operator">&lt;</span>_Unwind_Resume@plt<span class="token operator">></span>
    1b53:       <span class="token number">48</span> <span class="token number">89</span> c7                mov    %rax,%rdi
    1b56:       e8 05 00 00 00          call   1b60 <span class="token operator">&lt;</span>__clang_call_terminate<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>  当正常捕获异常时，cmp    %ecx,%eax位置的eax的值是2，正常进入异常分支。当异常捕获异常时，cmp    %ecx,%eax位置的eax的值是1，进入异常捕获分支。意味着在异常情况下：get_shim_type_info（scan_eh_tab中）返回值是0。（注意，第一次查找到了类型，但是不匹配，循环遍历链表下一此匹配到了catch(…)）</p>
<p>  上面是我们的猜测，我们直接重新构建libcxx/libcxxabi的debug版本，然后再构建我们的测试程序，然后在scan_eh_tab中我们得到了如下的图的核心结果：</p>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/blog_idx_146/1.png" alt="rep_img"/></center>
    </div>
</div>   
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/blog_idx_146/2.png" alt="rep_img"/></center>
    </div>
</div>   
<p>  从上面可知，我们不同的构建方法，导致了cxx底层无法对两个class类型进行dynamic_cast，导致无法匹配，因此进入了catch(…)的代码段。有兴趣的人可以去追踪dynamic_cast的底层实现函数如下：</p>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">__dynamic_cast(const void *static_ptr, const __class_type_info *static_type,
               const __class_type_info *dst_type,
               std::ptrdiff_t src2dst_offset)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>  也就是说，我们的核心原因就是__class_type_info在静态编译、动态编译不同情况下，虽然定义是一样的,当两个符号分别在libc++.so和libuser.so的不同符号的时候(地址不一样)，但是无法进行cast操作，这是合理的。</p>
<br/>
<br/>
<br/>
<br/>
<h3 id="后记">后记</h3>
<hr>
<p>  总的来说，上面的内容解答了如下两个问题：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>为什么会捕获到异常：编译条件导致的android系统底层对某些api有不同的控制行为？</p>
</li>
<li class="lvl-2">
<p>为什么符号都存在的情况下，走了不一样的异常捕获路径：核心在于typeinfo对象无法dynamic_cast</p>
</li>
</ul>
<p>  此次问题调查，加深了我对stl_static/stl_shared的理解，同时加深了我对c++底层实现的了解。加深了我对gcc/clang等编译器的底层功能结构的了解。</p>
<p>  同时，根据这次折腾llvm源码的过程，下次再一次想了解c++底层的实现的话，会快捷、方便不少。</p>
<h3 id="参考文献">参考文献</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://itanium-cxx-abi.github.io/cxx-abi/abi-eh.html#cxx-throw">https://itanium-cxx-abi.github.io/cxx-abi/abi-eh.html#cxx-throw</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://en.cppreference.com/w/cpp/filesystem/exists.html">https://en.cppreference.com/w/cpp/filesystem/exists.html</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://developer.android.com/ndk/guides/cpp-support?hl=zh-cn#selecting_a_c_runtime">https://developer.android.com/ndk/guides/cpp-support?hl=zh-cn#selecting_a_c_runtime</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://en.cppreference.com/w/cpp/language/throw.html">https://en.cppreference.com/w/cpp/language/throw.html</a></p>
</li>
</ul>
<br/>
<br/>
<div style="margin:50px auto;">
</div>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <hr/>
        <center><font color = #91e0b0 size = 5>打赏、订阅、收藏、丢香蕉、硬币，请关注公众号（攻城狮的搬砖之路）</font></center>
    </div>
</div>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/qrcode_for_wx_official_account.jpg" alt="qrc_img"/></center>
    </div>
</div>
<!-- ![alt 公众号图片](https://flyinskyin2013.github.io/ImageBed0/blogs/qrcode_for_wx_official_account.jpg "公众号图片") -->
<p><font color="red" size="7">PS: 请尊重原创，不喜勿喷。</font><br/><br>
<font color="red" size="7">PS: 要转载请注明出处，本人版权所有。</font><br/><br>
<font color="red" size="7">PS: 有问题请留言，看到后我会第一时间回复。</font><br/></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://e-x.top/2025/11/30/blog_idx_145/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Sky">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sky's Blogs">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | Sky's Blogs">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/11/30/blog_idx_145/" class="post-title-link" itemprop="url">大模型基础补全计划(八)---相关知识点回顾与Qwen3-VL-2B-Instruct实例分析(终章)</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-11-30 11:07:00 / 修改时间：11:11:30" itemprop="dateCreated datePublished" datetime="2025-11-30T11:07:00+08:00">2025-11-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/LLM/" itemprop="url" rel="index"><span itemprop="name">LLM</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/LLM/LM/" itemprop="url" rel="index"><span itemprop="name">LM</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>13 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <script src="\assets\js\APlayer.min.js"> </script><!--
 * @Description: 
 * @Author: Sky
 * @Date: 2020-08-24 16:37:34
 * @LastEditors: Sky
 * @LastEditTime: 2021-06-29 15:04:48
 * @Github: https://github.com/flyinskyin2013/
-->
<p><font color="red" size="7">PS：要转载请注明出处，本人版权所有。</font><br/><br>
<font color="red" size="7">PS: 这个只是基于《我自己》的理解，</font><br/><font color="red" size="7">如果和你的原则及想法相冲突，请谅解，勿喷。</font><br/></p>
<!-- ###### 前置说明
&emsp;&emsp;本文作为本人github blog/cnblog的主站的备份。（BlogID=120） 
&emsp;&emsp;本文发布于 2025-11-30 11:07:00             （BlogID=120） 
-->
<h6 id="环境说明">环境说明</h6>
<p>  无</p>
<h3 id="前言">前言</h3>
<hr>
<p>   本文是这个系列第八篇，也是本系列的终章，它们是：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>《大模型基础补全计划(一)—重温一些深度学习相关的数学知识》 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/Iflyinsky/p/18717317">https://www.cnblogs.com/Iflyinsky/p/18717317</a></p>
</li>
<li class="lvl-2">
<p>《大模型基础补全计划(二)—词嵌入(word embedding) 》 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/Iflyinsky/p/18775451">https://www.cnblogs.com/Iflyinsky/p/18775451</a></p>
</li>
<li class="lvl-2">
<p>《大模型基础补全计划(三)—RNN实例与测试》 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/Iflyinsky/p/18967569">https://www.cnblogs.com/Iflyinsky/p/18967569</a></p>
</li>
<li class="lvl-2">
<p>《大模型基础补全计划(四)—LSTM的实例与测试(RNN的改进)》 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/Iflyinsky/p/19091089">https://www.cnblogs.com/Iflyinsky/p/19091089</a></p>
</li>
<li class="lvl-2">
<p>《大模型基础补全计划(五)—seq2seq实例与测试(编码器、解码器架构)》 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/Iflyinsky/p/19150535">https://www.cnblogs.com/Iflyinsky/p/19150535</a></p>
</li>
<li class="lvl-2">
<p>《大模型基础补全计划(六)—带注意力机制的seq2seq实例与测试(Bahdanau Attention)》 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/Iflyinsky/p/19184558">https://www.cnblogs.com/Iflyinsky/p/19184558</a></p>
</li>
<li class="lvl-2">
<p>《大模型基础补全计划(七)—Transformer(多头注意力、自注意力、位置编码)及实例与测试》<a target="_blank" rel="noopener" href="https://www.cnblogs.com/Iflyinsky/p/19228410">https://www.cnblogs.com/Iflyinsky/p/19228410</a></p>
</li>
</ul>
<p>  本文主要是用一个实际的大模型例子来联系和回顾之前的知识点，让大家能够感受一些，前面文中的一些知识点是真正用到了实际大模型里面的哪些地方。</p>
<p>  由于近期正在学习和应用的Qwen3-VL系列相关模型，因此这里挑了一个Qwen3-VL-2B-Instruct来独立分析，并联系和回顾之前的知识点。</p>
<p>  注意：本文不会详细介绍Qwen3-VL-2B-Instruct的推理过程及原理，如果想学习详细的技术原理，请忽略本文内容，并查看其它相关的文章。</p>
<br/>
<br/>
<br/>
<br/>
<h3 id="Qwen3-VL-2B-Instruct-简介">Qwen3-VL-2B-Instruct 简介</h3>
<hr>
<p>  </p>
<br/>
<br/>
<h5 id="下载及运行">下载及运行</h5>
<p>   首先qwen3-vl的官方工程是 <a target="_blank" rel="noopener" href="https://github.com/QwenLM/Qwen3-VL">https://github.com/QwenLM/Qwen3-VL</a> ，下面的官方示例的下载及变更推理代码（由于国内的原因，从魔塔下载）：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">modelscope download <span class="token parameter variable">--model</span> Qwen/Qwen3-VL-2B-Instruct  <span class="token parameter variable">--local_dir</span> ./cache<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModelForImageTextToText<span class="token punctuation">,</span> AutoProcessor

model_path <span class="token operator">=</span> <span class="token string">"./cache"</span>

<span class="token comment"># default: Load the model on the available device(s)</span>
model <span class="token operator">=</span> AutoModelForImageTextToText<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
    model_path<span class="token punctuation">,</span> cache_dir<span class="token operator">=</span>model_path<span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token string">"auto"</span><span class="token punctuation">,</span> device_map<span class="token operator">=</span><span class="token string">"auto"</span>
<span class="token punctuation">)</span>

<span class="token comment"># We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.</span>
<span class="token comment"># model = AutoModelForImageTextToText.from_pretrained(</span>
<span class="token comment">#     "Qwen/Qwen3-VL-235B-A22B-Instruct",</span>
<span class="token comment">#     dtype=torch.bfloat16,</span>
<span class="token comment">#     attn_implementation="flash_attention_2",</span>
<span class="token comment">#     device_map="auto",</span>
<span class="token comment"># )</span>

processor <span class="token operator">=</span> AutoProcessor<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_path<span class="token punctuation">,</span> cache_dir<span class="token operator">=</span>model_path<span class="token punctuation">)</span>

messages <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token punctuation">&#123;</span>
        <span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"user"</span><span class="token punctuation">,</span>
        <span class="token string">"content"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>
            <span class="token punctuation">&#123;</span>
                <span class="token string">"type"</span><span class="token punctuation">:</span> <span class="token string">"image"</span><span class="token punctuation">,</span>
                <span class="token string">"image"</span><span class="token punctuation">:</span> <span class="token string">"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg"</span><span class="token punctuation">,</span>
            <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>
            <span class="token punctuation">&#123;</span><span class="token string">"type"</span><span class="token punctuation">:</span> <span class="token string">"text"</span><span class="token punctuation">,</span> <span class="token string">"text"</span><span class="token punctuation">:</span> <span class="token string">"Describe this image."</span><span class="token punctuation">&#125;</span><span class="token punctuation">,</span>
        <span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">&#125;</span>
<span class="token punctuation">]</span>

<span class="token comment"># Preparation for inference</span>
inputs <span class="token operator">=</span> processor<span class="token punctuation">.</span>apply_chat_template<span class="token punctuation">(</span>
    messages<span class="token punctuation">,</span>
    tokenize<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    add_generation_prompt<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    return_dict<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    return_tensors<span class="token operator">=</span><span class="token string">"pt"</span>
<span class="token punctuation">)</span>
inputs <span class="token operator">=</span> inputs<span class="token punctuation">.</span>to<span class="token punctuation">(</span>model<span class="token punctuation">.</span>device<span class="token punctuation">)</span>

<span class="token comment"># Inference: Generation of the output</span>
generated_ids <span class="token operator">=</span> model<span class="token punctuation">.</span>generate<span class="token punctuation">(</span><span class="token operator">**</span>inputs<span class="token punctuation">,</span> max_new_tokens<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">)</span>
generated_ids_trimmed <span class="token operator">=</span> <span class="token punctuation">[</span>
    out_ids<span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>in_ids<span class="token punctuation">)</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token keyword">for</span> in_ids<span class="token punctuation">,</span> out_ids <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>inputs<span class="token punctuation">.</span>input_ids<span class="token punctuation">,</span> generated_ids<span class="token punctuation">)</span>
<span class="token punctuation">]</span>
output_text <span class="token operator">=</span> processor<span class="token punctuation">.</span>batch_decode<span class="token punctuation">(</span>
    generated_ids_trimmed<span class="token punctuation">,</span> skip_special_tokens<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> clean_up_tokenization_spaces<span class="token operator">=</span><span class="token boolean">False</span>
<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>output_text<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<br/>
<br/>
<br/>
<br/>
<h5 id="模型结构">模型结构</h5>
<p>   我们在上面的例子基础上，添加如下代码打印其模型结构：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">)</span>
vit_model <span class="token operator">=</span> model<span class="token punctuation">.</span>visual
llm_model <span class="token operator">=</span> model<span class="token punctuation">.</span>language_model <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>  得到的模型结构如下：</p>
<pre class="line-numbers language-none"><code class="language-none">Qwen3VLForConditionalGeneration(
  (model): Qwen3VLModel(
    (visual): Qwen3VLVisionModel(
      (patch_embed): Qwen3VLVisionPatchEmbed(
        (proj): Conv3d(3, 1024, kernel_size&#x3D;(2, 16, 16), stride&#x3D;(2, 16, 16))
      )
      (pos_embed): Embedding(2304, 1024)
      (rotary_pos_emb): Qwen3VLVisionRotaryEmbedding()
      (blocks): ModuleList(
        (0-23): 24 x Qwen3VLVisionBlock(
          (norm1): LayerNorm((1024,), eps&#x3D;1e-06, elementwise_affine&#x3D;True)
          (norm2): LayerNorm((1024,), eps&#x3D;1e-06, elementwise_affine&#x3D;True)
          (attn): Qwen3VLVisionAttention(
            (qkv): Linear(in_features&#x3D;1024, out_features&#x3D;3072, bias&#x3D;True)
            (proj): Linear(in_features&#x3D;1024, out_features&#x3D;1024, bias&#x3D;True)
          )
          (mlp): Qwen3VLVisionMLP(
            (linear_fc1): Linear(in_features&#x3D;1024, out_features&#x3D;4096, bias&#x3D;True)
            (linear_fc2): Linear(in_features&#x3D;4096, out_features&#x3D;1024, bias&#x3D;True)
            (act_fn): GELUTanh()
          )
        )
      )
      (merger): Qwen3VLVisionPatchMerger(
        (norm): LayerNorm((1024,), eps&#x3D;1e-06, elementwise_affine&#x3D;True)
        (linear_fc1): Linear(in_features&#x3D;4096, out_features&#x3D;4096, bias&#x3D;True)
        (act_fn): GELU(approximate&#x3D;&#39;none&#39;)
        (linear_fc2): Linear(in_features&#x3D;4096, out_features&#x3D;2048, bias&#x3D;True)
      )
      (deepstack_merger_list): ModuleList(
        (0-2): 3 x Qwen3VLVisionPatchMerger(
          (norm): LayerNorm((4096,), eps&#x3D;1e-06, elementwise_affine&#x3D;True)
          (linear_fc1): Linear(in_features&#x3D;4096, out_features&#x3D;4096, bias&#x3D;True)
          (act_fn): GELU(approximate&#x3D;&#39;none&#39;)
          (linear_fc2): Linear(in_features&#x3D;4096, out_features&#x3D;2048, bias&#x3D;True)
        )
      )
    )
    (language_model): Qwen3VLTextModel(
      (embed_tokens): Embedding(151936, 2048)
      (layers): ModuleList(
        (0-27): 28 x Qwen3VLTextDecoderLayer(
          (self_attn): Qwen3VLTextAttention(
            (q_proj): Linear(in_features&#x3D;2048, out_features&#x3D;2048, bias&#x3D;False)
            (k_proj): Linear(in_features&#x3D;2048, out_features&#x3D;1024, bias&#x3D;False)
            (v_proj): Linear(in_features&#x3D;2048, out_features&#x3D;1024, bias&#x3D;False)
            (o_proj): Linear(in_features&#x3D;2048, out_features&#x3D;2048, bias&#x3D;False)
            (q_norm): Qwen3VLTextRMSNorm((128,), eps&#x3D;1e-06)
            (k_norm): Qwen3VLTextRMSNorm((128,), eps&#x3D;1e-06)
          )
          (mlp): Qwen3VLTextMLP(
            (gate_proj): Linear(in_features&#x3D;2048, out_features&#x3D;6144, bias&#x3D;False)
            (up_proj): Linear(in_features&#x3D;2048, out_features&#x3D;6144, bias&#x3D;False)
            (down_proj): Linear(in_features&#x3D;6144, out_features&#x3D;2048, bias&#x3D;False)
            (act_fn): SiLUActivation()
          )
          (input_layernorm): Qwen3VLTextRMSNorm((2048,), eps&#x3D;1e-06)
          (post_attention_layernorm): Qwen3VLTextRMSNorm((2048,), eps&#x3D;1e-06)
        )
      )
      (norm): Qwen3VLTextRMSNorm((2048,), eps&#x3D;1e-06)
      (rotary_emb): Qwen3VLTextRotaryEmbedding()
    )
  )
  (lm_head): Linear(in_features&#x3D;2048, out_features&#x3D;151936, bias&#x3D;False)
)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>  从上面的模型结构来看，我们可以知道其分为两个部分，一个是visual，一个是language_model，这也是现在的视觉多模态的常见结构。</p>
<br/>
<br/>
<br/>
<br/>
<h3 id="Qwen3-VL-2B-Instruct-的模型结构简单分析-及-知识回顾">Qwen3-VL-2B-Instruct 的模型结构简单分析 及 知识回顾</h3>
<hr>
<p>  还记得我们前面的模型中的词表这个概念吗？当时的做法是直接将将整个训练用到的文字映射成对应的id，将所有的id组合在一起作为一个词表。在现在的大模型中，其实就有类似的东西，一般放在tokenizer.json文件里面。对于当前这个模型来说，这里有几个特殊的东西说明一下：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>以前文章中的&lt;bos&gt;/&lt;eos&gt;对应的是当前这个模型的&lt;|im_start|&gt;/&lt;|im_end|&gt;</p>
</li>
<li class="lvl-2">
<p>由于是视觉多模态模型，当前这个模型还会有几个本文会用到的特殊token：&lt;|vision_start|&gt;/&lt;|image_pad|&gt;/&lt;|vision_end|&gt;，他们是用来描述一张图怎么被输入到大语言模型中被理解的。</p>
</li>
<li class="lvl-2">
<p>一个token不一定对应一个文字，可能对应多个、或者零点几个字，感兴趣可以私下了解一下，其和文字编码有关系。</p>
</li>
</ul>
<p>  当上文的 processor.apply_chat_template执行后，然后得到的inputs会有如下四个内容：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>input_ids （做完tokenizer之后的输出，已经将输入的文字“Describe this image.”和图片占位符“&lt;|vision_start|&gt;&lt;|image_pad|&gt;*N&lt;|vision_end|&gt;”转换为了对应的token id）</p>
</li>
<li class="lvl-2">
<p>attention_mask （input_ids的掩码，用于屏蔽无效或者pad输入序列）</p>
</li>
<li class="lvl-2">
<p>pixel_values （图片预处理好的矩阵，不仅仅做了归一化，还做了分patch操作，本文不用太关注）</p>
</li>
<li class="lvl-2">
<p>image_grid_thw （本文用不上，别管。）</p>
</li>
</ul>
<p>  对于input_ids来说，我们知道里面有图片的占位符的token_id，这里后面会替换为真实的图像数据，这样才能把图、文字送入到大语言模型，当然，语音等也是一样的。</p>
<p>  我们首先来看看上文model.generate调用之后发生了什么，他会经过一系列变化后，到达如下的Qwen3VLModel的forward的入口：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>
    self<span class="token punctuation">,</span>
    input_ids<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>LongTensor <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    attention_mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    position_ids<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    past_key_values<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Cache<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    inputs_embeds<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    pixel_values<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    pixel_values_videos<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    image_grid_thw<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    video_grid_thw<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    cache_position<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    <span class="token operator">**</span>kwargs<span class="token punctuation">:</span> Unpack<span class="token punctuation">[</span>TransformersKwargs<span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> Union<span class="token punctuation">[</span><span class="token builtin">tuple</span><span class="token punctuation">,</span> Qwen3VLModelOutputWithPast<span class="token punctuation">]</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">r"""
    image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):
        The temporal, height and width of feature shape of each image in LLM.
    video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):
        The temporal, height and width of feature shape of each video in LLM.
    """</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>input_ids <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">)</span> <span class="token operator">^</span> <span class="token punctuation">(</span>inputs_embeds <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"You must specify exactly one of input_ids or inputs_embeds"</span><span class="token punctuation">)</span>

    <span class="token keyword">if</span> inputs_embeds <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        inputs_embeds <span class="token operator">=</span> self<span class="token punctuation">.</span>get_input_embeddings<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span>

    image_mask <span class="token operator">=</span> <span class="token boolean">None</span>
    video_mask <span class="token operator">=</span> <span class="token boolean">None</span>

    <span class="token keyword">if</span> pixel_values <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        image_embeds<span class="token punctuation">,</span> deepstack_image_embeds <span class="token operator">=</span> self<span class="token punctuation">.</span>get_image_features<span class="token punctuation">(</span>pixel_values<span class="token punctuation">,</span> image_grid_thw<span class="token punctuation">)</span>
        image_embeds <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>image_embeds<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>inputs_embeds<span class="token punctuation">.</span>device<span class="token punctuation">,</span> inputs_embeds<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>
        image_mask<span class="token punctuation">,</span> _ <span class="token operator">=</span> self<span class="token punctuation">.</span>get_placeholder_mask<span class="token punctuation">(</span>
            input_ids<span class="token punctuation">,</span> inputs_embeds<span class="token operator">=</span>inputs_embeds<span class="token punctuation">,</span> image_features<span class="token operator">=</span>image_embeds
        <span class="token punctuation">)</span>
        inputs_embeds <span class="token operator">=</span> inputs_embeds<span class="token punctuation">.</span>masked_scatter<span class="token punctuation">(</span>image_mask<span class="token punctuation">,</span> image_embeds<span class="token punctuation">)</span>

    <span class="token keyword">if</span> pixel_values_videos <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        video_embeds<span class="token punctuation">,</span> deepstack_video_embeds <span class="token operator">=</span> self<span class="token punctuation">.</span>get_video_features<span class="token punctuation">(</span>pixel_values_videos<span class="token punctuation">,</span> video_grid_thw<span class="token punctuation">)</span>
        video_embeds <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>video_embeds<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>inputs_embeds<span class="token punctuation">.</span>device<span class="token punctuation">,</span> inputs_embeds<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>
        _<span class="token punctuation">,</span> video_mask <span class="token operator">=</span> self<span class="token punctuation">.</span>get_placeholder_mask<span class="token punctuation">(</span>
            input_ids<span class="token punctuation">,</span> inputs_embeds<span class="token operator">=</span>inputs_embeds<span class="token punctuation">,</span> video_features<span class="token operator">=</span>video_embeds
        <span class="token punctuation">)</span>
        inputs_embeds <span class="token operator">=</span> inputs_embeds<span class="token punctuation">.</span>masked_scatter<span class="token punctuation">(</span>video_mask<span class="token punctuation">,</span> video_embeds<span class="token punctuation">)</span>

    visual_pos_masks <span class="token operator">=</span> <span class="token boolean">None</span>
    deepstack_visual_embeds <span class="token operator">=</span> <span class="token boolean">None</span>
    <span class="token keyword">if</span> image_mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">and</span> video_mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        <span class="token comment"># aggregate visual_pos_masks and deepstack_visual_embeds</span>
        image_mask <span class="token operator">=</span> image_mask<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>
        video_mask <span class="token operator">=</span> video_mask<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>
        visual_pos_masks <span class="token operator">=</span> image_mask <span class="token operator">|</span> video_mask
        deepstack_visual_embeds <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        image_mask_joint <span class="token operator">=</span> image_mask<span class="token punctuation">[</span>visual_pos_masks<span class="token punctuation">]</span>
        video_mask_joint <span class="token operator">=</span> video_mask<span class="token punctuation">[</span>visual_pos_masks<span class="token punctuation">]</span>
        <span class="token keyword">for</span> img_embed<span class="token punctuation">,</span> vid_embed <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>deepstack_image_embeds<span class="token punctuation">,</span> deepstack_video_embeds<span class="token punctuation">)</span><span class="token punctuation">:</span>
            embed_joint <span class="token operator">=</span> img_embed<span class="token punctuation">.</span>new_zeros<span class="token punctuation">(</span>visual_pos_masks<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> img_embed<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>img_embed<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
            embed_joint<span class="token punctuation">[</span>image_mask_joint<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> img_embed
            embed_joint<span class="token punctuation">[</span>video_mask_joint<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> vid_embed
            deepstack_visual_embeds<span class="token punctuation">.</span>append<span class="token punctuation">(</span>embed_joint<span class="token punctuation">)</span>
    <span class="token keyword">elif</span> image_mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        image_mask <span class="token operator">=</span> image_mask<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>
        visual_pos_masks <span class="token operator">=</span> image_mask
        deepstack_visual_embeds <span class="token operator">=</span> deepstack_image_embeds
    <span class="token keyword">elif</span> video_mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        video_mask <span class="token operator">=</span> video_mask<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>
        visual_pos_masks <span class="token operator">=</span> video_mask
        deepstack_visual_embeds <span class="token operator">=</span> deepstack_video_embeds

    <span class="token keyword">if</span> position_ids <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        attention_mask_tensor <span class="token operator">=</span> <span class="token punctuation">(</span>
            attention_mask <span class="token keyword">if</span> <span class="token keyword">not</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>attention_mask<span class="token punctuation">,</span> <span class="token builtin">dict</span><span class="token punctuation">)</span> <span class="token keyword">else</span> attention_mask<span class="token punctuation">[</span><span class="token string">"full_attention"</span><span class="token punctuation">]</span>
        <span class="token punctuation">)</span>
        <span class="token keyword">if</span> attention_mask_tensor <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">and</span> attention_mask_tensor<span class="token punctuation">.</span>ndim <span class="token operator">==</span> <span class="token number">4</span><span class="token punctuation">:</span>
            attention_mask_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>diagonal<span class="token punctuation">(</span>attention_mask_tensor<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dim1<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> dim2<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
            <span class="token comment"># Only apply conversion for floating point tensors (inverted masks)</span>
            <span class="token keyword">if</span> attention_mask_tensor<span class="token punctuation">.</span>dtype<span class="token punctuation">.</span>is_floating_point<span class="token punctuation">:</span>
                attention_mask_tensor <span class="token operator">=</span> attention_mask_tensor <span class="token operator">/</span> torch<span class="token punctuation">.</span>finfo<span class="token punctuation">(</span>attention_mask_tensor<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">min</span>
                attention_mask_tensor <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">-</span> attention_mask_tensor<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">int</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token comment"># Calculate RoPE index once per generation in the pre-fill stage only.</span>
        <span class="token comment"># When compiling, we can't check tensor values thus we check only input length</span>
        <span class="token comment"># It is safe to assume that `length!=1` means we're in pre-fill because compiled</span>
        <span class="token comment"># models currently cannot do asssisted decoding</span>
        prefill_compiled_stage <span class="token operator">=</span> is_torchdynamo_compiling<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">and</span> <span class="token punctuation">(</span>
            <span class="token punctuation">(</span>input_ids <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">and</span> input_ids<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">!=</span> <span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token keyword">or</span> <span class="token punctuation">(</span>inputs_embeds <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">and</span> inputs_embeds<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">!=</span> <span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
        prefill_noncompiled_stage <span class="token operator">=</span> <span class="token keyword">not</span> is_torchdynamo_compiling<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">and</span> <span class="token punctuation">(</span>
            <span class="token punctuation">(</span>cache_position <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">and</span> cache_position<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span>
            <span class="token keyword">or</span> <span class="token punctuation">(</span>past_key_values <span class="token keyword">is</span> <span class="token boolean">None</span> <span class="token keyword">or</span> past_key_values<span class="token punctuation">.</span>get_seq_length<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
        <span class="token keyword">if</span> <span class="token punctuation">(</span>prefill_compiled_stage <span class="token keyword">or</span> prefill_noncompiled_stage<span class="token punctuation">)</span> <span class="token keyword">or</span> self<span class="token punctuation">.</span>rope_deltas <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            position_ids<span class="token punctuation">,</span> rope_deltas <span class="token operator">=</span> self<span class="token punctuation">.</span>get_rope_index<span class="token punctuation">(</span>
                input_ids<span class="token punctuation">,</span>
                image_grid_thw<span class="token punctuation">,</span>
                video_grid_thw<span class="token punctuation">,</span>
                attention_mask<span class="token operator">=</span>attention_mask_tensor<span class="token punctuation">,</span>
            <span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>rope_deltas <span class="token operator">=</span> rope_deltas
        <span class="token comment"># then use the prev pre-calculated rope-deltas to get the correct position ids</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            batch_size<span class="token punctuation">,</span> seq_length<span class="token punctuation">,</span> _ <span class="token operator">=</span> inputs_embeds<span class="token punctuation">.</span>shape
            delta <span class="token operator">=</span> <span class="token punctuation">(</span>
                <span class="token punctuation">(</span>cache_position<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>rope_deltas<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>inputs_embeds<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
                <span class="token keyword">if</span> cache_position <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span>
                <span class="token keyword">else</span> <span class="token number">0</span>
            <span class="token punctuation">)</span>
            position_ids <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>seq_length<span class="token punctuation">,</span> device<span class="token operator">=</span>inputs_embeds<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
            position_ids <span class="token operator">=</span> position_ids<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token keyword">if</span> cache_position <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>  <span class="token comment"># otherwise `deltas` is an int `0`</span>
                delta <span class="token operator">=</span> delta<span class="token punctuation">.</span>repeat_interleave<span class="token punctuation">(</span>batch_size <span class="token operator">//</span> delta<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
            position_ids <span class="token operator">=</span> position_ids<span class="token punctuation">.</span>add<span class="token punctuation">(</span>delta<span class="token punctuation">)</span>
            position_ids <span class="token operator">=</span> position_ids<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

    outputs <span class="token operator">=</span> self<span class="token punctuation">.</span>language_model<span class="token punctuation">(</span>
        input_ids<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        position_ids<span class="token operator">=</span>position_ids<span class="token punctuation">,</span>
        attention_mask<span class="token operator">=</span>attention_mask<span class="token punctuation">,</span>
        past_key_values<span class="token operator">=</span>past_key_values<span class="token punctuation">,</span>
        inputs_embeds<span class="token operator">=</span>inputs_embeds<span class="token punctuation">,</span>
        cache_position<span class="token operator">=</span>cache_position<span class="token punctuation">,</span>
        visual_pos_masks<span class="token operator">=</span>visual_pos_masks<span class="token punctuation">,</span>
        deepstack_visual_embeds<span class="token operator">=</span>deepstack_visual_embeds<span class="token punctuation">,</span>
        <span class="token operator">**</span>kwargs<span class="token punctuation">,</span>
    <span class="token punctuation">)</span>

    <span class="token keyword">return</span> Qwen3VLModelOutputWithPast<span class="token punctuation">(</span>
        last_hidden_state<span class="token operator">=</span>outputs<span class="token punctuation">.</span>last_hidden_state<span class="token punctuation">,</span>
        past_key_values<span class="token operator">=</span>outputs<span class="token punctuation">.</span>past_key_values<span class="token punctuation">,</span>
        rope_deltas<span class="token operator">=</span>self<span class="token punctuation">.</span>rope_deltas<span class="token punctuation">,</span>
    <span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>   看上面的代码，我们来看看 input_ids 中的主要的几个数据分别做了什么：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>input_ids 通过get_input_embeddings获取了input_ids对应的原始inputs_embeds，这一步和我们以前文章中做embedding是一样的。唯一注意的，这里的embedding向量里面包含&lt;|image_pad|&gt;对应的嵌入向量，是占位的，后面要替换为真实的数据。</p>
</li>
<li class="lvl-2">
<p>pixel_values 通过get_image_features获取了图像数据对应的image_embeds，这里对应Qwen3VLVisionModel的推理过程，下面会简单说明一下。</p>
</li>
<li class="lvl-2">
<p>在masked_scatter中，将inputs_embeds中的占位向量替换为image_embeds。</p>
</li>
<li class="lvl-2">
<p>根据输入的inputs_embeds，获取token对应的position_ids，也就是获取位置信息，在前面的文中提到了为什么transformer需要位置信息。</p>
</li>
<li class="lvl-2">
<p>将最终的position_ids，attention_mask，inputs_embeds，past_key_values（此项内容在下文解释）给Qwen3VLTextModel进行推理得到logits序列</p>
</li>
<li class="lvl-2">
<p>然后将logits按采样参数进行采样，得到最终的输出的文字token,然后进行tokenizer解码，得到最终输出的文字。(此部分不在上面所在代码范围内部，但是是大模型的后处理部分的必要逻辑部分。)</p>
</li>
</ul>
<p>   我们从上文已经知道，其模型分为两个部分，下面分别简单介绍这两部分的forward过程，看看我们之前提到的知识点在真实的多模态大模型中是怎么样的存在。</p>
<br/>
<br/>
<h5 id="visual-部分简单分析">visual 部分简单分析</h5>
<p>  本系列文章严格来说是不应该涉及到多模态大模型的，但是现在常见的多模态大模型应用场景已经逐渐扩大，因此这里用视觉多模态大模型为例子，看看视觉多模态大模型和普通的大模型有什么区别，首先visual部分的forward代码如下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> hidden_states<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> grid_thw<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Args:
        hidden_states (`torch.Tensor` of shape `(seq_len, hidden_size)`):
            The final hidden states of the model.
        grid_thw (`torch.Tensor` of shape `(num_images_or_videos, 3)`):
            The temporal, height and width of feature shape of each image in LLM.

    Returns:
        `torch.Tensor`: hidden_states.
    """</span>
    hidden_states <span class="token operator">=</span> self<span class="token punctuation">.</span>patch_embed<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>

    pos_embeds <span class="token operator">=</span> self<span class="token punctuation">.</span>fast_pos_embed_interpolate<span class="token punctuation">(</span>grid_thw<span class="token punctuation">)</span>
    hidden_states <span class="token operator">=</span> hidden_states <span class="token operator">+</span> pos_embeds

    rotary_pos_emb <span class="token operator">=</span> self<span class="token punctuation">.</span>rot_pos_emb<span class="token punctuation">(</span>grid_thw<span class="token punctuation">)</span>

    seq_len<span class="token punctuation">,</span> _ <span class="token operator">=</span> hidden_states<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>
    hidden_states <span class="token operator">=</span> hidden_states<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>seq_len<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    rotary_pos_emb <span class="token operator">=</span> rotary_pos_emb<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>seq_len<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    emb <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>rotary_pos_emb<span class="token punctuation">,</span> rotary_pos_emb<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    position_embeddings <span class="token operator">=</span> <span class="token punctuation">(</span>emb<span class="token punctuation">.</span>cos<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> emb<span class="token punctuation">.</span>sin<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    cu_seqlens <span class="token operator">=</span> torch<span class="token punctuation">.</span>repeat_interleave<span class="token punctuation">(</span>grid_thw<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">*</span> grid_thw<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> grid_thw<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cumsum<span class="token punctuation">(</span>
        dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>
        <span class="token comment"># Select dtype based on the following factors:</span>
        <span class="token comment">#  - FA2 requires that cu_seqlens_q must have dtype int32</span>
        <span class="token comment">#  - torch.onnx.export requires that cu_seqlens_q must have same dtype as grid_thw</span>
        <span class="token comment"># See https://github.com/huggingface/transformers/pull/34852 for more information</span>
        dtype<span class="token operator">=</span>grid_thw<span class="token punctuation">.</span>dtype <span class="token keyword">if</span> torch<span class="token punctuation">.</span>jit<span class="token punctuation">.</span>is_tracing<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> torch<span class="token punctuation">.</span>int32<span class="token punctuation">,</span>
    <span class="token punctuation">)</span>
    cu_seqlens <span class="token operator">=</span> F<span class="token punctuation">.</span>pad<span class="token punctuation">(</span>cu_seqlens<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> value<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>

    deepstack_feature_lists <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> layer_num<span class="token punctuation">,</span> blk <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>blocks<span class="token punctuation">)</span><span class="token punctuation">:</span>
        hidden_states <span class="token operator">=</span> blk<span class="token punctuation">(</span>
            hidden_states<span class="token punctuation">,</span>
            cu_seqlens<span class="token operator">=</span>cu_seqlens<span class="token punctuation">,</span>
            position_embeddings<span class="token operator">=</span>position_embeddings<span class="token punctuation">,</span>
            <span class="token operator">**</span>kwargs<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
        <span class="token keyword">if</span> layer_num <span class="token keyword">in</span> self<span class="token punctuation">.</span>deepstack_visual_indexes<span class="token punctuation">:</span>
            deepstack_feature <span class="token operator">=</span> self<span class="token punctuation">.</span>deepstack_merger_list<span class="token punctuation">[</span>self<span class="token punctuation">.</span>deepstack_visual_indexes<span class="token punctuation">.</span>index<span class="token punctuation">(</span>layer_num<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">(</span>
                hidden_states
            <span class="token punctuation">)</span>
            deepstack_feature_lists<span class="token punctuation">.</span>append<span class="token punctuation">(</span>deepstack_feature<span class="token punctuation">)</span>

    hidden_states <span class="token operator">=</span> self<span class="token punctuation">.</span>merger<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>

    <span class="token keyword">return</span> hidden_states<span class="token punctuation">,</span> deepstack_feature_lists<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>  由于本文并不是要细节介绍这个模型的结构，因此这里我们只需要知道其输入是：预处理好的图片数据+grid_thw，其输出是：hidden_states+deepstack_feature_lists。其中最重要的就是输出的hidden_states，它含义是图片token的ebedding向量矩阵，在上面已经提到了其作用。</p>
<br/>
<br/>
<br/>
<br/>
<h5 id="language-model-部分分析">language_model 部分分析</h5>
<p>  对于语言模型部分来说，这个部分才是和我们前面训练的模型比较像的，下面我们先来看看其forward过程：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>
    self<span class="token punctuation">,</span>
    input_ids<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    attention_mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    position_ids<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    past_key_values<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Cache<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    inputs_embeds<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    use_cache<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    cache_position<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    <span class="token comment"># args for deepstack</span>
    visual_pos_masks<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    deepstack_visual_embeds<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">list</span><span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    <span class="token operator">**</span>kwargs<span class="token punctuation">:</span> Unpack<span class="token punctuation">[</span>FlashAttentionKwargs<span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> Union<span class="token punctuation">[</span><span class="token builtin">tuple</span><span class="token punctuation">,</span> BaseModelOutputWithPast<span class="token punctuation">]</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">r"""
    visual_pos_masks (`torch.Tensor` of shape `(batch_size, seqlen)`, *optional*):
        The mask of the visual positions.
    deepstack_visual_embeds (`list[torch.Tensor]`, *optional*):
        The deepstack visual embeddings. The shape is (num_layers, visual_seqlen, embed_dim).
        The feature is extracted from the different visual encoder layers, and fed to the decoder
        hidden states. It's from the paper DeepStack(https://arxiv.org/abs/2406.04334).
    """</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>input_ids <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">)</span> <span class="token operator">^</span> <span class="token punctuation">(</span>inputs_embeds <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"You must specify exactly one of input_ids or inputs_embeds"</span><span class="token punctuation">)</span>

    <span class="token comment"># torch.jit.trace() doesn't support cache objects in the output</span>
    <span class="token keyword">if</span> use_cache <span class="token keyword">and</span> past_key_values <span class="token keyword">is</span> <span class="token boolean">None</span> <span class="token keyword">and</span> <span class="token keyword">not</span> torch<span class="token punctuation">.</span>jit<span class="token punctuation">.</span>is_tracing<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        past_key_values <span class="token operator">=</span> DynamicCache<span class="token punctuation">(</span>config<span class="token operator">=</span>self<span class="token punctuation">.</span>config<span class="token punctuation">)</span>

    <span class="token keyword">if</span> inputs_embeds <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        inputs_embeds <span class="token operator">=</span> self<span class="token punctuation">.</span>embed_tokens<span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span>

    <span class="token keyword">if</span> cache_position <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        past_seen_tokens <span class="token operator">=</span> past_key_values<span class="token punctuation">.</span>get_seq_length<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> past_key_values <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">else</span> <span class="token number">0</span>
        cache_position <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>
            past_seen_tokens<span class="token punctuation">,</span> past_seen_tokens <span class="token operator">+</span> inputs_embeds<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span>inputs_embeds<span class="token punctuation">.</span>device
        <span class="token punctuation">)</span>

    <span class="token comment"># the hard coded `3` is for temporal, height and width.</span>
    <span class="token keyword">if</span> position_ids <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        position_ids <span class="token operator">=</span> cache_position<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> inputs_embeds<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token keyword">elif</span> position_ids<span class="token punctuation">.</span>ndim <span class="token operator">==</span> <span class="token number">2</span><span class="token punctuation">:</span>
        position_ids <span class="token operator">=</span> position_ids<span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> position_ids<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token keyword">if</span> position_ids<span class="token punctuation">.</span>ndim <span class="token operator">==</span> <span class="token number">3</span> <span class="token keyword">and</span> position_ids<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">4</span><span class="token punctuation">:</span>
        text_position_ids <span class="token operator">=</span> position_ids<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        position_ids <span class="token operator">=</span> position_ids<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        text_position_ids <span class="token operator">=</span> position_ids<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>

    attention_mask <span class="token operator">=</span> create_causal_mask<span class="token punctuation">(</span>
        config<span class="token operator">=</span>self<span class="token punctuation">.</span>config<span class="token punctuation">,</span>
        input_embeds<span class="token operator">=</span>inputs_embeds<span class="token punctuation">,</span>
        attention_mask<span class="token operator">=</span>attention_mask<span class="token punctuation">,</span>
        cache_position<span class="token operator">=</span>cache_position<span class="token punctuation">,</span>
        past_key_values<span class="token operator">=</span>past_key_values<span class="token punctuation">,</span>
        position_ids<span class="token operator">=</span>text_position_ids<span class="token punctuation">,</span>
    <span class="token punctuation">)</span>

    hidden_states <span class="token operator">=</span> inputs_embeds

    <span class="token comment"># create position embeddings to be shared across the decoder layers</span>
    position_embeddings <span class="token operator">=</span> self<span class="token punctuation">.</span>rotary_emb<span class="token punctuation">(</span>hidden_states<span class="token punctuation">,</span> position_ids<span class="token punctuation">)</span>

    <span class="token comment"># decoder layers</span>
    <span class="token keyword">for</span> layer_idx<span class="token punctuation">,</span> decoder_layer <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>layers<span class="token punctuation">)</span><span class="token punctuation">:</span>
        layer_outputs <span class="token operator">=</span> decoder_layer<span class="token punctuation">(</span>
            hidden_states<span class="token punctuation">,</span>
            attention_mask<span class="token operator">=</span>attention_mask<span class="token punctuation">,</span>
            position_ids<span class="token operator">=</span>text_position_ids<span class="token punctuation">,</span>
            past_key_values<span class="token operator">=</span>past_key_values<span class="token punctuation">,</span>
            cache_position<span class="token operator">=</span>cache_position<span class="token punctuation">,</span>
            position_embeddings<span class="token operator">=</span>position_embeddings<span class="token punctuation">,</span>
            <span class="token operator">**</span>kwargs<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
        hidden_states <span class="token operator">=</span> layer_outputs

        <span class="token comment"># add visual features to the hidden states of first several layers</span>
        <span class="token keyword">if</span> deepstack_visual_embeds <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">and</span> layer_idx <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>deepstack_visual_embeds<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            hidden_states <span class="token operator">=</span> self<span class="token punctuation">.</span>_deepstack_process<span class="token punctuation">(</span>
                hidden_states<span class="token punctuation">,</span>
                visual_pos_masks<span class="token punctuation">,</span>
                deepstack_visual_embeds<span class="token punctuation">[</span>layer_idx<span class="token punctuation">]</span><span class="token punctuation">,</span>
            <span class="token punctuation">)</span>

    hidden_states <span class="token operator">=</span> self<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>

    <span class="token keyword">return</span> BaseModelOutputWithPast<span class="token punctuation">(</span>
        last_hidden_state<span class="token operator">=</span>hidden_states<span class="token punctuation">,</span>
        past_key_values<span class="token operator">=</span>past_key_values<span class="token punctuation">,</span>
    <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>  我们看到了将position_ids，attention_mask，inputs_embeds，past_key_values传入推理过程后，得到了两个重要的内容，一个logits，一个past_key_values，下面重点介绍一下这两个是什么：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>logits 输出的是一次推理后，词表大小的一个概率矩阵，然后根据我们的采样相关参数（例如我们常见的：Temperature/Top P/Frequency Penalty等就是在这一阶段生效），选择对应的token_id，然后转换为文字。</p>
</li>
<li class="lvl-2">
<p>past_key_values 保存的是每一层decoder layer的注意力机制里面的K/V内容，也就是我们常见的KV Cache一词的存在的地方。</p>
</li>
</ul>
<p>  最后我们来看看现在常见的KV cache（缓存命中、缓存未命中）到底意味着什么?我们举一个简单直观的例子：我们保存了“你好”的KV cache，那我们再一次推理“你好世界。”，那么我们可以直接使用“你好”的KV cache，不用重复计算前面部分，可以直接计算新的部分，加快推理速度、减少了计算资源使用。</p>
<br/>
<br/>
<br/>
<br/>
<h3 id="后记">后记</h3>
<hr>
<p>  本文基于Qwen3-VL-2B-Instruct，回顾了之前的一些知识，从这里我们可以看到，当前大模型里面用到的好多知识点，其实都来自于以前的某个地方。</p>
<p>  本系列到此，完结散花。</p>
<h3 id="参考文献">参考文献</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/QwenLM/Qwen3-VL">https://github.com/QwenLM/Qwen3-VL</a></p>
</li>
</ul>
<br/>
<br/>
<div style="margin:50px auto;">
</div>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <hr/>
        <center><font color = #91e0b0 size = 5>打赏、订阅、收藏、丢香蕉、硬币，请关注公众号（攻城狮的搬砖之路）</font></center>
    </div>
</div>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/qrcode_for_wx_official_account.jpg" alt="qrc_img"/></center>
    </div>
</div>
<!-- ![alt 公众号图片](https://flyinskyin2013.github.io/ImageBed0/blogs/qrcode_for_wx_official_account.jpg "公众号图片") -->
<p><font color="red" size="7">PS: 请尊重原创，不喜勿喷。</font><br/><br>
<font color="red" size="7">PS: 要转载请注明出处，本人版权所有。</font><br/><br>
<font color="red" size="7">PS: 有问题请留言，看到后我会第一时间回复。</font><br/></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://e-x.top/2025/11/16/blog_idx_144/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Sky">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sky's Blogs">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | Sky's Blogs">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/11/16/blog_idx_144/" class="post-title-link" itemprop="url">大模型基础补全计划(七)---Transformer(多头注意力、自注意力、位置编码)及实例与测试</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-11-16 16:27:00 / 修改时间：16:34:27" itemprop="dateCreated datePublished" datetime="2025-11-16T16:27:00+08:00">2025-11-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/LLM/" itemprop="url" rel="index"><span itemprop="name">LLM</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/LLM/LM/" itemprop="url" rel="index"><span itemprop="name">LM</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>6.3k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>23 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <script src="\assets\js\APlayer.min.js"> </script><!--
 * @Description: 
 * @Author: Sky
 * @Date: 2020-08-24 16:37:34
 * @LastEditors: Sky
 * @LastEditTime: 2021-06-29 15:04:48
 * @Github: https://github.com/flyinskyin2013/
-->
<p><font color="red" size="7">PS：要转载请注明出处，本人版权所有。</font><br/><br>
<font color="red" size="7">PS: 这个只是基于《我自己》的理解，</font><br/><font color="red" size="7">如果和你的原则及想法相冲突，请谅解，勿喷。</font><br/></p>
<!-- ###### 前置说明
&emsp;&emsp;本文作为本人github blog/cnblog的主站的备份。（BlogID=144） 
&emsp;&emsp;本文发布于 2025-11-16 16:27:00             （BlogID=144） 
-->
<h6 id="环境说明">环境说明</h6>
<p>  无</p>
<h3 id="前言">前言</h3>
<hr>
<p>   本文是这个系列第七篇，它们是：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>《大模型基础补全计划(一)—重温一些深度学习相关的数学知识》 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/Iflyinsky/p/18717317">https://www.cnblogs.com/Iflyinsky/p/18717317</a></p>
</li>
<li class="lvl-2">
<p>《大模型基础补全计划(二)—词嵌入(word embedding) 》 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/Iflyinsky/p/18775451">https://www.cnblogs.com/Iflyinsky/p/18775451</a></p>
</li>
<li class="lvl-2">
<p>《大模型基础补全计划(三)—RNN实例与测试》 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/Iflyinsky/p/18967569">https://www.cnblogs.com/Iflyinsky/p/18967569</a></p>
</li>
<li class="lvl-2">
<p>《大模型基础补全计划(四)—LSTM的实例与测试(RNN的改进)》 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/Iflyinsky/p/19091089">https://www.cnblogs.com/Iflyinsky/p/19091089</a></p>
</li>
<li class="lvl-2">
<p>《大模型基础补全计划(五)—seq2seq实例与测试(编码器、解码器架构)》 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/Iflyinsky/p/19150535">https://www.cnblogs.com/Iflyinsky/p/19150535</a></p>
</li>
<li class="lvl-2">
<p>《大模型基础补全计划(六)—带注意力机制的seq2seq实例与测试(Bahdanau Attention)》 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/Iflyinsky/p/19184558">https://www.cnblogs.com/Iflyinsky/p/19184558</a></p>
</li>
</ul>
<p>  本文的核心是介绍transformer模型结构，下面是transformer的网络结构示意图（图来源：见参考文献部分）。</p>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/blog_idx_144/transformer.png" alt="rep_img"/></center>
    </div>
</div>   
<p>  从上面的架构图可以知道，在开始介绍之前，需要提前介绍多头注意力、自注意力、位置编码等前置知识。</p>
<br/>
<br/>
<br/>
<br/>
<h3 id="点积注意力与自注意力">点积注意力与自注意力</h3>
<hr>
<p>   首先我们来介绍一种新的注意力评分方式，点积注意力，其计算公式是：$$\text{Attention}(Q, K, V) = \text{Softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right) V$$。</p>
<p>   回到前面文章中的seq2seq中的注意力机制（一种加法注意力评分方式），其KV来自于encoder的output，Q来自于decoder的隐藏态。这个时候，我们假设一下，如果QKV都是同一种数据，那么每一次Q，都会输出对整个KV（也就是Q本身）的注意力，这种特殊的注意力被称为自注意力。</p>
<p>   下面是点积注意力的代码，当QKV都是同一个输入时，下面的注意力就是自注意力。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">DotProductAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""Scaled dot product attention."""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>

    <span class="token comment"># Shape of queries: (batch_size, no. of queries, d)</span>
    <span class="token comment"># Shape of keys: (batch_size, no. of key-value pairs, d)</span>
    <span class="token comment"># Shape of values: (batch_size, no. of key-value pairs, value dimension)</span>
    <span class="token comment"># Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> queries<span class="token punctuation">,</span> keys<span class="token punctuation">,</span> values<span class="token punctuation">,</span> valid_lens<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        d <span class="token operator">=</span> queries<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>
        <span class="token comment"># Swap the last two dimensions of keys with keys.transpose(1, 2)</span>
        scores <span class="token operator">=</span> torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>queries<span class="token punctuation">,</span> keys<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>d<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>attention_weights <span class="token operator">=</span> masked_softmax<span class="token punctuation">(</span>scores<span class="token punctuation">,</span> valid_lens<span class="token punctuation">)</span>
        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>self<span class="token punctuation">.</span>attention_weights<span class="token punctuation">)</span><span class="token punctuation">,</span> values<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<br/>
<br/>
<br/>
<br/>
<h3 id="位置编码">位置编码</h3>
<hr>
<p>   我们知道，我们的序列数据中的每个数据都是在序列中有位置信息的，根据点积注意力的并行计算的实现，我们知道每个序列数据在同一时间进行了运算，没有序列之间的顺序信息。为了让我们的并行计算过程中，让模型感受到序列的顺序信息，因此我们需要在输入数据中含有位置信息，因此有人设计了位置编码。其代码实现如下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">PositionalEncoding</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""Positional encoding."""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> dropout<span class="token punctuation">,</span> max_len<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>
        <span class="token comment"># Create a long enough P</span>
        self<span class="token punctuation">.</span>P <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> max_len<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">)</span><span class="token punctuation">)</span>
        X <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>max_len<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>
            <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">/</span> torch<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span><span class="token number">10000</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>
            <span class="token number">0</span><span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span> <span class="token operator">/</span> num_hiddens<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>P<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>P<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>cos<span class="token punctuation">(</span>X<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>
        X <span class="token operator">=</span> X <span class="token operator">+</span> self<span class="token punctuation">.</span>P<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>X<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>   当我们的序列数据经过了位置编码后，在进行点积注意力计算时，我们的输入数据有了顺序信息，会让我们的模型学习到序列顺序相关的信息。</p>
<br/>
<br/>
<br/>
<br/>
<h3 id="多头注意力">多头注意力</h3>
<hr>
<p>   注意力机制已经可以对一个数据进行有侧重的关注。但是我们希望的是，注意力机制可以对数据的多个维度的侧重关注，因为我们的数据有很多的不同维度的属性信息。例如：一句英文，其有语法信息、有语境信息、有单词之间的信息等等。</p>
<p>   基于这里提到的问题，有人提出了多头注意力机制。从上面的介绍来看，很好理解这个机制，就是每个头单独分析数据的属性，这样我们可以同时关注数据的多个维度的属性，提升我们的模型的理解能力。</p>
<p>   其代码实现如下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MultiHeadAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""Multi-head attention."""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> dropout<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>num_heads <span class="token operator">=</span> num_heads
        self<span class="token punctuation">.</span>attention <span class="token operator">=</span> DotProductAttention<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>W_q <span class="token operator">=</span> nn<span class="token punctuation">.</span>LazyLinear<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> bias<span class="token operator">=</span>bias<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>W_k <span class="token operator">=</span> nn<span class="token punctuation">.</span>LazyLinear<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> bias<span class="token operator">=</span>bias<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>W_v <span class="token operator">=</span> nn<span class="token punctuation">.</span>LazyLinear<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> bias<span class="token operator">=</span>bias<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>W_o <span class="token operator">=</span> nn<span class="token punctuation">.</span>LazyLinear<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> bias<span class="token operator">=</span>bias<span class="token punctuation">)</span>


    <span class="token keyword">def</span> <span class="token function">transpose_qkv</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Transposition for parallel computation of multiple attention heads."""</span>
        <span class="token comment"># Shape of input X: (batch_size, no. of queries or key-value pairs,</span>
        <span class="token comment"># num_hiddens). Shape of output X: (batch_size, no. of queries or</span>
        <span class="token comment"># key-value pairs, num_heads, num_hiddens / num_heads)</span>
        X <span class="token operator">=</span> X<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># Shape of output X: (batch_size, num_heads, no. of queries or key-value</span>
        <span class="token comment"># pairs, num_hiddens / num_heads)</span>
        X <span class="token operator">=</span> X<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
        <span class="token comment"># Shape of output: (batch_size * num_heads, no. of queries or key-value</span>
        <span class="token comment"># pairs, num_hiddens / num_heads)</span>
        <span class="token keyword">return</span> X<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">transpose_output</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Reverse the operation of transpose_qkv."""</span>
        X <span class="token operator">=</span> X<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        X <span class="token operator">=</span> X<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> X<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> queries<span class="token punctuation">,</span> keys<span class="token punctuation">,</span> values<span class="token punctuation">,</span> valid_lens<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># Shape of queries, keys, or values:</span>
        <span class="token comment"># (batch_size, no. of queries or key-value pairs, num_hiddens)</span>
        <span class="token comment"># Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)</span>
        <span class="token comment"># After transposing, shape of output queries, keys, or values:</span>
        <span class="token comment"># (batch_size * num_heads, no. of queries or key-value pairs,</span>
        <span class="token comment"># num_hiddens / num_heads)</span>
        queries <span class="token operator">=</span> self<span class="token punctuation">.</span>transpose_qkv<span class="token punctuation">(</span>self<span class="token punctuation">.</span>W_q<span class="token punctuation">(</span>queries<span class="token punctuation">)</span><span class="token punctuation">)</span>
        keys <span class="token operator">=</span> self<span class="token punctuation">.</span>transpose_qkv<span class="token punctuation">(</span>self<span class="token punctuation">.</span>W_k<span class="token punctuation">(</span>keys<span class="token punctuation">)</span><span class="token punctuation">)</span>
        values <span class="token operator">=</span> self<span class="token punctuation">.</span>transpose_qkv<span class="token punctuation">(</span>self<span class="token punctuation">.</span>W_v<span class="token punctuation">(</span>values<span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> valid_lens <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            <span class="token comment"># On axis 0, copy the first item (scalar or vector) for num_heads</span>
            <span class="token comment"># times, then copy the next item, and so on</span>
            valid_lens <span class="token operator">=</span> torch<span class="token punctuation">.</span>repeat_interleave<span class="token punctuation">(</span>
                valid_lens<span class="token punctuation">,</span> repeats<span class="token operator">=</span>self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>

        <span class="token comment"># Shape of output: (batch_size * num_heads, no. of queries,</span>
        <span class="token comment"># num_hiddens / num_heads)</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>attention<span class="token punctuation">(</span>queries<span class="token punctuation">,</span> keys<span class="token punctuation">,</span> values<span class="token punctuation">,</span> valid_lens<span class="token punctuation">)</span>
        <span class="token comment"># Shape of output_concat: (batch_size, no. of queries, num_hiddens)</span>
        output_concat <span class="token operator">=</span> self<span class="token punctuation">.</span>transpose_output<span class="token punctuation">(</span>output<span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>W_o<span class="token punctuation">(</span>output_concat<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>   上面的代码透露了一个问题，多头注意力并不是简单的创建N个相同的注意力进行运算，而是通过nn.LazyLinear投影后，在num_hiddens维度进行num_heads个数的划分，注意经过nn.LazyLinear后，num_hiddens维度的每一个数据其实都和输入的数据有关联，因此这个时候进行num_heads个数的划分是有效的，因为这个时候每个num_heads的组都携带了输入数据的全部信息。</p>
<br/>
<br/>
<br/>
<br/>
<h3 id="位置前馈网络">位置前馈网络</h3>
<hr>
<p>   引入非线性计算，加强网络认知能力。代码如下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">PositionWiseFFN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""The positionwise feed-forward network."""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> ffn_num_hiddens<span class="token punctuation">,</span> ffn_num_outputs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dense1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>LazyLinear<span class="token punctuation">(</span>ffn_num_hiddens<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>relu <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dense2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>LazyLinear<span class="token punctuation">(</span>ffn_num_outputs<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>dense2<span class="token punctuation">(</span>self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>dense1<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<br/>
<br/>
<br/>
<br/>
<h3 id="残差连接和层归一化">残差连接和层归一化</h3>
<hr>
<p>   这个结构主要将原始输入叠加到一个其他计算（例如注意力）的输出上面，这样可以保证输出不会丢失原始输入信息，这个在网络层数大的情况下有奇效。代码如下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">AddNorm</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""The residual connection followed by layer normalization."""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> norm_shape<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>ln <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>norm_shape<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> Y<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>ln<span class="token punctuation">(</span>self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>Y<span class="token punctuation">)</span> <span class="token operator">+</span> X<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<br/>
<br/>
<br/>
<br/>
<h3 id="Transformer-Encoder结构">Transformer Encoder结构</h3>
<hr>
<p>   下面是transformer-Encoder部分的代码</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">TransformerEncoderBlock</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""The Transformer encoder block."""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> ffn_num_hiddens<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> dropout<span class="token punctuation">,</span>
                 use_bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>attention <span class="token operator">=</span> MultiHeadAttention<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span>
                                                dropout<span class="token punctuation">,</span> use_bias<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>addnorm1 <span class="token operator">=</span> AddNorm<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>ffn <span class="token operator">=</span> PositionWiseFFN<span class="token punctuation">(</span>ffn_num_hiddens<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>addnorm2 <span class="token operator">=</span> AddNorm<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> valid_lens<span class="token punctuation">)</span><span class="token punctuation">:</span>
        Y <span class="token operator">=</span> self<span class="token punctuation">.</span>addnorm1<span class="token punctuation">(</span>X<span class="token punctuation">,</span> self<span class="token punctuation">.</span>attention<span class="token punctuation">(</span>X<span class="token punctuation">,</span> X<span class="token punctuation">,</span> X<span class="token punctuation">,</span> valid_lens<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>addnorm2<span class="token punctuation">(</span>Y<span class="token punctuation">,</span> self<span class="token punctuation">.</span>ffn<span class="token punctuation">(</span>Y<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>   从代码中可以知道，其计算过程就是多头注意力、残差连接及层归一化、位置前馈网络、残差连接及层归一化的过程。</p>
<br/>
<br/>
<br/>
<br/>
<h3 id="Transformer-Decoder结构">Transformer Decoder结构</h3>
<hr>
<p>  下面是transformer-Decoder部分的代码</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">TransformerDecoderBlock</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># The i-th block in the Transformer decoder</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> ffn_num_hiddens<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> dropout<span class="token punctuation">,</span> i<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>i <span class="token operator">=</span> i
        self<span class="token punctuation">.</span>attention1 <span class="token operator">=</span> MultiHeadAttention<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span>
                                                 dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>addnorm1 <span class="token operator">=</span> AddNorm<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>attention2 <span class="token operator">=</span> MultiHeadAttention<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span>
                                                 dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>addnorm2 <span class="token operator">=</span> AddNorm<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>ffn <span class="token operator">=</span> PositionWiseFFN<span class="token punctuation">(</span>ffn_num_hiddens<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>addnorm3 <span class="token operator">=</span> AddNorm<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> state<span class="token punctuation">)</span><span class="token punctuation">:</span>
        enc_outputs<span class="token punctuation">,</span> enc_valid_lens <span class="token operator">=</span> state<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> state<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
        <span class="token comment"># During training, all the tokens of any output sequence are processed</span>
        <span class="token comment"># at the same time, so state[2][self.i] is None as initialized. When</span>
        <span class="token comment"># decoding any output sequence token by token during prediction,</span>
        <span class="token comment"># state[2][self.i] contains representations of the decoded output at</span>
        <span class="token comment"># the i-th block up to the current time step</span>
        <span class="token keyword">if</span> state<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">[</span>self<span class="token punctuation">.</span>i<span class="token punctuation">]</span> <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            key_values <span class="token operator">=</span> X
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            key_values <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>state<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">[</span>self<span class="token punctuation">.</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        state<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">[</span>self<span class="token punctuation">.</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> key_values
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>training<span class="token punctuation">:</span>
            batch_size<span class="token punctuation">,</span> num_steps<span class="token punctuation">,</span> _ <span class="token operator">=</span> X<span class="token punctuation">.</span>shape
            <span class="token comment"># Shape of dec_valid_lens: (batch_size, num_steps), where every</span>
            <span class="token comment"># row is [1, 2, ..., num_steps]</span>
            dec_valid_lens <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>
                <span class="token number">1</span><span class="token punctuation">,</span> num_steps <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> device<span class="token operator">=</span>X<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            dec_valid_lens <span class="token operator">=</span> <span class="token boolean">None</span>
        <span class="token comment"># Self-attention</span>
        X2 <span class="token operator">=</span> self<span class="token punctuation">.</span>attention1<span class="token punctuation">(</span>X<span class="token punctuation">,</span> key_values<span class="token punctuation">,</span> key_values<span class="token punctuation">,</span> dec_valid_lens<span class="token punctuation">)</span>
        Y <span class="token operator">=</span> self<span class="token punctuation">.</span>addnorm1<span class="token punctuation">(</span>X<span class="token punctuation">,</span> X2<span class="token punctuation">)</span>
        <span class="token comment"># Encoder-decoder attention. Shape of enc_outputs:</span>
        <span class="token comment"># (batch_size, num_steps, num_hiddens)</span>
        Y2 <span class="token operator">=</span> self<span class="token punctuation">.</span>attention2<span class="token punctuation">(</span>Y<span class="token punctuation">,</span> enc_outputs<span class="token punctuation">,</span> enc_outputs<span class="token punctuation">,</span> enc_valid_lens<span class="token punctuation">)</span>
        Z <span class="token operator">=</span> self<span class="token punctuation">.</span>addnorm2<span class="token punctuation">(</span>Y<span class="token punctuation">,</span> Y2<span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>addnorm3<span class="token punctuation">(</span>Z<span class="token punctuation">,</span> self<span class="token punctuation">.</span>ffn<span class="token punctuation">(</span>Z<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> state<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>   从代码中可以知道，其计算过程就是多头注意力、残差连接及层归一化、多头注意力、残差连接及层归一化、位置前馈网络、残差连接及层归一化的过程。</p>
<br/>
<br/>
<br/>
<br/>
<h3 id="基于transformer的类似seq2seq-英文翻译中文-的实例">基于transformer的类似seq2seq  英文翻译中文  的实例</h3>
<hr>
<p>   关于dataset部分的内容，请参考前面seq2seq相关文章。</p>
<br/>
<br/>
<h5 id="完整代码如下">完整代码如下</h5>
<p>  </p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> os
<span class="token keyword">import</span> random
<span class="token keyword">import</span> torch
<span class="token keyword">import</span> math
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> functional <span class="token keyword">as</span> F
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> time
<span class="token keyword">import</span> visdom
<span class="token keyword">import</span> collections
<span class="token keyword">import</span> dataset
<span class="token keyword">class</span> <span class="token class-name">Accumulator</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""在n个变量上累加"""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Defined in :numref:`sec_softmax_scratch`"""</span>
        self<span class="token punctuation">.</span>data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0.0</span><span class="token punctuation">]</span> <span class="token operator">*</span> n

    <span class="token keyword">def</span> <span class="token function">add</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>data <span class="token operator">=</span> <span class="token punctuation">[</span>a <span class="token operator">+</span> <span class="token builtin">float</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span> <span class="token keyword">for</span> a<span class="token punctuation">,</span> b <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>data<span class="token punctuation">,</span> args<span class="token punctuation">)</span><span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">reset</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0.0</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>data<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> idx<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>data<span class="token punctuation">[</span>idx<span class="token punctuation">]</span>
    
<span class="token keyword">class</span> <span class="token class-name">Timer</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""记录多次运行时间"""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Defined in :numref:`subsec_linear_model`"""</span>
        self<span class="token punctuation">.</span>times <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">start</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""启动计时器"""</span>
        self<span class="token punctuation">.</span>tik <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">stop</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""停止计时器并将时间记录在列表中"""</span>
        self<span class="token punctuation">.</span>times<span class="token punctuation">.</span>append<span class="token punctuation">(</span>time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> self<span class="token punctuation">.</span>tik<span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>times<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">avg</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""返回平均时间"""</span>
        <span class="token keyword">return</span> <span class="token builtin">sum</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>times<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>times<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">sum</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""返回时间总和"""</span>
        <span class="token keyword">return</span> <span class="token builtin">sum</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>times<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">cumsum</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""返回累计时间"""</span>
        <span class="token keyword">return</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>self<span class="token punctuation">.</span>times<span class="token punctuation">)</span><span class="token punctuation">.</span>cumsum<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">class</span> <span class="token class-name">Encoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""编码器-解码器架构的基本编码器接口"""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 调用父类nn.Module的构造函数，确保正确初始化</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Encoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 抛出未实现错误，意味着该方法需要在子类中具体实现</span>
        <span class="token keyword">raise</span> NotImplementedError

<span class="token keyword">class</span> <span class="token class-name">Decoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""编码器-解码器架构的基本解码器接口

    Defined in :numref:`sec_encoder-decoder`"""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 调用父类nn.Module的构造函数，确保正确初始化</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Decoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">init_state</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> enc_outputs<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 抛出未实现错误，意味着该方法需要在子类中具体实现</span>
        <span class="token keyword">raise</span> NotImplementedError

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> state<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 抛出未实现错误，意味着该方法需要在子类中具体实现</span>
        <span class="token keyword">raise</span> NotImplementedError

<span class="token keyword">class</span> <span class="token class-name">EncoderDecoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""编码器-解码器架构的基类

    Defined in :numref:`sec_encoder-decoder`"""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> encoder<span class="token punctuation">,</span> decoder<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 调用父类nn.Module的构造函数，确保正确初始化</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>EncoderDecoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
        <span class="token comment"># 将传入的编码器实例赋值给类的属性</span>
        self<span class="token punctuation">.</span>encoder <span class="token operator">=</span> encoder
        <span class="token comment"># 将传入的解码器实例赋值给类的属性</span>
        self<span class="token punctuation">.</span>decoder <span class="token operator">=</span> decoder

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> enc_X<span class="token punctuation">,</span> dec_X<span class="token punctuation">,</span> enc_X_valid_len<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 调用编码器的前向传播方法，处理输入的编码器输入数据enc_X</span>
        enc_outputs <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>enc_X<span class="token punctuation">,</span> enc_X_valid_len<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span>
        <span class="token comment"># 调用解码器的init_state方法，根据编码器的输出初始化解码器的状态</span>
        dec_state <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">.</span>init_state<span class="token punctuation">(</span>enc_outputs<span class="token punctuation">,</span> enc_X_valid_len<span class="token punctuation">)</span>
        <span class="token comment"># 调用解码器的前向传播方法，处理输入的解码器输入数据dec_X和初始化后的状态</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>dec_X<span class="token punctuation">,</span> dec_state<span class="token punctuation">)</span>
    

<span class="token keyword">def</span> <span class="token function">masked_softmax</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> valid_lens<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""Perform softmax operation by masking elements on the last axis."""</span>
    <span class="token comment"># X: 3D tensor, valid_lens: 1D or 2D tensor</span>
    <span class="token keyword">def</span> <span class="token function">_sequence_mask</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> valid_len<span class="token punctuation">,</span> value<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        maxlen <span class="token operator">=</span> X<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token punctuation">(</span>maxlen<span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">,</span>
                            device<span class="token operator">=</span>X<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">&lt;</span> valid_len<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span>
        X<span class="token punctuation">[</span><span class="token operator">~</span>mask<span class="token punctuation">]</span> <span class="token operator">=</span> value
        <span class="token keyword">return</span> X

    <span class="token keyword">if</span> valid_lens <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>X<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        shape <span class="token operator">=</span> X<span class="token punctuation">.</span>shape
        <span class="token keyword">if</span> valid_lens<span class="token punctuation">.</span>dim<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
            valid_lens <span class="token operator">=</span> torch<span class="token punctuation">.</span>repeat_interleave<span class="token punctuation">(</span>valid_lens<span class="token punctuation">,</span> shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            valid_lens <span class="token operator">=</span> valid_lens<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># On the last axis, replace masked elements with a very large negative</span>
        <span class="token comment"># value, whose exponentiation outputs 0</span>
        X <span class="token operator">=</span> _sequence_mask<span class="token punctuation">(</span>X<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> valid_lens<span class="token punctuation">,</span> value<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1e6</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>X<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>shape<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    
<span class="token keyword">class</span> <span class="token class-name">DotProductAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""Scaled dot product attention."""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>

    <span class="token comment"># Shape of queries: (batch_size, no. of queries, d)</span>
    <span class="token comment"># Shape of keys: (batch_size, no. of key-value pairs, d)</span>
    <span class="token comment"># Shape of values: (batch_size, no. of key-value pairs, value dimension)</span>
    <span class="token comment"># Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> queries<span class="token punctuation">,</span> keys<span class="token punctuation">,</span> values<span class="token punctuation">,</span> valid_lens<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        d <span class="token operator">=</span> queries<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>
        <span class="token comment"># Swap the last two dimensions of keys with keys.transpose(1, 2)</span>
        scores <span class="token operator">=</span> torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>queries<span class="token punctuation">,</span> keys<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>d<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>attention_weights <span class="token operator">=</span> masked_softmax<span class="token punctuation">(</span>scores<span class="token punctuation">,</span> valid_lens<span class="token punctuation">)</span>
        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>self<span class="token punctuation">.</span>attention_weights<span class="token punctuation">)</span><span class="token punctuation">,</span> values<span class="token punctuation">)</span>
    

<span class="token keyword">class</span> <span class="token class-name">MultiHeadAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""Multi-head attention."""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> dropout<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>num_heads <span class="token operator">=</span> num_heads
        self<span class="token punctuation">.</span>attention <span class="token operator">=</span> DotProductAttention<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>W_q <span class="token operator">=</span> nn<span class="token punctuation">.</span>LazyLinear<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> bias<span class="token operator">=</span>bias<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>W_k <span class="token operator">=</span> nn<span class="token punctuation">.</span>LazyLinear<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> bias<span class="token operator">=</span>bias<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>W_v <span class="token operator">=</span> nn<span class="token punctuation">.</span>LazyLinear<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> bias<span class="token operator">=</span>bias<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>W_o <span class="token operator">=</span> nn<span class="token punctuation">.</span>LazyLinear<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> bias<span class="token operator">=</span>bias<span class="token punctuation">)</span>


    <span class="token keyword">def</span> <span class="token function">transpose_qkv</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Transposition for parallel computation of multiple attention heads."""</span>
        <span class="token comment"># Shape of input X: (batch_size, no. of queries or key-value pairs,</span>
        <span class="token comment"># num_hiddens). Shape of output X: (batch_size, no. of queries or</span>
        <span class="token comment"># key-value pairs, num_heads, num_hiddens / num_heads)</span>
        X <span class="token operator">=</span> X<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># Shape of output X: (batch_size, num_heads, no. of queries or key-value</span>
        <span class="token comment"># pairs, num_hiddens / num_heads)</span>
        X <span class="token operator">=</span> X<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
        <span class="token comment"># Shape of output: (batch_size * num_heads, no. of queries or key-value</span>
        <span class="token comment"># pairs, num_hiddens / num_heads)</span>
        <span class="token keyword">return</span> X<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">transpose_output</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Reverse the operation of transpose_qkv."""</span>
        X <span class="token operator">=</span> X<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        X <span class="token operator">=</span> X<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> X<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> queries<span class="token punctuation">,</span> keys<span class="token punctuation">,</span> values<span class="token punctuation">,</span> valid_lens<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># Shape of queries, keys, or values:</span>
        <span class="token comment"># (batch_size, no. of queries or key-value pairs, num_hiddens)</span>
        <span class="token comment"># Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)</span>
        <span class="token comment"># After transposing, shape of output queries, keys, or values:</span>
        <span class="token comment"># (batch_size * num_heads, no. of queries or key-value pairs,</span>
        <span class="token comment"># num_hiddens / num_heads)</span>
        queries <span class="token operator">=</span> self<span class="token punctuation">.</span>transpose_qkv<span class="token punctuation">(</span>self<span class="token punctuation">.</span>W_q<span class="token punctuation">(</span>queries<span class="token punctuation">)</span><span class="token punctuation">)</span>
        keys <span class="token operator">=</span> self<span class="token punctuation">.</span>transpose_qkv<span class="token punctuation">(</span>self<span class="token punctuation">.</span>W_k<span class="token punctuation">(</span>keys<span class="token punctuation">)</span><span class="token punctuation">)</span>
        values <span class="token operator">=</span> self<span class="token punctuation">.</span>transpose_qkv<span class="token punctuation">(</span>self<span class="token punctuation">.</span>W_v<span class="token punctuation">(</span>values<span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> valid_lens <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            <span class="token comment"># On axis 0, copy the first item (scalar or vector) for num_heads</span>
            <span class="token comment"># times, then copy the next item, and so on</span>
            valid_lens <span class="token operator">=</span> torch<span class="token punctuation">.</span>repeat_interleave<span class="token punctuation">(</span>
                valid_lens<span class="token punctuation">,</span> repeats<span class="token operator">=</span>self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>

        <span class="token comment"># Shape of output: (batch_size * num_heads, no. of queries,</span>
        <span class="token comment"># num_hiddens / num_heads)</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>attention<span class="token punctuation">(</span>queries<span class="token punctuation">,</span> keys<span class="token punctuation">,</span> values<span class="token punctuation">,</span> valid_lens<span class="token punctuation">)</span>
        <span class="token comment"># Shape of output_concat: (batch_size, no. of queries, num_hiddens)</span>
        output_concat <span class="token operator">=</span> self<span class="token punctuation">.</span>transpose_output<span class="token punctuation">(</span>output<span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>W_o<span class="token punctuation">(</span>output_concat<span class="token punctuation">)</span>
    

<span class="token keyword">class</span> <span class="token class-name">PositionWiseFFN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""The positionwise feed-forward network."""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> ffn_num_hiddens<span class="token punctuation">,</span> ffn_num_outputs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dense1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>LazyLinear<span class="token punctuation">(</span>ffn_num_hiddens<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>relu <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dense2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>LazyLinear<span class="token punctuation">(</span>ffn_num_outputs<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>dense2<span class="token punctuation">(</span>self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>dense1<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    

<span class="token keyword">class</span> <span class="token class-name">AddNorm</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""The residual connection followed by layer normalization."""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> norm_shape<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>ln <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>norm_shape<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> Y<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>ln<span class="token punctuation">(</span>self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>Y<span class="token punctuation">)</span> <span class="token operator">+</span> X<span class="token punctuation">)</span>
    

<span class="token keyword">class</span> <span class="token class-name">TransformerEncoderBlock</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""The Transformer encoder block."""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> ffn_num_hiddens<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> dropout<span class="token punctuation">,</span>
                 use_bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>attention <span class="token operator">=</span> MultiHeadAttention<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span>
                                                dropout<span class="token punctuation">,</span> use_bias<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>addnorm1 <span class="token operator">=</span> AddNorm<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>ffn <span class="token operator">=</span> PositionWiseFFN<span class="token punctuation">(</span>ffn_num_hiddens<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>addnorm2 <span class="token operator">=</span> AddNorm<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> valid_lens<span class="token punctuation">)</span><span class="token punctuation">:</span>
        Y <span class="token operator">=</span> self<span class="token punctuation">.</span>addnorm1<span class="token punctuation">(</span>X<span class="token punctuation">,</span> self<span class="token punctuation">.</span>attention<span class="token punctuation">(</span>X<span class="token punctuation">,</span> X<span class="token punctuation">,</span> X<span class="token punctuation">,</span> valid_lens<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>addnorm2<span class="token punctuation">(</span>Y<span class="token punctuation">,</span> self<span class="token punctuation">.</span>ffn<span class="token punctuation">(</span>Y<span class="token punctuation">)</span><span class="token punctuation">)</span>
    
<span class="token keyword">class</span> <span class="token class-name">PositionalEncoding</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""Positional encoding."""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> dropout<span class="token punctuation">,</span> max_len<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>
        <span class="token comment"># Create a long enough P</span>
        self<span class="token punctuation">.</span>P <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> max_len<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">)</span><span class="token punctuation">)</span>
        X <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>max_len<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>
            <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">/</span> torch<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span><span class="token number">10000</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>
            <span class="token number">0</span><span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span> <span class="token operator">/</span> num_hiddens<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>P<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>P<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>cos<span class="token punctuation">(</span>X<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>
        X <span class="token operator">=</span> X <span class="token operator">+</span> self<span class="token punctuation">.</span>P<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>X<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>X<span class="token punctuation">)</span>



<span class="token keyword">class</span> <span class="token class-name">TransformerEncoder</span><span class="token punctuation">(</span>Encoder<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""The Transformer encoder."""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> ffn_num_hiddens<span class="token punctuation">,</span>
                 num_heads<span class="token punctuation">,</span> num_blks<span class="token punctuation">,</span> dropout<span class="token punctuation">,</span> use_bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>num_hiddens <span class="token operator">=</span> num_hiddens
        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>pos_encoding <span class="token operator">=</span> PositionalEncoding<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>blks <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_blks<span class="token punctuation">)</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>blks<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">"block"</span><span class="token operator">+</span><span class="token builtin">str</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">,</span> TransformerEncoderBlock<span class="token punctuation">(</span>
                num_hiddens<span class="token punctuation">,</span> ffn_num_hiddens<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> dropout<span class="token punctuation">,</span> use_bias<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> valid_lens<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># Since positional encoding values are between -1 and 1, the embedding</span>
        <span class="token comment"># values are multiplied by the square root of the embedding dimension</span>
        <span class="token comment"># to rescale before they are summed up</span>
        <span class="token comment"># X[batch_size, seq_len, num_hidden]</span>
        X <span class="token operator">=</span> self<span class="token punctuation">.</span>pos_encoding<span class="token punctuation">(</span>self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>X<span class="token punctuation">)</span> <span class="token operator">*</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_hiddens<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>attention_weights <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>blks<span class="token punctuation">)</span>
        <span class="token keyword">for</span> i<span class="token punctuation">,</span> blk <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>blks<span class="token punctuation">)</span><span class="token punctuation">:</span>
            X <span class="token operator">=</span> blk<span class="token punctuation">(</span>X<span class="token punctuation">,</span> valid_lens<span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>attention_weights<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> blk<span class="token punctuation">.</span>attention<span class="token punctuation">.</span>attention<span class="token punctuation">.</span>attention_weights
        <span class="token comment"># X[batch_size, seq_len, num_hidden]</span>
        <span class="token keyword">return</span> X
    


<span class="token keyword">class</span> <span class="token class-name">TransformerDecoderBlock</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># The i-th block in the Transformer decoder</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> ffn_num_hiddens<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> dropout<span class="token punctuation">,</span> i<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>i <span class="token operator">=</span> i
        self<span class="token punctuation">.</span>attention1 <span class="token operator">=</span> MultiHeadAttention<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span>
                                                 dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>addnorm1 <span class="token operator">=</span> AddNorm<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>attention2 <span class="token operator">=</span> MultiHeadAttention<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span>
                                                 dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>addnorm2 <span class="token operator">=</span> AddNorm<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>ffn <span class="token operator">=</span> PositionWiseFFN<span class="token punctuation">(</span>ffn_num_hiddens<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>addnorm3 <span class="token operator">=</span> AddNorm<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> state<span class="token punctuation">)</span><span class="token punctuation">:</span>
        enc_outputs<span class="token punctuation">,</span> enc_valid_lens <span class="token operator">=</span> state<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> state<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
        <span class="token comment"># During training, all the tokens of any output sequence are processed</span>
        <span class="token comment"># at the same time, so state[2][self.i] is None as initialized. When</span>
        <span class="token comment"># decoding any output sequence token by token during prediction,</span>
        <span class="token comment"># state[2][self.i] contains representations of the decoded output at</span>
        <span class="token comment"># the i-th block up to the current time step</span>
        <span class="token keyword">if</span> state<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">[</span>self<span class="token punctuation">.</span>i<span class="token punctuation">]</span> <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            key_values <span class="token operator">=</span> X
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            key_values <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>state<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">[</span>self<span class="token punctuation">.</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        state<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">[</span>self<span class="token punctuation">.</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> key_values
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>training<span class="token punctuation">:</span>
            batch_size<span class="token punctuation">,</span> num_steps<span class="token punctuation">,</span> _ <span class="token operator">=</span> X<span class="token punctuation">.</span>shape
            <span class="token comment"># Shape of dec_valid_lens: (batch_size, num_steps), where every</span>
            <span class="token comment"># row is [1, 2, ..., num_steps]</span>
            dec_valid_lens <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>
                <span class="token number">1</span><span class="token punctuation">,</span> num_steps <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> device<span class="token operator">=</span>X<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            dec_valid_lens <span class="token operator">=</span> <span class="token boolean">None</span>
        <span class="token comment"># Self-attention</span>
        X2 <span class="token operator">=</span> self<span class="token punctuation">.</span>attention1<span class="token punctuation">(</span>X<span class="token punctuation">,</span> key_values<span class="token punctuation">,</span> key_values<span class="token punctuation">,</span> dec_valid_lens<span class="token punctuation">)</span>
        Y <span class="token operator">=</span> self<span class="token punctuation">.</span>addnorm1<span class="token punctuation">(</span>X<span class="token punctuation">,</span> X2<span class="token punctuation">)</span>
        <span class="token comment"># Encoder-decoder attention. Shape of enc_outputs:</span>
        <span class="token comment"># (batch_size, num_steps, num_hiddens)</span>
        Y2 <span class="token operator">=</span> self<span class="token punctuation">.</span>attention2<span class="token punctuation">(</span>Y<span class="token punctuation">,</span> enc_outputs<span class="token punctuation">,</span> enc_outputs<span class="token punctuation">,</span> enc_valid_lens<span class="token punctuation">)</span>
        Z <span class="token operator">=</span> self<span class="token punctuation">.</span>addnorm2<span class="token punctuation">(</span>Y<span class="token punctuation">,</span> Y2<span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>addnorm3<span class="token punctuation">(</span>Z<span class="token punctuation">,</span> self<span class="token punctuation">.</span>ffn<span class="token punctuation">(</span>Z<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> state
    

<span class="token keyword">class</span> <span class="token class-name">TransformerDecoder</span><span class="token punctuation">(</span>Decoder<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> ffn_num_hiddens<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span>
                 num_blks<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>num_hiddens <span class="token operator">=</span> num_hiddens
        self<span class="token punctuation">.</span>num_blks <span class="token operator">=</span> num_blks
        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>pos_encoding <span class="token operator">=</span> PositionalEncoding<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>blks <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_blks<span class="token punctuation">)</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>blks<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">"block"</span><span class="token operator">+</span><span class="token builtin">str</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">,</span> TransformerDecoderBlock<span class="token punctuation">(</span>
                num_hiddens<span class="token punctuation">,</span> ffn_num_hiddens<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> dropout<span class="token punctuation">,</span> i<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dense <span class="token operator">=</span> nn<span class="token punctuation">.</span>LazyLinear<span class="token punctuation">(</span>vocab_size<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">init_state</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> enc_outputs<span class="token punctuation">,</span> enc_valid_lens<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token punctuation">[</span>enc_outputs<span class="token punctuation">,</span> enc_valid_lens<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">]</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>num_blks<span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> state<span class="token punctuation">)</span><span class="token punctuation">:</span>
        X <span class="token operator">=</span> self<span class="token punctuation">.</span>pos_encoding<span class="token punctuation">(</span>self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>X<span class="token punctuation">)</span> <span class="token operator">*</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_hiddens<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>_attention_weights <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>blks<span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> i<span class="token punctuation">,</span> blk <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>blks<span class="token punctuation">)</span><span class="token punctuation">:</span>
            X<span class="token punctuation">,</span> state <span class="token operator">=</span> blk<span class="token punctuation">(</span>X<span class="token punctuation">,</span> state<span class="token punctuation">)</span>
            <span class="token comment"># Decoder self-attention weights</span>
            self<span class="token punctuation">.</span>_attention_weights<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span>
                i<span class="token punctuation">]</span> <span class="token operator">=</span> blk<span class="token punctuation">.</span>attention1<span class="token punctuation">.</span>attention<span class="token punctuation">.</span>attention_weights
            <span class="token comment"># Encoder-decoder attention weights</span>
            self<span class="token punctuation">.</span>_attention_weights<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span>
                i<span class="token punctuation">]</span> <span class="token operator">=</span> blk<span class="token punctuation">.</span>attention2<span class="token punctuation">.</span>attention<span class="token punctuation">.</span>attention_weights
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>dense<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">,</span> state

    <span class="token decorator annotation punctuation">@property</span>
    <span class="token keyword">def</span> <span class="token function">attention_weights</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>_attention_weights
    


<span class="token keyword">def</span> <span class="token function">sequence_mask</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> valid_len<span class="token punctuation">,</span> value<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""在序列中屏蔽不相关的项"""</span>
    maxlen <span class="token operator">=</span> X<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
    mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token punctuation">(</span>maxlen<span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">,</span>
                        device<span class="token operator">=</span>X<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">&lt;</span> valid_len<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span>
    X<span class="token punctuation">[</span><span class="token operator">~</span>mask<span class="token punctuation">]</span> <span class="token operator">=</span> value
    <span class="token keyword">return</span> X

<span class="token keyword">class</span> <span class="token class-name">MaskedSoftmaxCELoss</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""带遮蔽的softmax交叉熵损失函数"""</span>
    <span class="token comment"># pred的形状：(batch_size,num_steps,vocab_size)</span>
    <span class="token comment"># label的形状：(batch_size,num_steps)</span>
    <span class="token comment"># valid_len的形状：(batch_size,)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> pred<span class="token punctuation">,</span> label<span class="token punctuation">,</span> valid_len<span class="token punctuation">)</span><span class="token punctuation">:</span>
        weights <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span>label<span class="token punctuation">)</span>
        weights <span class="token operator">=</span> sequence_mask<span class="token punctuation">(</span>weights<span class="token punctuation">,</span> valid_len<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>reduction<span class="token operator">=</span><span class="token string">'none'</span>
        unweighted_loss <span class="token operator">=</span> <span class="token builtin">super</span><span class="token punctuation">(</span>MaskedSoftmaxCELoss<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>forward<span class="token punctuation">(</span>
            pred<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> label<span class="token punctuation">)</span>
        weighted_loss <span class="token operator">=</span> <span class="token punctuation">(</span>unweighted_loss <span class="token operator">*</span> weights<span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> weighted_loss
    
<span class="token keyword">def</span> <span class="token function">grad_clipping</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> theta<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""裁剪梯度"""</span>
    <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
        params <span class="token operator">=</span> <span class="token punctuation">[</span>p <span class="token keyword">for</span> p <span class="token keyword">in</span> net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> p<span class="token punctuation">.</span>requires_grad<span class="token punctuation">]</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        params <span class="token operator">=</span> net<span class="token punctuation">.</span>params
    norm <span class="token operator">=</span> torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token builtin">sum</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">(</span>p<span class="token punctuation">.</span>grad <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">for</span> p <span class="token keyword">in</span> params<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> norm <span class="token operator">></span> theta<span class="token punctuation">:</span>
        <span class="token keyword">for</span> param <span class="token keyword">in</span> params<span class="token punctuation">:</span>
            param<span class="token punctuation">.</span>grad<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">*=</span> theta <span class="token operator">/</span> norm

<span class="token keyword">def</span> <span class="token function">train_seq2seq</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> data_iter<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> tgt_vocab<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""训练序列到序列模型"""</span>
    <span class="token keyword">def</span> <span class="token function">xavier_init_weights</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token builtin">type</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span> <span class="token operator">==</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">:</span>
            nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>xavier_uniform_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>
        <span class="token keyword">if</span> <span class="token builtin">type</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span> <span class="token operator">==</span> nn<span class="token punctuation">.</span>GRU<span class="token punctuation">:</span>
            <span class="token keyword">for</span> param <span class="token keyword">in</span> m<span class="token punctuation">.</span>_flat_weights_names<span class="token punctuation">:</span>
                <span class="token keyword">if</span> <span class="token string">"weight"</span> <span class="token keyword">in</span> param<span class="token punctuation">:</span>
                    nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>xavier_uniform_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>_parameters<span class="token punctuation">[</span>param<span class="token punctuation">]</span><span class="token punctuation">)</span>

    net<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>xavier_init_weights<span class="token punctuation">)</span>
    net<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
    optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">)</span>
    loss <span class="token operator">=</span> MaskedSoftmaxCELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
    net<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
    vis <span class="token operator">=</span> visdom<span class="token punctuation">.</span>Visdom<span class="token punctuation">(</span>env<span class="token operator">=</span><span class="token string">u'test1'</span><span class="token punctuation">,</span> server<span class="token operator">=</span><span class="token string">"http://127.0.0.1"</span><span class="token punctuation">,</span> port<span class="token operator">=</span><span class="token number">8097</span><span class="token punctuation">)</span>
    animator <span class="token operator">=</span> vis
    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        timer <span class="token operator">=</span> Timer<span class="token punctuation">(</span><span class="token punctuation">)</span>
        metric <span class="token operator">=</span> Accumulator<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># 训练损失总和，词元数量</span>
        <span class="token keyword">for</span> batch <span class="token keyword">in</span> data_iter<span class="token punctuation">:</span>
            <span class="token comment">#清零（reset）优化器中的梯度缓存</span>
            optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token comment"># x.shape = [batch_size, num_steps]</span>
            X<span class="token punctuation">,</span> X_valid_len<span class="token punctuation">,</span> Y<span class="token punctuation">,</span> Y_valid_len <span class="token operator">=</span> <span class="token punctuation">[</span>x<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> batch<span class="token punctuation">]</span>
            <span class="token comment"># bos.shape = batch_size 个 bos-id</span>
            bos <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>tgt_vocab<span class="token punctuation">[</span><span class="token string">'&lt;bos>'</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">*</span> Y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                          device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token comment"># dec_input.shape = (batch_size, num_steps)</span>
            <span class="token comment"># 解码器的输入通常由序列的起始标志 bos 和目标序列（去掉末尾的部分 Y[:, :-1]）组成。</span>
            dec_input <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>bos<span class="token punctuation">,</span> Y<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 强制教学</span>
            <span class="token comment"># Y_hat的形状:(batch_size,num_steps,vocab_size)</span>
            Y_hat<span class="token punctuation">,</span> _ <span class="token operator">=</span> net<span class="token punctuation">(</span>X<span class="token punctuation">,</span> dec_input<span class="token punctuation">,</span> X_valid_len<span class="token punctuation">)</span>
            l <span class="token operator">=</span> loss<span class="token punctuation">(</span>Y_hat<span class="token punctuation">,</span> Y<span class="token punctuation">,</span> Y_valid_len<span class="token punctuation">)</span>
            l<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>      <span class="token comment"># 损失函数的标量进行“反向传播”</span>
            grad_clipping<span class="token punctuation">(</span>net<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
            num_tokens <span class="token operator">=</span> Y_valid_len<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
            optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                metric<span class="token punctuation">.</span>add<span class="token punctuation">(</span>l<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> num_tokens<span class="token punctuation">)</span>

        <span class="token keyword">if</span> <span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">10</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            <span class="token comment"># print(predict('你是？'))</span>
            <span class="token comment"># print(epoch)</span>
            <span class="token comment"># animator.add(epoch + 1, )</span>

            <span class="token keyword">if</span> epoch <span class="token operator">==</span> <span class="token number">9</span><span class="token punctuation">:</span>
                <span class="token comment"># 清空图表：使用空数组来替换现有内容</span>
                vis<span class="token punctuation">.</span>line<span class="token punctuation">(</span>X<span class="token operator">=</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> Y<span class="token operator">=</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> win<span class="token operator">=</span><span class="token string">'train_ch8'</span><span class="token punctuation">,</span> update<span class="token operator">=</span><span class="token string">'replace'</span><span class="token punctuation">)</span>
            <span class="token comment"># _loss_val = l</span>
            <span class="token comment"># _loss_val = _loss_val.cpu().sum().detach().numpy()</span>
            vis<span class="token punctuation">.</span>line<span class="token punctuation">(</span>
                X<span class="token operator">=</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                Y<span class="token operator">=</span><span class="token punctuation">[</span> metric<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">/</span> metric<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                win<span class="token operator">=</span><span class="token string">'train_ch8'</span><span class="token punctuation">,</span>
                update<span class="token operator">=</span><span class="token string">'append'</span><span class="token punctuation">,</span>
                opts<span class="token operator">=</span><span class="token punctuation">&#123;</span>
                    <span class="token string">'title'</span><span class="token punctuation">:</span> <span class="token string">'train_ch8'</span><span class="token punctuation">,</span>
                    <span class="token string">'xlabel'</span><span class="token punctuation">:</span> <span class="token string">'epoch'</span><span class="token punctuation">,</span>
                    <span class="token string">'ylabel'</span><span class="token punctuation">:</span> <span class="token string">'loss'</span><span class="token punctuation">,</span>
                    <span class="token string">'linecolor'</span><span class="token punctuation">:</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">255</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment"># 蓝色线条</span>
                <span class="token punctuation">&#125;</span>
            <span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'loss </span><span class="token interpolation"><span class="token punctuation">&#123;</span>metric<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">/</span> metric<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">&#125;</span></span><span class="token string">, </span><span class="token interpolation"><span class="token punctuation">&#123;</span>metric<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">/</span> timer<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.1f</span><span class="token punctuation">&#125;</span></span><span class="token string"> '</span></span>
        <span class="token string-interpolation"><span class="token string">f'tokens/sec on </span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">str</span><span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
    torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>net<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'model_h.pt'</span><span class="token punctuation">)</span>  <span class="token comment"># [[6]]</span>
    torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>net<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'model.pt'</span><span class="token punctuation">)</span>  <span class="token comment"># [[6]]</span>

<span class="token keyword">def</span> <span class="token function">predict_seq2seq</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> src_sentence<span class="token punctuation">,</span> src_vocab<span class="token punctuation">,</span> tgt_vocab<span class="token punctuation">,</span> num_steps<span class="token punctuation">,</span>
                    device<span class="token punctuation">,</span> save_attention_weights<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""序列到序列模型的预测"""</span>
    <span class="token comment"># 在预测时将net设置为评估模式</span>
    net<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    src_tokens <span class="token operator">=</span> src_vocab<span class="token punctuation">[</span>src_sentence<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">[</span>
        src_vocab<span class="token punctuation">[</span><span class="token string">'&lt;eos>'</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
    enc_valid_len <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>src_tokens<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>
    src_tokens <span class="token operator">=</span> dataset<span class="token punctuation">.</span>truncate_pad<span class="token punctuation">(</span>src_tokens<span class="token punctuation">,</span> num_steps<span class="token punctuation">,</span> src_vocab<span class="token punctuation">[</span><span class="token string">'&lt;pad>'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token comment"># 添加批量轴</span>
    enc_X <span class="token operator">=</span> torch<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>
        torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>src_tokens<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
    enc_outputs <span class="token operator">=</span> net<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>enc_X<span class="token punctuation">,</span> enc_valid_len<span class="token punctuation">)</span>
    dec_state <span class="token operator">=</span> net<span class="token punctuation">.</span>decoder<span class="token punctuation">.</span>init_state<span class="token punctuation">(</span>enc_outputs<span class="token punctuation">,</span> enc_valid_len<span class="token punctuation">)</span>
    <span class="token comment"># 添加批量轴</span>
    dec_X <span class="token operator">=</span> torch<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>
        <span class="token punctuation">[</span>tgt_vocab<span class="token punctuation">[</span><span class="token string">'&lt;bos>'</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
    output_seq<span class="token punctuation">,</span> attention_weight_seq <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_steps<span class="token punctuation">)</span><span class="token punctuation">:</span>
        Y<span class="token punctuation">,</span> dec_state <span class="token operator">=</span> net<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>dec_X<span class="token punctuation">,</span> dec_state<span class="token punctuation">)</span>
        <span class="token comment"># 我们使用具有预测最高可能性的词元，作为解码器在下一时间步的输入</span>
        dec_X <span class="token operator">=</span> Y<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        pred <span class="token operator">=</span> dec_X<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>int32<span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 保存注意力权重（稍后讨论）</span>
        <span class="token keyword">if</span> save_attention_weights<span class="token punctuation">:</span>
            <span class="token comment"># 2'st block&amp;2'st attention</span>
            attention_weight_seq<span class="token punctuation">.</span>append<span class="token punctuation">(</span>net<span class="token punctuation">.</span>decoder<span class="token punctuation">.</span>attention_weights<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># 一旦序列结束词元被预测，输出序列的生成就完成了</span>
        <span class="token keyword">if</span> pred <span class="token operator">==</span> tgt_vocab<span class="token punctuation">[</span><span class="token string">'&lt;eos>'</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
            <span class="token keyword">break</span>
        output_seq<span class="token punctuation">.</span>append<span class="token punctuation">(</span>pred<span class="token punctuation">)</span>
    <span class="token keyword">return</span> <span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>tgt_vocab<span class="token punctuation">.</span>to_tokens<span class="token punctuation">(</span>output_seq<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> attention_weight_seq


<span class="token keyword">def</span> <span class="token function">bleu</span><span class="token punctuation">(</span>pred_seq<span class="token punctuation">,</span> label_seq<span class="token punctuation">,</span> k<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""计算BLEU"""</span>
    pred_tokens<span class="token punctuation">,</span> label_tokens <span class="token operator">=</span> pred_seq<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>i <span class="token keyword">for</span> i <span class="token keyword">in</span> label_seq<span class="token punctuation">]</span>
    len_pred<span class="token punctuation">,</span> len_label <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>pred_tokens<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>label_tokens<span class="token punctuation">)</span>
    score <span class="token operator">=</span> math<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token builtin">min</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span> <span class="token operator">-</span> len_label <span class="token operator">/</span> len_pred<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> n <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> k <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        num_matches<span class="token punctuation">,</span> label_subs <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> collections<span class="token punctuation">.</span>defaultdict<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>len_label <span class="token operator">-</span> n <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            label_subs<span class="token punctuation">[</span><span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>label_tokens<span class="token punctuation">[</span>i<span class="token punctuation">:</span> i <span class="token operator">+</span> n<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">1</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>len_pred <span class="token operator">-</span> n <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> label_subs<span class="token punctuation">[</span><span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>pred_tokens<span class="token punctuation">[</span>i<span class="token punctuation">:</span> i <span class="token operator">+</span> n<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>
                num_matches <span class="token operator">+=</span> <span class="token number">1</span>
                label_subs<span class="token punctuation">[</span><span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>pred_tokens<span class="token punctuation">[</span>i<span class="token punctuation">:</span> i <span class="token operator">+</span> n<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">-=</span> <span class="token number">1</span>
        score <span class="token operator">*=</span> math<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>num_matches <span class="token operator">/</span> <span class="token punctuation">(</span>len_pred <span class="token operator">-</span> n <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> math<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> score

<span class="token keyword">def</span> <span class="token function">try_gpu</span><span class="token punctuation">(</span>i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""如果存在，则返回gpu(i)，否则返回cpu()

    Defined in :numref:`sec_use_gpu`"""</span>
    <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>device_count<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">>=</span> i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'cuda:</span><span class="token interpolation"><span class="token punctuation">&#123;</span>i<span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cpu'</span><span class="token punctuation">)</span>


<span class="token keyword">from</span> matplotlib <span class="token keyword">import</span> pyplot <span class="token keyword">as</span> plt
<span class="token keyword">import</span> matplotlib
<span class="token comment"># from matplotlib_inline import backend_inline</span>
<span class="token keyword">def</span> <span class="token function">show_heatmaps</span><span class="token punctuation">(</span>matrices<span class="token punctuation">,</span> xlabel<span class="token punctuation">,</span> ylabel<span class="token punctuation">,</span> titles<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2.5</span><span class="token punctuation">,</span> <span class="token number">2.5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                  cmap<span class="token operator">=</span><span class="token string">'Reds'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    显示矩阵的热图（Heatmaps）。
    这个函数旨在以子图网格的形式绘制多个矩阵，通常用于可视化注意力权重等。

    参数:
        matrices (numpy.ndarray 或 torch.Tensor 数组): 
            一个四维数组，形状应为 (num_rows, num_cols, height, width)。
            其中，num_rows 和 num_cols 决定了子图网格的布局，
            height 和 width 是每个热图（即每个矩阵）的维度。
        xlabel (str): 
            所有最底行子图的 x 轴标签。
        ylabel (str): 
            所有最左列子图的 y 轴标签。
        titles (list of str, optional): 
            一个包含 num_cols 个标题的列表，用于设置每一列子图的标题。默认 None。
        figsize (tuple, optional): 
            整个图形（figure）的大小。默认 (2.5, 2.5)。
        cmap (str, optional): 
            用于绘制热图的颜色映射（colormap）。默认 'Reds'。
    """</span>
    <span class="token comment"># 导入所需的 matplotlib 模块，确保图形在 Jupyter/IPython 环境中正确显示为 SVG 格式</span>
    <span class="token comment"># （假设在包含这个函数的环境中已经导入了 matplotlib 的 backend_inline）</span>
    <span class="token comment"># backend_inline.set_matplotlib_formats('svg')</span>
    matplotlib<span class="token punctuation">.</span>use<span class="token punctuation">(</span><span class="token string">'TkAgg'</span><span class="token punctuation">)</span>
    <span class="token comment"># 从输入的 matrices 形状中解构出子图网格的行数和列数</span>
    <span class="token comment"># 假设 matrices 的形状是 (num_rows, num_cols, height, width)</span>
    num_rows<span class="token punctuation">,</span> num_cols<span class="token punctuation">,</span> _<span class="token punctuation">,</span> _ <span class="token operator">=</span> matrices<span class="token punctuation">.</span>shape
    
    <span class="token comment"># 创建一个包含多个子图（axes）的图形（fig）</span>
    <span class="token comment"># fig: 整个图形对象</span>
    <span class="token comment"># axes: 一个 num_rows x num_cols 的子图对象数组</span>
    fig<span class="token punctuation">,</span> axes <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span>
        num_rows<span class="token punctuation">,</span> num_cols<span class="token punctuation">,</span> 
        figsize<span class="token operator">=</span>figsize<span class="token punctuation">,</span>
        sharex<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    <span class="token comment"># 所有子图共享 x 轴刻度</span>
        sharey<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    <span class="token comment"># 所有子图共享 y 轴刻度</span>
        squeeze<span class="token operator">=</span><span class="token boolean">False</span>   <span class="token comment"># 即使只有一行或一列，也强制返回二维数组的 axes，方便后续循环</span>
    <span class="token punctuation">)</span>
    
    <span class="token comment"># 遍历子图的行和对应的矩阵行</span>
    <span class="token comment"># i 是行索引, row_axes 是当前行的子图数组, row_matrices 是当前行的矩阵数组</span>
    <span class="token keyword">for</span> i<span class="token punctuation">,</span> <span class="token punctuation">(</span>row_axes<span class="token punctuation">,</span> row_matrices<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span><span class="token builtin">zip</span><span class="token punctuation">(</span>axes<span class="token punctuation">,</span> matrices<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 遍历当前行中的子图和对应的矩阵</span>
        <span class="token comment"># j 是列索引, ax 是当前的子图对象, matrix 是当前的待绘矩阵</span>
        <span class="token keyword">for</span> j<span class="token punctuation">,</span> <span class="token punctuation">(</span>ax<span class="token punctuation">,</span> matrix<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span><span class="token builtin">zip</span><span class="token punctuation">(</span>row_axes<span class="token punctuation">,</span> row_matrices<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            
            <span class="token comment"># 使用 ax.imshow() 绘制热图</span>
            <span class="token comment"># matrix.detach().numpy()：将 PyTorch Tensor 转换为 numpy 数组，并从计算图中分离（如果它是 Tensor）</span>
            <span class="token comment"># cmap：指定颜色映射</span>
            pcm <span class="token operator">=</span> ax<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>matrix<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cmap<span class="token operator">=</span>cmap<span class="token punctuation">)</span>
            
            <span class="token comment"># --- 设置轴标签和标题 ---</span>
            
            <span class="token comment"># 只有最底行 (i == num_rows - 1) 的子图才显示 x 轴标签</span>
            <span class="token keyword">if</span> i <span class="token operator">==</span> num_rows <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">:</span>
                ax<span class="token punctuation">.</span>set_xlabel<span class="token punctuation">(</span>xlabel<span class="token punctuation">)</span>
                
            <span class="token comment"># 只有最左列 (j == 0) 的子图才显示 y 轴标签</span>
            <span class="token keyword">if</span> j <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
                ax<span class="token punctuation">.</span>set_ylabel<span class="token punctuation">(</span>ylabel<span class="token punctuation">)</span>
                
            <span class="token comment"># 如果提供了标题列表，则设置当前列的子图标题（所有行共享列标题）</span>
            <span class="token keyword">if</span> titles<span class="token punctuation">:</span>
                ax<span class="token punctuation">.</span>set_title<span class="token punctuation">(</span>titles<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">)</span>
                
    <span class="token comment"># --- 添加颜色条（Colorbar） ---</span>
    
    <span class="token comment"># 为整个图形添加一个颜色条，用于表示数值和颜色的对应关系</span>
    <span class="token comment"># pcm: 之前绘制的第一个热图返回的 Colormap </span>
    <span class="token comment"># ax=axes: 颜色条将参照整个子图网格进行定位和缩放</span>
    <span class="token comment"># shrink=0.6: 缩小颜色条的高度/长度，使其只占图形高度的 60%</span>
    fig<span class="token punctuation">.</span>colorbar<span class="token punctuation">(</span>pcm<span class="token punctuation">,</span> ax<span class="token operator">=</span>axes<span class="token punctuation">,</span> shrink<span class="token operator">=</span><span class="token number">0.6</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    num_hiddens<span class="token punctuation">,</span> num_blks<span class="token punctuation">,</span> dropout <span class="token operator">=</span> <span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0.2</span>
    ffn_num_hiddens<span class="token punctuation">,</span> num_heads <span class="token operator">=</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">4</span>
    batch_size <span class="token operator">=</span> <span class="token number">1024</span>
    num_steps <span class="token operator">=</span> <span class="token number">10</span>
    lr<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> device <span class="token operator">=</span> <span class="token number">0.001</span><span class="token punctuation">,</span> <span class="token number">2000</span><span class="token punctuation">,</span> try_gpu<span class="token punctuation">(</span><span class="token punctuation">)</span>

    train_iter<span class="token punctuation">,</span> src_vocab<span class="token punctuation">,</span> tgt_vocab<span class="token punctuation">,</span> source<span class="token punctuation">,</span> target <span class="token operator">=</span> dataset<span class="token punctuation">.</span>load_data<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> num_steps<span class="token punctuation">)</span>

    encoder <span class="token operator">=</span> TransformerEncoder<span class="token punctuation">(</span>
        <span class="token builtin">len</span><span class="token punctuation">(</span>src_vocab<span class="token punctuation">)</span><span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> ffn_num_hiddens<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span>
        num_blks<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span>
    decoder <span class="token operator">=</span> TransformerDecoder<span class="token punctuation">(</span>
        <span class="token builtin">len</span><span class="token punctuation">(</span>tgt_vocab<span class="token punctuation">)</span><span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> ffn_num_hiddens<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span>
        num_blks<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span>

    net <span class="token operator">=</span> EncoderDecoder<span class="token punctuation">(</span>encoder<span class="token punctuation">,</span> decoder<span class="token punctuation">)</span>
    
    is_train <span class="token operator">=</span> <span class="token boolean">False</span>
    is_show <span class="token operator">=</span> <span class="token boolean">True</span>
    <span class="token keyword">if</span> is_train<span class="token punctuation">:</span>
        train_seq2seq<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> tgt_vocab<span class="token punctuation">,</span> device<span class="token punctuation">)</span>
    <span class="token keyword">elif</span> is_show<span class="token punctuation">:</span>
        state_dict <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'model_h.pt'</span><span class="token punctuation">)</span>
        net<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>state_dict<span class="token punctuation">)</span>
        net<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>

        src_text <span class="token operator">=</span> <span class="token string">"Call us."</span>
        translation<span class="token punctuation">,</span> attention_weight_seq <span class="token operator">=</span> predict_seq2seq<span class="token punctuation">(</span>
                net<span class="token punctuation">,</span> src_text<span class="token punctuation">,</span> src_vocab<span class="token punctuation">,</span> tgt_vocab<span class="token punctuation">,</span> num_steps<span class="token punctuation">,</span> device<span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token comment"># attention_weights = torch.eye(10).reshape((1, 1, 10, 10))</span>
        <span class="token comment"># (num_rows, num_cols, height, width)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'translation=</span><span class="token interpolation"><span class="token punctuation">&#123;</span>translation<span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
        <span class="token comment"># print(attention_weight_seq.shape)</span>
        
        stacked_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>attention_weight_seq<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>stacked_tensor<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        show_heatmaps<span class="token punctuation">(</span>
            stacked_tensor<span class="token punctuation">,</span>
            xlabel<span class="token operator">=</span><span class="token string">'Attention weight'</span><span class="token punctuation">,</span> ylabel<span class="token operator">=</span><span class="token string">'Decode Step'</span><span class="token punctuation">,</span> titles<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'Head %d'</span> <span class="token operator">%</span> i <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        state_dict <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'model_h.pt'</span><span class="token punctuation">)</span>
        net<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>state_dict<span class="token punctuation">)</span>
        net<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
        C <span class="token operator">=</span> <span class="token number">0</span>
        C1 <span class="token operator">=</span> <span class="token number">0</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">2000</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># print(source[i])</span>
            <span class="token comment"># print(target[i])</span>
            translation<span class="token punctuation">,</span> attention_weight_seq <span class="token operator">=</span> predict_seq2seq<span class="token punctuation">(</span>
                net<span class="token punctuation">,</span> source<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> src_vocab<span class="token punctuation">,</span> tgt_vocab<span class="token punctuation">,</span> num_steps<span class="token punctuation">,</span> device<span class="token punctuation">)</span>
            
            score <span class="token operator">=</span> bleu<span class="token punctuation">(</span>translation<span class="token punctuation">,</span> target<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> k<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
            <span class="token keyword">if</span> score <span class="token operator">></span> <span class="token number">0.0</span><span class="token punctuation">:</span>
                C <span class="token operator">=</span> C <span class="token operator">+</span> <span class="token number">1</span>
                <span class="token keyword">if</span> score <span class="token operator">></span> <span class="token number">0.8</span><span class="token punctuation">:</span>
                    C1 <span class="token operator">=</span> C1 <span class="token operator">+</span> <span class="token number">1</span>
                <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'</span><span class="token interpolation"><span class="token punctuation">&#123;</span>source<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">&#125;</span></span><span class="token string"> => </span><span class="token interpolation"><span class="token punctuation">&#123;</span>translation<span class="token punctuation">&#125;</span></span><span class="token string">, bleu </span><span class="token interpolation"><span class="token punctuation">&#123;</span>score<span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>

        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Counter(bleu > 0) = </span><span class="token interpolation"><span class="token punctuation">&#123;</span>C<span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Valid-Counter(bleu > 0.8) = </span><span class="token interpolation"><span class="token punctuation">&#123;</span>C1<span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>  我们先看一下TransformerEncoder做了什么：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>和前面类似，首先输入做了embedding，然后叠加位置编码</p>
</li>
<li class="lvl-2">
<p>然后循环计算每一个TransformerEncoderBlock</p>
</li>
</ul>
<p>  TransformerEncoderBlock中做了：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>计算自注意力</p>
</li>
<li class="lvl-2">
<p>残差连接和层归一化</p>
</li>
<li class="lvl-2">
<p>位置前馈网络</p>
</li>
<li class="lvl-2">
<p>残差连接和层归一化</p>
</li>
</ul>
<p>  然后我们来看看TransformerDecoder做了什么：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>和TransformerEncoder类似，首先输入做了embedding，然后叠加位置编码</p>
</li>
<li class="lvl-2">
<p>然后循环计算每一个TransformerDecoderBlock</p>
</li>
<li class="lvl-2">
<p>最后接一个全连接，映射到词表大小</p>
</li>
</ul>
<p>  TransformerDecoderBlock做了：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>首先准备自注意力的$K_1 V_1$，其更新过程是每次输入X的拼接过程</p>
</li>
<li class="lvl-2">
<p>将输入X 作为Q，$K_1 V_1$作为KV开始自注意力的运算过程</p>
</li>
<li class="lvl-2">
<p>残差连接和层归一化，得到Y</p>
</li>
<li class="lvl-2">
<p>将enc_output作为KV, Y作为Q，计算编码器-解码器注意力</p>
</li>
<li class="lvl-2">
<p>残差连接和层归一化</p>
</li>
<li class="lvl-2">
<p>位置前馈网络</p>
</li>
<li class="lvl-2">
<p>残差连接和层归一化</p>
</li>
</ul>
<p>  下面是训练和测试的一些结果</p>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/blog_idx_144/train.png" alt="rep_img"/></center>
    </div>
</div>   
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/blog_idx_144/ret.png" alt="rep_img"/></center>
    </div>
</div>   
<p>  从上面的图可以看到，这个模型的效果比seq2seq原始模型、seq2seq带注意力的模型要好很多。</p>
<p>  此外，下面是我们翻译：“Call us.”-&gt; “联 系 我 们 。” 的attention weight的可视化（block=2, head=4, mask=3）</p>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/blog_idx_144/multi_attention.png" alt="rep_img"/></center>
    </div>
</div>   
<p>  从每一个decode step的每个head的注意力权重来看，不同head关注了不一样的重点，有效的识别了特征中的多种属性，提高了模型的能力。</p>
<br/>
<br/>
<br/>
<br/>
<h3 id="后记">后记</h3>
<hr>
<p>    本文介绍了transformer结构以及其示例，这里也引入了很多现在LLM的很多概念，例如：位置编码等。</p>
<h3 id="参考文献">参考文献</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html">https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html</a></p>
</li>
</ul>
<br/>
<br/>
<div style="margin:50px auto;">
</div>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <hr/>
        <center><font color = #91e0b0 size = 5>打赏、订阅、收藏、丢香蕉、硬币，请关注公众号（攻城狮的搬砖之路）</font></center>
    </div>
</div>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/qrcode_for_wx_official_account.jpg" alt="qrc_img"/></center>
    </div>
</div>
<!-- ![alt 公众号图片](https://flyinskyin2013.github.io/ImageBed0/blogs/qrcode_for_wx_official_account.jpg "公众号图片") -->
<p><font color="red" size="7">PS: 请尊重原创，不喜勿喷。</font><br/><br>
<font color="red" size="7">PS: 要转载请注明出处，本人版权所有。</font><br/><br>
<font color="red" size="7">PS: 有问题请留言，看到后我会第一时间回复。</font><br/></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://e-x.top/2025/11/02/blog_idx_143/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Sky">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sky's Blogs">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | Sky's Blogs">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/11/02/blog_idx_143/" class="post-title-link" itemprop="url">大模型基础补全计划(六)---带注意力机制的seq2seq实例与测试(Bahdanau Attention)</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-11-02 11:08:00 / 修改时间：11:12:05" itemprop="dateCreated datePublished" datetime="2025-11-02T11:08:00+08:00">2025-11-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/LLM/" itemprop="url" rel="index"><span itemprop="name">LLM</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/LLM/LM/" itemprop="url" rel="index"><span itemprop="name">LM</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>6.2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>22 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <script src="\assets\js\APlayer.min.js"> </script><!--
 * @Description: 
 * @Author: Sky
 * @Date: 2020-08-24 16:37:34
 * @LastEditors: Sky
 * @LastEditTime: 2021-06-29 15:04:48
 * @Github: https://github.com/flyinskyin2013/
-->
<p><font color="red" size="7">PS：要转载请注明出处，本人版权所有。</font><br/><br>
<font color="red" size="7">PS: 这个只是基于《我自己》的理解，</font><br/><font color="red" size="7">如果和你的原则及想法相冲突，请谅解，勿喷。</font><br/></p>
<!-- ###### 前置说明
&emsp;&emsp;本文作为本人github blog/cnblog的主站的备份。（BlogID=143） 
&emsp;&emsp;本文发布于 2025-11-02 11:08:00             （BlogID=143） 
-->
<h6 id="环境说明">环境说明</h6>
<p>  无</p>
<h3 id="前言">前言</h3>
<hr>
<p>   本文是这个系列第六篇，它们是：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>《大模型基础补全计划(一)—重温一些深度学习相关的数学知识》 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/Iflyinsky/p/18717317">https://www.cnblogs.com/Iflyinsky/p/18717317</a></p>
</li>
<li class="lvl-2">
<p>《大模型基础补全计划(二)—词嵌入(word embedding) 》 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/Iflyinsky/p/18775451">https://www.cnblogs.com/Iflyinsky/p/18775451</a></p>
</li>
<li class="lvl-2">
<p>《大模型基础补全计划(三)—RNN实例与测试》 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/Iflyinsky/p/18967569">https://www.cnblogs.com/Iflyinsky/p/18967569</a></p>
</li>
<li class="lvl-2">
<p>《大模型基础补全计划(四)—LSTM的实例与测试(RNN的改进)》 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/Iflyinsky/p/19091089">https://www.cnblogs.com/Iflyinsky/p/19091089</a></p>
</li>
<li class="lvl-2">
<p>《大模型基础补全计划(五)—seq2seq实例与测试(编码器、解码器架构)》 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/Iflyinsky/p/19150535">https://www.cnblogs.com/Iflyinsky/p/19150535</a></p>
</li>
</ul>
<p>  本文，介绍一下注意力机制，并在上文的机翻模型seq2seq的实例中添加一个简单的注意力机制，并看看模型效果是否有提升。</p>
<br/>
<br/>
<br/>
<br/>
<h3 id="注意力机制（Bahdanau-Attention）">注意力机制（Bahdanau Attention）</h3>
<hr>
<p>   举一个例子：在日常生活中，比如我们看一幅黑白画（画中有一个红色的苹果，其他的都是黑白的物体，例如香蕉），这个时候我们无意识的看一眼画，很有可能第一个关注的就是这个红色的苹果，但是我有意识的控制眼睛集中去看香蕉，这个时候我关注的就是香蕉。</p>
<p>  在上面的例子中，我们的注意力，最开始是无意识的看苹果，后面有意识的注意香蕉，这里面的区别就是我们在这个动作里面加了：意识。当加了意识后，我们就可以有选择的根据条件来关注这幅画的我想关注的地方。</p>
<p>  然后我们可以对上面的现象进行建模：$R=Attention(Q,K)*V$，这里我们将Attention当作意识，V当作黑白画的特征，Q是画中是什么？K是V的标签（你可以把K当作是V有关联的部分，不同的K，对应的不同的V），如果没有Attention，R就是苹果，有了Attention，R就可以是香蕉。</p>
<p>  我们回头想一想上一篇文的seq2seq中，我们的encoder的output是最后一层rnn的所有时间步的隐藏状态（num_steps,batch_size,num_hiddens），这里包含了我们的序列数据在不同时间步的特征变化，当我们在做decoder的时候，我们是拿着这个encoder的最后一层rnn的最后一个时间步的隐藏状态（1,batch_size,num_hiddens）来作为context的，是一个固定的值，这样有几个问题：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>对于长序列来说，context可能丢失信息。</p>
</li>
<li class="lvl-2">
<p>我们从固定context中解码信息，导致了我们对序列在特定解码步骤中，对context关注重点是一样的。</p>
</li>
</ul>
<p>  针对上面seq2seq的问题，Bahdanau设计了一种模型，可以解决我们遇到的问题，其定义如下：$$c_{t’} = \sum_{t=1}^{T} \alpha(s_{t’-1}, h_{t})h_{t}$$，看公式我们可以知道，这里定义了Q（decoder的上一次隐藏态$s_{t’-1}$）/K（encoder的output的部分$h_{t}$）/V（encoder的output的部分$h_{t}$）三个概念，含义就是通过Q+K来计算一个权重矩阵W（通过softmax归一化），然后然后将W和V进行计算，得到了我们通过W关注到的新的$V_{new}$，这里的W就是我们的注意力矩阵，代表我们关注V中的哪些部分。整个计算过程，就相当于我们生成了新成context具备了注意力机制。</p>
<br/>
<br/>
<br/>
<br/>
<h3 id="带注意力机制的seq2seq-英文翻译中文-的实例">带注意力机制的seq2seq  英文翻译中文  的实例</h3>
<hr>
<p>   下面的代码和上一篇文章的代码只有decoder部分有比较大的差别，其他的基本类似。如dataset部分的内容，请参考上一篇文章。</p>
<br/>
<br/>
<h5 id="seq2seq完整代码如下">seq2seq完整代码如下</h5>
<p>  </p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> os
<span class="token keyword">import</span> random
<span class="token keyword">import</span> torch
<span class="token keyword">import</span> math
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> functional <span class="token keyword">as</span> F
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> time
<span class="token keyword">import</span> visdom
<span class="token keyword">import</span> collections

<span class="token keyword">import</span> dataset
<span class="token keyword">class</span> <span class="token class-name">Accumulator</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""在n个变量上累加"""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Defined in :numref:`sec_softmax_scratch`"""</span>
        self<span class="token punctuation">.</span>data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0.0</span><span class="token punctuation">]</span> <span class="token operator">*</span> n

    <span class="token keyword">def</span> <span class="token function">add</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>data <span class="token operator">=</span> <span class="token punctuation">[</span>a <span class="token operator">+</span> <span class="token builtin">float</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span> <span class="token keyword">for</span> a<span class="token punctuation">,</span> b <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>data<span class="token punctuation">,</span> args<span class="token punctuation">)</span><span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">reset</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0.0</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>data<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> idx<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>data<span class="token punctuation">[</span>idx<span class="token punctuation">]</span>
    
<span class="token keyword">class</span> <span class="token class-name">Timer</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""记录多次运行时间"""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Defined in :numref:`subsec_linear_model`"""</span>
        self<span class="token punctuation">.</span>times <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">start</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""启动计时器"""</span>
        self<span class="token punctuation">.</span>tik <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">stop</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""停止计时器并将时间记录在列表中"""</span>
        self<span class="token punctuation">.</span>times<span class="token punctuation">.</span>append<span class="token punctuation">(</span>time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> self<span class="token punctuation">.</span>tik<span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>times<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">avg</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""返回平均时间"""</span>
        <span class="token keyword">return</span> <span class="token builtin">sum</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>times<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>times<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">sum</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""返回时间总和"""</span>
        <span class="token keyword">return</span> <span class="token builtin">sum</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>times<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">cumsum</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""返回累计时间"""</span>
        <span class="token keyword">return</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>self<span class="token punctuation">.</span>times<span class="token punctuation">)</span><span class="token punctuation">.</span>cumsum<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
<span class="token keyword">class</span> <span class="token class-name">Encoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""编码器-解码器架构的基本编码器接口"""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 调用父类nn.Module的构造函数，确保正确初始化</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Encoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 抛出未实现错误，意味着该方法需要在子类中具体实现</span>
        <span class="token keyword">raise</span> NotImplementedError

<span class="token keyword">class</span> <span class="token class-name">Decoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""编码器-解码器架构的基本解码器接口

    Defined in :numref:`sec_encoder-decoder`"""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 调用父类nn.Module的构造函数，确保正确初始化</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Decoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">init_state</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> enc_outputs<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 抛出未实现错误，意味着该方法需要在子类中具体实现</span>
        <span class="token keyword">raise</span> NotImplementedError

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> state<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 抛出未实现错误，意味着该方法需要在子类中具体实现</span>
        <span class="token keyword">raise</span> NotImplementedError

<span class="token keyword">class</span> <span class="token class-name">EncoderDecoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""编码器-解码器架构的基类

    Defined in :numref:`sec_encoder-decoder`"""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> encoder<span class="token punctuation">,</span> decoder<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 调用父类nn.Module的构造函数，确保正确初始化</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>EncoderDecoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
        <span class="token comment"># 将传入的编码器实例赋值给类的属性</span>
        self<span class="token punctuation">.</span>encoder <span class="token operator">=</span> encoder
        <span class="token comment"># 将传入的解码器实例赋值给类的属性</span>
        self<span class="token punctuation">.</span>decoder <span class="token operator">=</span> decoder

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> enc_X<span class="token punctuation">,</span> dec_X<span class="token punctuation">,</span> enc_X_valid_len<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 调用编码器的前向传播方法，处理输入的编码器输入数据enc_X</span>
        enc_outputs <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>enc_X<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span>
        <span class="token comment"># 调用解码器的init_state方法，根据编码器的输出初始化解码器的状态</span>
        dec_state <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">.</span>init_state<span class="token punctuation">(</span>enc_outputs<span class="token punctuation">,</span> enc_X_valid_len<span class="token punctuation">)</span>
        <span class="token comment"># 调用解码器的前向传播方法，处理输入的解码器输入数据dec_X和初始化后的状态</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>dec_X<span class="token punctuation">,</span> dec_state<span class="token punctuation">)</span>
    

<span class="token keyword">def</span> <span class="token function">masked_softmax</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> valid_lens<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""
    执行带掩码的 Softmax 操作。
    
    参数:
        X (torch.Tensor): 待 Softmax 的张量，通常是注意力机制中的“分数”（scores）。
                          其形状通常为 (批量大小, 查询数量/序列长度, 键值对数量/序列长度)。
        valid_lens (torch.Tensor): 序列的有效长度张量。
                                   形状可以是 (批量大小,) 或 (批量大小, 键值对数量)。
                                   用于指示每个序列的哪个部分是有效的（非填充）。
    
    返回:
        torch.Tensor: 经过 Softmax 归一化且填充部分被忽略的概率分布张量。
    """</span>
    
    <span class="token comment"># 辅助函数：创建一个序列掩码，并用特定值覆盖被掩码（填充）的元素</span>
    <span class="token keyword">def</span> <span class="token function">_sequence_mask</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> valid_len<span class="token punctuation">,</span> value<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        根据有效长度（valid_len）创建掩码，并应用于张量 X。
        
        参数:
            X (torch.Tensor): 形状为 (批量大小 * 查询数量, 最大长度) 的张量。
            valid_len (torch.Tensor): 形状为 (批量大小 * 查询数量,) 的有效长度向量。
            value (float): 用于替换被掩码元素的填充值。
            
        返回:
            torch.Tensor: 被填充值覆盖后的张量 X。
        """</span>
        <span class="token comment"># 获取序列的最大长度（张量的第二个维度）</span>
        maxlen <span class="token operator">=</span> X<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        
        <span class="token comment"># 核心掩码逻辑：</span>
        <span class="token comment"># 1. torch.arange((maxlen), ...) 创建一个从 0 到 maxlen-1 的序列（代表时间步索引）</span>
        <span class="token comment"># 2. [None, :] 使其形状变为 (1, maxlen)，用于广播</span>
        <span class="token comment"># 3. valid_len[:, None] 使有效长度形状变为 (批量大小 * 查询数量, 1)，用于广播</span>
        <span class="token comment"># 4. &lt; 比较操作：当索引 &lt; 有效长度时，结果为 True（有效元素），否则为 False（填充元素）</span>
        mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token punctuation">(</span>maxlen<span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">,</span>
                            device<span class="token operator">=</span>X<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">&lt;</span> valid_len<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span>
        
        <span class="token comment"># 逻辑非 ~mask 得到填充部分的掩码（True 表示填充部分）</span>
        <span class="token comment"># 使用填充值（value，通常是 -1e6）覆盖填充元素</span>
        X<span class="token punctuation">[</span><span class="token operator">~</span>mask<span class="token punctuation">]</span> <span class="token operator">=</span> value
        <span class="token keyword">return</span> X

    <span class="token comment"># 1. 处理无需掩码的情况</span>
    <span class="token keyword">if</span> valid_lens <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        <span class="token comment"># 如果未提供有效长度，则执行标准 Softmax</span>
        <span class="token keyword">return</span> nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>X<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 2. 处理需要掩码的情况</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        <span class="token comment"># 备份原始形状，用于后续重塑</span>
        shape <span class="token operator">=</span> X<span class="token punctuation">.</span>shape
        
        <span class="token comment"># 统一 valid_lens 的形状，使其与 X 的前两个维度相匹配</span>
        <span class="token keyword">if</span> valid_lens<span class="token punctuation">.</span>dim<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
            <span class="token comment"># 适用于批量中每个序列只有一个有效长度的情况（例如，K-V 序列是等长的）</span>
            <span class="token comment"># 将 valid_lens 重复 shape[1] 次，匹配 X 的查询/序列长度维度</span>
            valid_lens <span class="token operator">=</span> torch<span class="token punctuation">.</span>repeat_interleave<span class="token punctuation">(</span>valid_lens<span class="token punctuation">,</span> shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token comment"># 适用于每个查询-键值对的有效长度都不同的情况</span>
            <span class="token comment"># 将 2D 张量展平为 1D 向量</span>
            valid_lens <span class="token operator">=</span> valid_lens<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
            
        <span class="token comment"># 预处理 Softmax 输入：将 X 调整为 2D 矩阵 (批量*查询数量, 键值对数量)</span>
        <span class="token comment"># 并在最后一个轴（Softmax 轴）上，用一个非常大的负值替换被掩码的元素</span>
        <span class="token comment"># Softmax 时 exp(-1e6) 趋近于 0，从而忽略填充部分。</span>
        X <span class="token operator">=</span> _sequence_mask<span class="token punctuation">(</span>X<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> valid_lens<span class="token punctuation">,</span> value<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1e6</span><span class="token punctuation">)</span>
        
        <span class="token comment"># 对经过掩码处理的 X 执行 Softmax</span>
        <span class="token comment"># 结果张量 X 被重塑回原始的 3D 形状 (批量大小, 查询数量, 键值对数量)</span>
        <span class="token comment"># 并在最后一个维度（dim=-1）上进行归一化，得到注意力权重</span>
        <span class="token keyword">return</span> nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>X<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>shape<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    

    
<span class="token keyword">class</span> <span class="token class-name">AdditiveAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""
    加性注意力（Additive Attention）模块。
    通过将 Query 和 Key 投影到相同的维度后相加，再通过 tanh 激活和线性层计算注意力分数。
    
    公式核心：score(Q, K) = w_v^T * tanh(W_q*Q + W_k*K)
    """</span>
    
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> dropout<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        初始化加性注意力模块。
        
        参数:
            num_hiddens (int): 隐藏层维度，Q 和 K 投影后的维度。
            dropout (float): Dropout 率。
        """</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>AdditiveAttention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
        
        <span class="token comment"># W_k：将 Key (K) 向量投影到 num_hiddens 维度的线性层</span>
        <span class="token comment"># 使用 nn.LazyLinear 延迟初始化，直到第一次 forward 传入 K 的维度</span>
        self<span class="token punctuation">.</span>W_k <span class="token operator">=</span> nn<span class="token punctuation">.</span>LazyLinear<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        
        <span class="token comment"># W_q：将 Query (Q) 向量投影到 num_hiddens 维度的线性层</span>
        <span class="token comment"># 使用 nn.LazyLinear 延迟初始化</span>
        self<span class="token punctuation">.</span>W_q <span class="token operator">=</span> nn<span class="token punctuation">.</span>LazyLinear<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        
        <span class="token comment"># w_v：将激活后的特征向量 (W_q*Q + W_k*K) 投影成一个标量分数（维度为 1）</span>
        <span class="token comment"># 使用 nn.LazyLinear 延迟初始化</span>
        self<span class="token punctuation">.</span>w_v <span class="token operator">=</span> nn<span class="token punctuation">.</span>LazyLinear<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        
        <span class="token comment"># Dropout 层，用于防止过拟合</span>
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> queries<span class="token punctuation">,</span> keys<span class="token punctuation">,</span> values<span class="token punctuation">,</span> valid_lens<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        执行前向传播计算。
        
        参数:
            queries (torch.Tensor): 查询向量 Q。形状通常为 (批量大小, 查询数量, 查询维度)。
            keys (torch.Tensor): 键向量 K。形状通常为 (批量大小, 键值对数量, 键维度)。
            values (torch.Tensor): 值向量 V。形状通常为 (批量大小, 键值对数量, 值维度)。
            valid_lens (torch.Tensor): 键值对序列的有效长度，用于掩盖填充部分。
            
        返回:
            torch.Tensor: 注意力加权后的值向量。形状为 (批量大小, 查询数量, 值维度)。
        """</span>
        <span class="token comment"># 1. 线性变换：分别对 Q 和 K 进行投影</span>
        queries<span class="token punctuation">,</span> keys <span class="token operator">=</span> self<span class="token punctuation">.</span>W_q<span class="token punctuation">(</span>queries<span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>W_k<span class="token punctuation">(</span>keys<span class="token punctuation">)</span>
        
        <span class="token comment"># 2. 维度扩展与相加（Attention Scoring 的核心步骤）</span>
        <span class="token comment"># queries.unsqueeze(2): 形状从 (批量大小, 查询数量, num_hiddens) </span>
        <span class="token comment">#                       变为 (批量大小, 查询数量, 1, num_hiddens)。</span>
        <span class="token comment"># keys.unsqueeze(1): 形状从 (批量大小, 键值对数量, num_hiddens) </span>
        <span class="token comment">#                     变为 (批量大小, 1, 键值对数量, num_hiddens)。</span>
        <span class="token comment"># 两个张量通过广播机制相加，得到 features，形状为：</span>
        <span class="token comment"># (批量大小, 查询数量, 键值对数量, num_hiddens)</span>
        features <span class="token operator">=</span> queries<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">+</span> keys<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        
        <span class="token comment"># 3. 激活函数：应用 tanh 激活（加性注意力机制的要求）</span>
        features <span class="token operator">=</span> torch<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>features<span class="token punctuation">)</span>
        
        <span class="token comment"># 4. 投影到标量分数</span>
        <span class="token comment"># self.w_v(features): 将 features 的最后一个维度（num_hiddens）投影成 1。</span>
        <span class="token comment"># scores.squeeze(-1): 移除最后一个单维度 (1)，得到最终的注意力分数张量。</span>
        <span class="token comment"># 形状为：(批量大小, 查询数量, 键值对数量)</span>
        scores <span class="token operator">=</span> self<span class="token punctuation">.</span>w_v<span class="token punctuation">(</span>features<span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        
        <span class="token comment"># 5. 归一化（Softmax）：使用带掩码的 Softmax 得到注意力权重</span>
        <span class="token comment"># 填充部分的得分会被设置为一个极小的负值，Softmax 后权重趋近于 0。</span>
        self<span class="token punctuation">.</span>attention_weights <span class="token operator">=</span> masked_softmax<span class="token punctuation">(</span>scores<span class="token punctuation">,</span> valid_lens<span class="token punctuation">)</span>
        
        <span class="token comment"># 6. 加权求和</span>
        <span class="token comment"># torch.bmm: 批量矩阵乘法 (Batch Matrix Multiplication)。</span>
        <span class="token comment"># 将 [注意力权重] (批量, Q数量, K数量) 与 [值向量] (批量, K数量, V维度) 相乘</span>
        <span class="token comment"># 得到最终的注意力输出，形状为：(批量大小, 查询数量, 值维度)</span>
        <span class="token comment"># 在 BMM 之前，对注意力权重应用 Dropout。</span>
        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>self<span class="token punctuation">.</span>attention_weights<span class="token punctuation">)</span><span class="token punctuation">,</span> values<span class="token punctuation">)</span>


<span class="token comment">#@save</span>
<span class="token keyword">class</span> <span class="token class-name">Seq2SeqEncoder</span><span class="token punctuation">(</span>Encoder<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""用于序列到序列学习的循环神经网络编码器"""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> embed_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> num_layers<span class="token punctuation">,</span>
                 dropout<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Seq2SeqEncoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
        <span class="token comment"># 嵌入层</span>
        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embed_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>rnn <span class="token operator">=</span> nn<span class="token punctuation">.</span>GRU<span class="token punctuation">(</span>embed_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> num_layers<span class="token punctuation">,</span>
                          dropout<span class="token operator">=</span>dropout<span class="token punctuation">)</span>
        <span class="token comment"># self.lstm = nn.LSTM(embed_size, num_hiddens, num_layers)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 输入X.shape = (batch_size,num_steps)</span>
        <span class="token comment"># 输出'X'的形状：(batch_size,num_steps,embed_size)</span>
        X <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
        <span class="token comment"># 在循环神经网络模型中，第一个轴对应于时间步</span>
        X <span class="token operator">=</span> X<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
        <span class="token comment"># 如果未提及状态，则默认为0</span>
        output<span class="token punctuation">,</span> state <span class="token operator">=</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
        <span class="token comment"># output : 这个返回值是所有时间步的隐藏状态序列</span>
        <span class="token comment"># output的形状:(num_steps,batch_size,num_hiddens)</span>
        <span class="token comment"># hn (hidden) : 这是每一层rnn的最后一个时间步的隐藏状态</span>
        <span class="token comment"># state的形状:(num_layers,batch_size,num_hiddens)</span>
        <span class="token keyword">return</span> output<span class="token punctuation">,</span> state
    
<span class="token keyword">class</span> <span class="token class-name">AttentionDecoder</span><span class="token punctuation">(</span>Decoder<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""The base attention-based decoder interface."""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token decorator annotation punctuation">@property</span>
    <span class="token keyword">def</span> <span class="token function">attention_weights</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">raise</span> NotImplementedError
    
<span class="token keyword">class</span> <span class="token class-name">Seq2SeqAttentionDecoder</span><span class="token punctuation">(</span>AttentionDecoder<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> embed_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> num_layers<span class="token punctuation">,</span>
                 dropout<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>attention <span class="token operator">=</span> AdditiveAttention<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embed_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>rnn <span class="token operator">=</span> nn<span class="token punctuation">.</span>GRU<span class="token punctuation">(</span>
            embed_size <span class="token operator">+</span> num_hiddens<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> num_layers<span class="token punctuation">,</span>
            dropout<span class="token operator">=</span>dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dense <span class="token operator">=</span> nn<span class="token punctuation">.</span>LazyLinear<span class="token punctuation">(</span>vocab_size<span class="token punctuation">)</span>
        <span class="token comment"># self.apply(d2l.init_seq2seq)</span>

    <span class="token keyword">def</span> <span class="token function">init_state</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> enc_outputs<span class="token punctuation">,</span> enc_valid_lens<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># Shape of outputs: (num_steps, batch_size, num_hiddens).</span>
        <span class="token comment"># Shape of hidden_state: (num_layers, batch_size, num_hiddens)</span>
        outputs<span class="token punctuation">,</span> hidden_state <span class="token operator">=</span> enc_outputs
        <span class="token keyword">return</span> <span class="token punctuation">(</span>outputs<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> hidden_state<span class="token punctuation">,</span> enc_valid_lens<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> state<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># Shape of enc_outputs: (batch_size, num_steps, num_hiddens).</span>
        <span class="token comment"># Shape of hidden_state: (num_layers, batch_size, num_hiddens)</span>
        enc_outputs<span class="token punctuation">,</span> hidden_state<span class="token punctuation">,</span> enc_valid_lens <span class="token operator">=</span> state
        <span class="token comment"># Shape of the output X: (num_steps, batch_size, embed_size)</span>
        X <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
        outputs<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_attention_weights <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> x <span class="token keyword">in</span> X<span class="token punctuation">:</span>
            <span class="token comment"># Shape of query: (batch_size, 1, num_hiddens)</span>
            query <span class="token operator">=</span> torch<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>hidden_state<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token comment"># Shape of context: (batch_size, 1, num_hiddens)</span>
            context  <span class="token operator">=</span> self<span class="token punctuation">.</span>attention<span class="token punctuation">(</span>
                query<span class="token punctuation">,</span> enc_outputs<span class="token punctuation">,</span> enc_outputs<span class="token punctuation">,</span> enc_valid_lens<span class="token punctuation">)</span>
            <span class="token comment"># Concatenate on the feature dimension</span>
            x <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>context<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>x<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token comment"># Reshape x as (1, batch_size, embed_size + num_hiddens)</span>
            out<span class="token punctuation">,</span> hidden_state <span class="token operator">=</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">(</span>x<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> hidden_state<span class="token punctuation">)</span>
            outputs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>out<span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>_attention_weights<span class="token punctuation">.</span>append<span class="token punctuation">(</span>self<span class="token punctuation">.</span>attention<span class="token punctuation">.</span>attention_weights<span class="token punctuation">)</span>
        <span class="token comment"># After fully connected layer transformation, shape of outputs:</span>
        <span class="token comment"># (num_steps, batch_size, vocab_size)</span>
        outputs <span class="token operator">=</span> self<span class="token punctuation">.</span>dense<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> outputs<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>enc_outputs<span class="token punctuation">,</span> hidden_state<span class="token punctuation">,</span>
                                          enc_valid_lens<span class="token punctuation">]</span>

    <span class="token decorator annotation punctuation">@property</span>
    <span class="token keyword">def</span> <span class="token function">attention_weights</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>_attention_weights

    
<span class="token keyword">def</span> <span class="token function">try_gpu</span><span class="token punctuation">(</span>i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""如果存在，则返回gpu(i)，否则返回cpu()

    Defined in :numref:`sec_use_gpu`"""</span>
    <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>device_count<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">>=</span> i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'cuda:</span><span class="token interpolation"><span class="token punctuation">&#123;</span>i<span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cpu'</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">sequence_mask</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> valid_len<span class="token punctuation">,</span> value<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""在序列中屏蔽不相关的项"""</span>
    maxlen <span class="token operator">=</span> X<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
    mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token punctuation">(</span>maxlen<span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">,</span>
                        device<span class="token operator">=</span>X<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">&lt;</span> valid_len<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span>
    X<span class="token punctuation">[</span><span class="token operator">~</span>mask<span class="token punctuation">]</span> <span class="token operator">=</span> value
    <span class="token keyword">return</span> X

<span class="token keyword">class</span> <span class="token class-name">MaskedSoftmaxCELoss</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""带遮蔽的softmax交叉熵损失函数"""</span>
    <span class="token comment"># pred的形状：(batch_size,num_steps,vocab_size)</span>
    <span class="token comment"># label的形状：(batch_size,num_steps)</span>
    <span class="token comment"># valid_len的形状：(batch_size,)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> pred<span class="token punctuation">,</span> label<span class="token punctuation">,</span> valid_len<span class="token punctuation">)</span><span class="token punctuation">:</span>
        weights <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span>label<span class="token punctuation">)</span>
        weights <span class="token operator">=</span> sequence_mask<span class="token punctuation">(</span>weights<span class="token punctuation">,</span> valid_len<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>reduction<span class="token operator">=</span><span class="token string">'none'</span>
        unweighted_loss <span class="token operator">=</span> <span class="token builtin">super</span><span class="token punctuation">(</span>MaskedSoftmaxCELoss<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>forward<span class="token punctuation">(</span>
            pred<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> label<span class="token punctuation">)</span>
        weighted_loss <span class="token operator">=</span> <span class="token punctuation">(</span>unweighted_loss <span class="token operator">*</span> weights<span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> weighted_loss
    
<span class="token keyword">def</span> <span class="token function">grad_clipping</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> theta<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""裁剪梯度"""</span>
    <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
        params <span class="token operator">=</span> <span class="token punctuation">[</span>p <span class="token keyword">for</span> p <span class="token keyword">in</span> net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> p<span class="token punctuation">.</span>requires_grad<span class="token punctuation">]</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        params <span class="token operator">=</span> net<span class="token punctuation">.</span>params
    norm <span class="token operator">=</span> torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token builtin">sum</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">(</span>p<span class="token punctuation">.</span>grad <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">for</span> p <span class="token keyword">in</span> params<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> norm <span class="token operator">></span> theta<span class="token punctuation">:</span>
        <span class="token keyword">for</span> param <span class="token keyword">in</span> params<span class="token punctuation">:</span>
            param<span class="token punctuation">.</span>grad<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">*=</span> theta <span class="token operator">/</span> norm

<span class="token keyword">def</span> <span class="token function">train_seq2seq</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> data_iter<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> tgt_vocab<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""训练序列到序列模型"""</span>
    <span class="token keyword">def</span> <span class="token function">xavier_init_weights</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token builtin">type</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span> <span class="token operator">==</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">:</span>
            nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>xavier_uniform_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>
        <span class="token keyword">if</span> <span class="token builtin">type</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span> <span class="token operator">==</span> nn<span class="token punctuation">.</span>GRU<span class="token punctuation">:</span>
            <span class="token keyword">for</span> param <span class="token keyword">in</span> m<span class="token punctuation">.</span>_flat_weights_names<span class="token punctuation">:</span>
                <span class="token keyword">if</span> <span class="token string">"weight"</span> <span class="token keyword">in</span> param<span class="token punctuation">:</span>
                    nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>xavier_uniform_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>_parameters<span class="token punctuation">[</span>param<span class="token punctuation">]</span><span class="token punctuation">)</span>

    net<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>xavier_init_weights<span class="token punctuation">)</span>
    net<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
    optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">)</span>
    loss <span class="token operator">=</span> MaskedSoftmaxCELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
    net<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
    vis <span class="token operator">=</span> visdom<span class="token punctuation">.</span>Visdom<span class="token punctuation">(</span>env<span class="token operator">=</span><span class="token string">u'test1'</span><span class="token punctuation">,</span> server<span class="token operator">=</span><span class="token string">"http://127.0.0.1"</span><span class="token punctuation">,</span> port<span class="token operator">=</span><span class="token number">8097</span><span class="token punctuation">)</span>
    animator <span class="token operator">=</span> vis
    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        timer <span class="token operator">=</span> Timer<span class="token punctuation">(</span><span class="token punctuation">)</span>
        metric <span class="token operator">=</span> Accumulator<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># 训练损失总和，词元数量</span>
        <span class="token keyword">for</span> batch <span class="token keyword">in</span> data_iter<span class="token punctuation">:</span>
            <span class="token comment">#清零（reset）优化器中的梯度缓存</span>
            optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token comment"># x.shape = [batch_size, num_steps]</span>
            X<span class="token punctuation">,</span> X_valid_len<span class="token punctuation">,</span> Y<span class="token punctuation">,</span> Y_valid_len <span class="token operator">=</span> <span class="token punctuation">[</span>x<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> batch<span class="token punctuation">]</span>
            <span class="token comment"># bos.shape = batch_size 个 bos-id</span>
            bos <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>tgt_vocab<span class="token punctuation">[</span><span class="token string">'&lt;bos>'</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">*</span> Y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                          device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token comment"># dec_input.shape = (batch_size, num_steps)</span>
            <span class="token comment"># 解码器的输入通常由序列的起始标志 bos 和目标序列（去掉末尾的部分 Y[:, :-1]）组成。</span>
            dec_input <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>bos<span class="token punctuation">,</span> Y<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 强制教学</span>
            <span class="token comment"># Y_hat的形状:(batch_size,num_steps,vocab_size)</span>
            Y_hat<span class="token punctuation">,</span> _ <span class="token operator">=</span> net<span class="token punctuation">(</span>X<span class="token punctuation">,</span> dec_input<span class="token punctuation">,</span> X_valid_len<span class="token punctuation">)</span>
            l <span class="token operator">=</span> loss<span class="token punctuation">(</span>Y_hat<span class="token punctuation">,</span> Y<span class="token punctuation">,</span> Y_valid_len<span class="token punctuation">)</span>
            l<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>      <span class="token comment"># 损失函数的标量进行“反向传播”</span>
            grad_clipping<span class="token punctuation">(</span>net<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
            num_tokens <span class="token operator">=</span> Y_valid_len<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
            optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                metric<span class="token punctuation">.</span>add<span class="token punctuation">(</span>l<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> num_tokens<span class="token punctuation">)</span>

        <span class="token keyword">if</span> <span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">10</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            <span class="token comment"># print(predict('你是？'))</span>
            <span class="token comment"># print(epoch)</span>
            <span class="token comment"># animator.add(epoch + 1, )</span>

            <span class="token keyword">if</span> epoch <span class="token operator">==</span> <span class="token number">9</span><span class="token punctuation">:</span>
                <span class="token comment"># 清空图表：使用空数组来替换现有内容</span>
                vis<span class="token punctuation">.</span>line<span class="token punctuation">(</span>X<span class="token operator">=</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> Y<span class="token operator">=</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> win<span class="token operator">=</span><span class="token string">'train_ch8'</span><span class="token punctuation">,</span> update<span class="token operator">=</span><span class="token string">'replace'</span><span class="token punctuation">)</span>
            <span class="token comment"># _loss_val = l</span>
            <span class="token comment"># _loss_val = _loss_val.cpu().sum().detach().numpy()</span>
            vis<span class="token punctuation">.</span>line<span class="token punctuation">(</span>
                X<span class="token operator">=</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                Y<span class="token operator">=</span><span class="token punctuation">[</span> metric<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">/</span> metric<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                win<span class="token operator">=</span><span class="token string">'train_ch8'</span><span class="token punctuation">,</span>
                update<span class="token operator">=</span><span class="token string">'append'</span><span class="token punctuation">,</span>
                opts<span class="token operator">=</span><span class="token punctuation">&#123;</span>
                    <span class="token string">'title'</span><span class="token punctuation">:</span> <span class="token string">'train_ch8'</span><span class="token punctuation">,</span>
                    <span class="token string">'xlabel'</span><span class="token punctuation">:</span> <span class="token string">'epoch'</span><span class="token punctuation">,</span>
                    <span class="token string">'ylabel'</span><span class="token punctuation">:</span> <span class="token string">'loss'</span><span class="token punctuation">,</span>
                    <span class="token string">'linecolor'</span><span class="token punctuation">:</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">255</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment"># 蓝色线条</span>
                <span class="token punctuation">&#125;</span>
            <span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'loss </span><span class="token interpolation"><span class="token punctuation">&#123;</span>metric<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">/</span> metric<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">&#125;</span></span><span class="token string">, </span><span class="token interpolation"><span class="token punctuation">&#123;</span>metric<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">/</span> timer<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.1f</span><span class="token punctuation">&#125;</span></span><span class="token string"> '</span></span>
        <span class="token string-interpolation"><span class="token string">f'tokens/sec on </span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">str</span><span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
    torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>net<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'model_h.pt'</span><span class="token punctuation">)</span>  <span class="token comment"># [[6]]</span>
    torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>net<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'model.pt'</span><span class="token punctuation">)</span>  <span class="token comment"># [[6]]</span>

<span class="token keyword">def</span> <span class="token function">predict_seq2seq</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> src_sentence<span class="token punctuation">,</span> src_vocab<span class="token punctuation">,</span> tgt_vocab<span class="token punctuation">,</span> num_steps<span class="token punctuation">,</span>
                    device<span class="token punctuation">,</span> save_attention_weights<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""序列到序列模型的预测"""</span>
    <span class="token comment"># 在预测时将net设置为评估模式</span>
    net<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    src_tokens <span class="token operator">=</span> src_vocab<span class="token punctuation">[</span>src_sentence<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">[</span>
        src_vocab<span class="token punctuation">[</span><span class="token string">'&lt;eos>'</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
    enc_valid_len <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>src_tokens<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>
    src_tokens <span class="token operator">=</span> dataset<span class="token punctuation">.</span>truncate_pad<span class="token punctuation">(</span>src_tokens<span class="token punctuation">,</span> num_steps<span class="token punctuation">,</span> src_vocab<span class="token punctuation">[</span><span class="token string">'&lt;pad>'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token comment"># 添加批量轴</span>
    enc_X <span class="token operator">=</span> torch<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>
        torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>src_tokens<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
    enc_outputs <span class="token operator">=</span> net<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>enc_X<span class="token punctuation">,</span> enc_valid_len<span class="token punctuation">)</span>
    dec_state <span class="token operator">=</span> net<span class="token punctuation">.</span>decoder<span class="token punctuation">.</span>init_state<span class="token punctuation">(</span>enc_outputs<span class="token punctuation">,</span> enc_valid_len<span class="token punctuation">)</span>
    <span class="token comment"># 添加批量轴</span>
    dec_X <span class="token operator">=</span> torch<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>
        <span class="token punctuation">[</span>tgt_vocab<span class="token punctuation">[</span><span class="token string">'&lt;bos>'</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
    output_seq<span class="token punctuation">,</span> attention_weight_seq <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_steps<span class="token punctuation">)</span><span class="token punctuation">:</span>
        Y<span class="token punctuation">,</span> dec_state <span class="token operator">=</span> net<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>dec_X<span class="token punctuation">,</span> dec_state<span class="token punctuation">)</span>
        <span class="token comment"># 我们使用具有预测最高可能性的词元，作为解码器在下一时间步的输入</span>
        dec_X <span class="token operator">=</span> Y<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        pred <span class="token operator">=</span> dec_X<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>int32<span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 保存注意力权重（稍后讨论）</span>
        <span class="token keyword">if</span> save_attention_weights<span class="token punctuation">:</span>
            attention_weight_seq<span class="token punctuation">.</span>append<span class="token punctuation">(</span>net<span class="token punctuation">.</span>decoder<span class="token punctuation">.</span>attention_weights<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>num_steps<span class="token punctuation">)</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># 一旦序列结束词元被预测，输出序列的生成就完成了</span>
        <span class="token keyword">if</span> pred <span class="token operator">==</span> tgt_vocab<span class="token punctuation">[</span><span class="token string">'&lt;eos>'</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
            <span class="token keyword">break</span>
        output_seq<span class="token punctuation">.</span>append<span class="token punctuation">(</span>pred<span class="token punctuation">)</span>
    <span class="token keyword">return</span> <span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>tgt_vocab<span class="token punctuation">.</span>to_tokens<span class="token punctuation">(</span>output_seq<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> attention_weight_seq


<span class="token keyword">def</span> <span class="token function">bleu</span><span class="token punctuation">(</span>pred_seq<span class="token punctuation">,</span> label_seq<span class="token punctuation">,</span> k<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""计算BLEU"""</span>
    pred_tokens<span class="token punctuation">,</span> label_tokens <span class="token operator">=</span> pred_seq<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>i <span class="token keyword">for</span> i <span class="token keyword">in</span> label_seq<span class="token punctuation">]</span>
    len_pred<span class="token punctuation">,</span> len_label <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>pred_tokens<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>label_tokens<span class="token punctuation">)</span>
    score <span class="token operator">=</span> math<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token builtin">min</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span> <span class="token operator">-</span> len_label <span class="token operator">/</span> len_pred<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> n <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> k <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        num_matches<span class="token punctuation">,</span> label_subs <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> collections<span class="token punctuation">.</span>defaultdict<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>len_label <span class="token operator">-</span> n <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            label_subs<span class="token punctuation">[</span><span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>label_tokens<span class="token punctuation">[</span>i<span class="token punctuation">:</span> i <span class="token operator">+</span> n<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">1</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>len_pred <span class="token operator">-</span> n <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> label_subs<span class="token punctuation">[</span><span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>pred_tokens<span class="token punctuation">[</span>i<span class="token punctuation">:</span> i <span class="token operator">+</span> n<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>
                num_matches <span class="token operator">+=</span> <span class="token number">1</span>
                label_subs<span class="token punctuation">[</span><span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>pred_tokens<span class="token punctuation">[</span>i<span class="token punctuation">:</span> i <span class="token operator">+</span> n<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">-=</span> <span class="token number">1</span>
        score <span class="token operator">*=</span> math<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>num_matches <span class="token operator">/</span> <span class="token punctuation">(</span>len_pred <span class="token operator">-</span> n <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> math<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> score


<span class="token keyword">from</span> matplotlib <span class="token keyword">import</span> pyplot <span class="token keyword">as</span> plt
<span class="token keyword">import</span> matplotlib
<span class="token comment"># from matplotlib_inline import backend_inline</span>
<span class="token keyword">def</span> <span class="token function">show_heatmaps</span><span class="token punctuation">(</span>matrices<span class="token punctuation">,</span> xlabel<span class="token punctuation">,</span> ylabel<span class="token punctuation">,</span> titles<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2.5</span><span class="token punctuation">,</span> <span class="token number">2.5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                  cmap<span class="token operator">=</span><span class="token string">'Reds'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    显示矩阵的热图（Heatmaps）。
    这个函数旨在以子图网格的形式绘制多个矩阵，通常用于可视化注意力权重等。

    参数:
        matrices (numpy.ndarray 或 torch.Tensor 数组): 
            一个四维数组，形状应为 (num_rows, num_cols, height, width)。
            其中，num_rows 和 num_cols 决定了子图网格的布局，
            height 和 width 是每个热图（即每个矩阵）的维度。
        xlabel (str): 
            所有最底行子图的 x 轴标签。
        ylabel (str): 
            所有最左列子图的 y 轴标签。
        titles (list of str, optional): 
            一个包含 num_cols 个标题的列表，用于设置每一列子图的标题。默认 None。
        figsize (tuple, optional): 
            整个图形（figure）的大小。默认 (2.5, 2.5)。
        cmap (str, optional): 
            用于绘制热图的颜色映射（colormap）。默认 'Reds'。
    """</span>
    <span class="token comment"># 导入所需的 matplotlib 模块，确保图形在 Jupyter/IPython 环境中正确显示为 SVG 格式</span>
    <span class="token comment"># （假设在包含这个函数的环境中已经导入了 matplotlib 的 backend_inline）</span>
    <span class="token comment"># backend_inline.set_matplotlib_formats('svg')</span>
    matplotlib<span class="token punctuation">.</span>use<span class="token punctuation">(</span><span class="token string">'TkAgg'</span><span class="token punctuation">)</span>
    <span class="token comment"># 从输入的 matrices 形状中解构出子图网格的行数和列数</span>
    <span class="token comment"># 假设 matrices 的形状是 (num_rows, num_cols, height, width)</span>
    num_rows<span class="token punctuation">,</span> num_cols<span class="token punctuation">,</span> _<span class="token punctuation">,</span> _ <span class="token operator">=</span> matrices<span class="token punctuation">.</span>shape
    
    <span class="token comment"># 创建一个包含多个子图（axes）的图形（fig）</span>
    <span class="token comment"># fig: 整个图形对象</span>
    <span class="token comment"># axes: 一个 num_rows x num_cols 的子图对象数组</span>
    fig<span class="token punctuation">,</span> axes <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span>
        num_rows<span class="token punctuation">,</span> num_cols<span class="token punctuation">,</span> 
        figsize<span class="token operator">=</span>figsize<span class="token punctuation">,</span>
        sharex<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    <span class="token comment"># 所有子图共享 x 轴刻度</span>
        sharey<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    <span class="token comment"># 所有子图共享 y 轴刻度</span>
        squeeze<span class="token operator">=</span><span class="token boolean">False</span>   <span class="token comment"># 即使只有一行或一列，也强制返回二维数组的 axes，方便后续循环</span>
    <span class="token punctuation">)</span>
    
    <span class="token comment"># 遍历子图的行和对应的矩阵行</span>
    <span class="token comment"># i 是行索引, row_axes 是当前行的子图数组, row_matrices 是当前行的矩阵数组</span>
    <span class="token keyword">for</span> i<span class="token punctuation">,</span> <span class="token punctuation">(</span>row_axes<span class="token punctuation">,</span> row_matrices<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span><span class="token builtin">zip</span><span class="token punctuation">(</span>axes<span class="token punctuation">,</span> matrices<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 遍历当前行中的子图和对应的矩阵</span>
        <span class="token comment"># j 是列索引, ax 是当前的子图对象, matrix 是当前的待绘矩阵</span>
        <span class="token keyword">for</span> j<span class="token punctuation">,</span> <span class="token punctuation">(</span>ax<span class="token punctuation">,</span> matrix<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span><span class="token builtin">zip</span><span class="token punctuation">(</span>row_axes<span class="token punctuation">,</span> row_matrices<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            
            <span class="token comment"># 使用 ax.imshow() 绘制热图</span>
            <span class="token comment"># matrix.detach().numpy()：将 PyTorch Tensor 转换为 numpy 数组，并从计算图中分离（如果它是 Tensor）</span>
            <span class="token comment"># cmap：指定颜色映射</span>
            pcm <span class="token operator">=</span> ax<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>matrix<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cmap<span class="token operator">=</span>cmap<span class="token punctuation">)</span>
            
            <span class="token comment"># --- 设置轴标签和标题 ---</span>
            
            <span class="token comment"># 只有最底行 (i == num_rows - 1) 的子图才显示 x 轴标签</span>
            <span class="token keyword">if</span> i <span class="token operator">==</span> num_rows <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">:</span>
                ax<span class="token punctuation">.</span>set_xlabel<span class="token punctuation">(</span>xlabel<span class="token punctuation">)</span>
                
            <span class="token comment"># 只有最左列 (j == 0) 的子图才显示 y 轴标签</span>
            <span class="token keyword">if</span> j <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
                ax<span class="token punctuation">.</span>set_ylabel<span class="token punctuation">(</span>ylabel<span class="token punctuation">)</span>
                
            <span class="token comment"># 如果提供了标题列表，则设置当前列的子图标题（所有行共享列标题）</span>
            <span class="token keyword">if</span> titles<span class="token punctuation">:</span>
                ax<span class="token punctuation">.</span>set_title<span class="token punctuation">(</span>titles<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">)</span>
                
    <span class="token comment"># --- 添加颜色条（Colorbar） ---</span>
    
    <span class="token comment"># 为整个图形添加一个颜色条，用于表示数值和颜色的对应关系</span>
    <span class="token comment"># pcm: 之前绘制的第一个热图返回的 Colormap </span>
    <span class="token comment"># ax=axes: 颜色条将参照整个子图网格进行定位和缩放</span>
    <span class="token comment"># shrink=0.6: 缩小颜色条的高度/长度，使其只占图形高度的 60%</span>
    fig<span class="token punctuation">.</span>colorbar<span class="token punctuation">(</span>pcm<span class="token punctuation">,</span> ax<span class="token operator">=</span>axes<span class="token punctuation">,</span> shrink<span class="token operator">=</span><span class="token number">0.6</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    embed_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> num_layers<span class="token punctuation">,</span> dropout <span class="token operator">=</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0.1</span>
    batch_size<span class="token punctuation">,</span> num_steps <span class="token operator">=</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">10</span>
    lr<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> device <span class="token operator">=</span> <span class="token number">0.005</span><span class="token punctuation">,</span> <span class="token number">2000</span><span class="token punctuation">,</span> try_gpu<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># train_iter 每个迭代输出：(batch_size, num_steps)</span>
    train_iter<span class="token punctuation">,</span> src_vocab<span class="token punctuation">,</span> tgt_vocab<span class="token punctuation">,</span> source<span class="token punctuation">,</span> target <span class="token operator">=</span> dataset<span class="token punctuation">.</span>load_data<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> num_steps<span class="token punctuation">)</span>
    encoder <span class="token operator">=</span> Seq2SeqEncoder<span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>src_vocab<span class="token punctuation">)</span><span class="token punctuation">,</span> embed_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> num_layers<span class="token punctuation">,</span>
                        dropout<span class="token punctuation">)</span>
    decoder <span class="token operator">=</span> Seq2SeqAttentionDecoder<span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>tgt_vocab<span class="token punctuation">)</span><span class="token punctuation">,</span> embed_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> num_layers<span class="token punctuation">,</span>
                            dropout<span class="token punctuation">)</span>
    net <span class="token operator">=</span> EncoderDecoder<span class="token punctuation">(</span>encoder<span class="token punctuation">,</span> decoder<span class="token punctuation">)</span>

    is_train <span class="token operator">=</span> <span class="token boolean">False</span>
    is_show <span class="token operator">=</span> <span class="token boolean">False</span>
    <span class="token keyword">if</span> is_train<span class="token punctuation">:</span>
        train_seq2seq<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> tgt_vocab<span class="token punctuation">,</span> device<span class="token punctuation">)</span>
    <span class="token keyword">elif</span> is_show<span class="token punctuation">:</span>
        state_dict <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'model_h.pt'</span><span class="token punctuation">)</span>
        net<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>state_dict<span class="token punctuation">)</span>
        net<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>

        src_text <span class="token operator">=</span> <span class="token string">"Call us."</span>
        translation<span class="token punctuation">,</span> attention_weight_seq <span class="token operator">=</span> predict_seq2seq<span class="token punctuation">(</span>
                net<span class="token punctuation">,</span> src_text<span class="token punctuation">,</span> src_vocab<span class="token punctuation">,</span> tgt_vocab<span class="token punctuation">,</span> num_steps<span class="token punctuation">,</span> device<span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token comment"># attention_weights = torch.eye(10).reshape((1, 1, 10, 10))</span>
        <span class="token comment"># (num_rows, num_cols, height, width)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'translation=</span><span class="token interpolation"><span class="token punctuation">&#123;</span>translation<span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>attention_weight_seq<span class="token punctuation">)</span>

        stacked_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>attention_weight_seq<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
        stacked_tensor <span class="token operator">=</span> stacked_tensor<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
        show_heatmaps<span class="token punctuation">(</span>
            stacked_tensor<span class="token punctuation">,</span>
            xlabel<span class="token operator">=</span><span class="token string">'Attention weight'</span><span class="token punctuation">,</span> ylabel<span class="token operator">=</span><span class="token string">'Decode Step'</span><span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        state_dict <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'model_h.pt'</span><span class="token punctuation">)</span>
        net<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>state_dict<span class="token punctuation">)</span>
        net<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
        C <span class="token operator">=</span> <span class="token number">0</span>
        C1 <span class="token operator">=</span> <span class="token number">0</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">2000</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># print(source[i])</span>
            <span class="token comment"># print(target[i])</span>
            translation<span class="token punctuation">,</span> attention_weight_seq <span class="token operator">=</span> predict_seq2seq<span class="token punctuation">(</span>
                net<span class="token punctuation">,</span> source<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> src_vocab<span class="token punctuation">,</span> tgt_vocab<span class="token punctuation">,</span> num_steps<span class="token punctuation">,</span> device<span class="token punctuation">)</span>
            
            score <span class="token operator">=</span> bleu<span class="token punctuation">(</span>translation<span class="token punctuation">,</span> target<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> k<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
            <span class="token keyword">if</span> score <span class="token operator">></span> <span class="token number">0.0</span><span class="token punctuation">:</span>
                C <span class="token operator">=</span> C <span class="token operator">+</span> <span class="token number">1</span>
                <span class="token keyword">if</span> score <span class="token operator">></span> <span class="token number">0.8</span><span class="token punctuation">:</span>
                    C1 <span class="token operator">=</span> C1 <span class="token operator">+</span> <span class="token number">1</span>
                <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'</span><span class="token interpolation"><span class="token punctuation">&#123;</span>source<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">&#125;</span></span><span class="token string"> => </span><span class="token interpolation"><span class="token punctuation">&#123;</span>translation<span class="token punctuation">&#125;</span></span><span class="token string">, bleu </span><span class="token interpolation"><span class="token punctuation">&#123;</span>score<span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>

        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Counter(bleu > 0) = </span><span class="token interpolation"><span class="token punctuation">&#123;</span>C<span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Valid-Counter(bleu > 0.8) = </span><span class="token interpolation"><span class="token punctuation">&#123;</span>C1<span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>  下面是encoder过程的简单分析：</p>
<ol>
<li class="lvl-3">
<p>将x通过nn.Embedding得到了(batch_size,num_steps,embed_size)的输入嵌入向量。</p>
</li>
<li class="lvl-3">
<p>将嵌入向量传给nn.GRU，得到了两个输出，并返回：</p>
<ul class="lvl-2">
<li class="lvl-6">output，最后一层rnn的所有时间步的隐藏状态（num_steps,batch_size,num_hiddens）。</li>
<li class="lvl-6">h_n,所有rnn层的，最后一个时间步的隐藏状态（num_layers,batch_size,num_hiddens）。</li>
</ul>
</li>
</ol>
<p>  下面是decoder过程的简单分析：</p>
<ol>
<li class="lvl-3">
<p>将decoder_x通过nn.Embedding得到了(batch_size,num_steps,embed_size)的输入嵌入向量。</p>
</li>
<li class="lvl-3">
<p>将嵌入向量沿着num_steps进行单步运行，每一步经过Attention过程，得到最终的output，以及最后一个时间步的所有rnn层的h_n，每一步执行如下步骤：</p>
<ul class="lvl-2">
<li class="lvl-6">将rnn最后一层的隐藏态作为Q（第一次Q是来自于encoder，后续都是decoder的每一次运行过程产生的隐藏态）</li>
<li class="lvl-6">将encoder的output作为K,V，得到当前动态的上下文 context</li>
<li class="lvl-6">将decoder_x_step 和 context进行组合，得到decoder_x_step_new</li>
<li class="lvl-6">将decoder_x_step_new送入nn.GRU，得到当前时间步的output, h_t</li>
<li class="lvl-6">将每一步的output收集起来作为输出，将h_t作为下一个时间步的Q循环起来</li>
</ul>
</li>
<li class="lvl-3">
<p>将所有的output经过nn.LazyLinear 映射为(num_steps, batch_size, vocab_size)，并和h_t返回</p>
</li>
</ol>
<p>  和原版本的seq2seq进行对比可知：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>在原版中，我们的decoder依赖于一个固定的enc_outputs进行循环解码</p>
</li>
<li class="lvl-2">
<p>在新版中，我们的decoder每次界面，都会有一个Q（第一次是来至于encoder，后续都是decoder的每一次运行过程产生的隐藏态）来计算enc_outputs的权重分数，然后根据权重分数得到一个动态的enc_outputs，这样可以让解码器每一步都关注enc_outputs中的不同的重点。</p>
</li>
</ul>
<p>  attention weight 的解释：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>把Encoder Output（num_steps,1,num_hiddens）作为K,V</p>
</li>
<li class="lvl-2">
<p>将Decoder的隐藏态h_t（1,1,num_hiddens）(初始值来自于Encoder的隐藏态h_t)作为Q，计算出当前step的attention_weight，其是一个softmax概率数据。</p>
</li>
<li class="lvl-2">
<p>然后将attention_weight 与 V进行计算，代表模型当前关注EncoderOutput的那部分数据，得到新的Context</p>
</li>
</ul>
<p>  下面是训练和测试的一些结果</p>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/blog_idx_143/train.png" alt="rep_img"/></center>
    </div>
</div>   
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/blog_idx_143/ret.png" alt="rep_img"/></center>
    </div>
</div>   
<p>  从上面的图可以看到，这个模型有一定的翻译效果，并且，比上一篇文章的模型效果要好一点。</p>
<p>  此外，下面是我们翻译：“Call us.”-&gt; “联 系 我 们 。” 的attention weight的可视化（带mask=3，在不同的decode step中权重变化。）</p>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/blog_idx_143/attention_weight.png" alt="rep_img"/></center>
    </div>
</div>   
<p>  从上面的图可以知道，每一个decode step的注意力权重矩阵值都不一样，意味着，每一步解码的时候，关注的内容也不一样。</p>
<br/>
<br/>
<br/>
<br/>
<h3 id="后记">后记</h3>
<hr>
<p>  本文引入了注意力机制，及注意力机制在seq2seq中，在应用注意力机制后，和原版的seq2seq的结论相比，模型效果有提升。</p>
<h3 id="参考文献">参考文献</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://zh.d2l.ai/chapter_recurrent-modern/encoder-decoder.html">https://zh.d2l.ai/chapter_recurrent-modern/encoder-decoder.html</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://zh.d2l.ai/chapter_recurrent-modern/seq2seq.html">https://zh.d2l.ai/chapter_recurrent-modern/seq2seq.html</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://d2l.ai/chapter_attention-mechanisms-and-transformers/bahdanau-attention.html">https://d2l.ai/chapter_attention-mechanisms-and-transformers/bahdanau-attention.html</a></p>
</li>
</ul>
<br/>
<br/>
<div style="margin:50px auto;">
</div>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <hr/>
        <center><font color = #91e0b0 size = 5>打赏、订阅、收藏、丢香蕉、硬币，请关注公众号（攻城狮的搬砖之路）</font></center>
    </div>
</div>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/qrcode_for_wx_official_account.jpg" alt="qrc_img"/></center>
    </div>
</div>
<!-- ![alt 公众号图片](https://flyinskyin2013.github.io/ImageBed0/blogs/qrcode_for_wx_official_account.jpg "公众号图片") -->
<p><font color="red" size="7">PS: 请尊重原创，不喜勿喷。</font><br/><br>
<font color="red" size="7">PS: 要转载请注明出处，本人版权所有。</font><br/><br>
<font color="red" size="7">PS: 有问题请留言，看到后我会第一时间回复。</font><br/></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://e-x.top/2025/10/19/blog_idx_142/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Sky">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sky's Blogs">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | Sky's Blogs">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/10/19/blog_idx_142/" class="post-title-link" itemprop="url">大模型基础补全计划(五)---seq2seq实例与测试(编码器、解码器架构)</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-10-19 09:57:00 / 修改时间：10:00:26" itemprop="dateCreated datePublished" datetime="2025-10-19T09:57:00+08:00">2025-10-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/LLM/" itemprop="url" rel="index"><span itemprop="name">LLM</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/LLM/LM/" itemprop="url" rel="index"><span itemprop="name">LM</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>5.2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>19 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <script src="\assets\js\APlayer.min.js"> </script><!--
 * @Description: 
 * @Author: Sky
 * @Date: 2020-08-24 16:37:34
 * @LastEditors: Sky
 * @LastEditTime: 2021-06-29 15:04:48
 * @Github: https://github.com/flyinskyin2013/
-->
<p><font color="red" size="7">PS：要转载请注明出处，本人版权所有。</font><br/><br>
<font color="red" size="7">PS: 这个只是基于《我自己》的理解，</font><br/><font color="red" size="7">如果和你的原则及想法相冲突，请谅解，勿喷。</font><br/></p>
<!-- ###### 前置说明
&emsp;&emsp;本文作为本人github blog/cnblog的主站的备份。（BlogID=142） 
&emsp;&emsp;本文发布于 2025-10-19 09:57:00            （BlogID=142） 
-->
<h6 id="环境说明">环境说明</h6>
<p>  无</p>
<h3 id="前言">前言</h3>
<hr>
<p>      本文是这个系列第五篇，它们是：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>《大模型基础补全计划(一)—重温一些深度学习相关的数学知识》 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/Iflyinsky/p/18717317">https://www.cnblogs.com/Iflyinsky/p/18717317</a></p>
</li>
<li class="lvl-2">
<p>《大模型基础补全计划(二)—词嵌入(word embedding) 》 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/Iflyinsky/p/18775451">https://www.cnblogs.com/Iflyinsky/p/18775451</a></p>
</li>
<li class="lvl-2">
<p>《大模型基础补全计划(三)—RNN实例与测试》 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/Iflyinsky/p/18967569">https://www.cnblogs.com/Iflyinsky/p/18967569</a></p>
</li>
<li class="lvl-2">
<p>《大模型基础补全计划(四)—LSTM的实例与测试(RNN的改进)》 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/Iflyinsky/p/19091089">https://www.cnblogs.com/Iflyinsky/p/19091089</a></p>
</li>
</ul>
<p>  本文，我们先简单介绍一下编码器-解码器架构，然后介绍一个基于这种架构的机翻模型seq2seq的简单实例。</p>
<br/>
<br/>
<br/>
<br/>
<h3 id="编码器-解码器（encoder-decoder）架构">编码器-解码器（encoder-decoder）架构</h3>
<hr>
<p>   前面的文章中我们的模型示例都是根据已有的文字序列，续写N个字。在自然语言处理中，还有有一类需求也是比较经典，那就是机器翻译。</p>
<p>   对于机器翻译来说，其核心就是将一种语言翻译为另外一种语言，换句话说就是一种序列数据到另外一种序列数据。从这里来看，出现了两种序列数据，那么必然的很容易想到类似两个RNN的独立网络来处理这种任务，基于这种情况，有人提出了编码器-解码器架构，下图是这种架构的示意图。</p>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/blog_idx_142/encoder_decoder.png" alt="rep_img"/></center>
    </div>
</div>   
<p>  从示意图可知，这种架构的核心就是处理输入序列，得到中间状态，将中间状态传给解码器，解码器负责生成输出序列。对于翻译任务来说，输入序列就是原文，输出序列就是译文。</p>
<p>  这里说起来还是概念性的，我们下面从一个经典的编码器、解码器结构的模型来实际 演示一下翻译需求的模型是什么样子的。</p>
<br/>
<br/>
<br/>
<br/>
<h3 id="基于-seq2seq-的-英文翻译中文-的实例">基于 seq2seq 的  英文翻译中文  的实例</h3>
<hr>
<p>  </p>
<br/>
<br/>
<h5 id="英文中文翻译数据集">英文中文翻译数据集</h5>
<p>   首先数据集下载地址是<a target="_blank" rel="noopener" href="http://www.manythings.org/anki/">http://www.manythings.org/anki/</a> 中的cmn-eng.zip 文件，其内部的数据集格式大概如下：</p>
<pre class="line-numbers language-txt" data-language="txt"><code class="language-txt">I try.	我试试。	CC-BY 2.0 (France) Attribution: tatoeba.org #20776 (CK) &amp; #8870261 (will66)
I won!	我赢了。	CC-BY 2.0 (France) Attribution: tatoeba.org #2005192 (CK) &amp; #5102367 (mirrorvan)
Oh no!	不会吧。	CC-BY 2.0 (France) Attribution: tatoeba.org #1299275 (CK) &amp; #5092475 (mirrorvan)
Cheers!	乾杯!	CC-BY 2.0 (France) Attribution: tatoeba.org #487006 (human600) &amp; #765577 (Martha)
Got it?	知道了没有？	CC-BY 2.0 (France) Attribution: tatoeba.org #455353 (CM) &amp; #455357 (GlossaMatik)
Got it?	懂了吗？	CC-BY 2.0 (France) Attribution: tatoeba.org #455353 (CM) &amp; #2032276 (ydcok)
Got it?	你懂了吗？	CC-BY 2.0 (France) Attribution: tatoeba.org #455353 (CM) &amp; #7768205 (jiangche)
He ran.	他跑了。	CC-BY 2.0 (France) Attribution: tatoeba.org #672229 (CK) &amp; #5092389 (mirrorvan)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>   由于我的卡（3060 12G）有点拉库，为了效率，因此整个数据集只用前面2千条即可。</p>
<br/>
<br/>
<br/>
<br/>
<h5 id="文本预处理">文本预处理</h5>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># dataset.py</span>
<span class="token keyword">import</span> collections
<span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils <span class="token keyword">import</span> data


<span class="token comment"># 下面返回的数据是：</span>
<span class="token comment"># [['Hi.', '嗨。'], ['Hi.', '你好。'], ['Run.', '你用跑的。'], ['Stop!', '住手！'], ['Wait!', '等等！'], ... ...]</span>
<span class="token keyword">def</span> <span class="token function">read_data</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'cmn-eng/cmn.txt'</span><span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">,</span>
             encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
        lines <span class="token operator">=</span> f<span class="token punctuation">.</span>readlines<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
    <span class="token keyword">return</span> <span class="token punctuation">[</span>line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"	"</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token keyword">for</span> line <span class="token keyword">in</span> lines<span class="token punctuation">]</span>
    

<span class="token comment"># 输出是：</span>
<span class="token comment"># [['Hi.'], ['Hi.'], ['Run.'], ['Stop!'], ['Wait!']]</span>
<span class="token comment"># [['嗨', '。'], ['你', '好', '。'], ['你', '用', '跑', '的', '。'], ['住', '手', '！'], ['等', '等', '！']]</span>
<span class="token comment"># ['Hi.', 'Hi.', 'Run.', 'Stop!', 'Wait!']</span>
<span class="token comment"># ['嗨。', '你好。', '你用跑的。', '住手！', '等等！']</span>
<span class="token keyword">def</span> <span class="token function">tokenize</span><span class="token punctuation">(</span>lines<span class="token punctuation">,</span> token<span class="token operator">=</span><span class="token string">'char'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""将文本行拆分为单词或字符词元"""</span>
    source_tokenize<span class="token punctuation">,</span> target_tokenize <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    source_line<span class="token punctuation">,</span> target_line <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'dataset len = </span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">len</span><span class="token punctuation">(</span>lines<span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> line <span class="token keyword">in</span> lines<span class="token punctuation">:</span>

        s <span class="token operator">=</span> line<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        t <span class="token operator">=</span> line<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
        source_line<span class="token punctuation">.</span>append<span class="token punctuation">(</span>s<span class="token punctuation">)</span>
        target_line<span class="token punctuation">.</span>append<span class="token punctuation">(</span>t<span class="token punctuation">)</span>
        source_tokenize<span class="token punctuation">.</span>append<span class="token punctuation">(</span>s<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        target_tokenize<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">[</span>word <span class="token keyword">for</span> word <span class="token keyword">in</span> t<span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token keyword">return</span> source_tokenize<span class="token punctuation">,</span> target_tokenize<span class="token punctuation">,</span> source_line<span class="token punctuation">,</span> target_line

<span class="token comment"># 词元的类型是字符串，而模型需要的输入是数字，因此这种类型不方便模型使用。 现在，让我们构建一个字典，</span>
<span class="token comment"># 通常也叫做词表（vocabulary）， 用来将字符串类型的词元映射到从开始的数字索引中。</span>
<span class="token keyword">def</span> <span class="token function">count_corpus</span><span class="token punctuation">(</span>tokens<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""统计词元的频率"""</span>
    <span class="token comment"># 这里的tokens是1D列表或2D列表</span>
    <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>tokens<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">or</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>tokens<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">list</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 将词元列表展平成一个列表</span>
        tokens <span class="token operator">=</span> <span class="token punctuation">[</span>token <span class="token keyword">for</span> line <span class="token keyword">in</span> tokens <span class="token keyword">for</span> token <span class="token keyword">in</span> line<span class="token punctuation">]</span>
    <span class="token keyword">return</span> collections<span class="token punctuation">.</span>Counter<span class="token punctuation">(</span>tokens<span class="token punctuation">)</span>

<span class="token comment"># 返回类似&#123;'l': 3, 'o': 2, 'h': 1, 'e': 1, ' ': 1, 'w': 1, 'r': 1, 'd': 1&#125;的一个字典</span>
<span class="token keyword">class</span> <span class="token class-name">Vocab</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""文本词表"""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> tokens<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> min_freq<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> reserved_tokens<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> tokens <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            tokens <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">if</span> reserved_tokens <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            reserved_tokens <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token comment"># 按出现频率排序</span>
        <span class="token comment"># 对于Counter("hello world")，结果如下</span>
        <span class="token comment"># Counter(&#123;'l': 3, 'o': 2, 'h': 1, 'e': 1, ' ': 1, 'w': 1, 'r': 1, 'd': 1&#125;)</span>
        counter <span class="token operator">=</span> count_corpus<span class="token punctuation">(</span>tokens<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>_token_freqs <span class="token operator">=</span> <span class="token builtin">sorted</span><span class="token punctuation">(</span>counter<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> key<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                                   reverse<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token comment"># 未知词元的索引为0</span>
        self<span class="token punctuation">.</span>idx_to_token <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'&lt;unk>'</span><span class="token punctuation">]</span> <span class="token operator">+</span> reserved_tokens
        self<span class="token punctuation">.</span>token_to_idx <span class="token operator">=</span> <span class="token punctuation">&#123;</span>token<span class="token punctuation">:</span> idx
                             <span class="token keyword">for</span> idx<span class="token punctuation">,</span> token <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>idx_to_token<span class="token punctuation">)</span><span class="token punctuation">&#125;</span>
        <span class="token keyword">for</span> token<span class="token punctuation">,</span> freq <span class="token keyword">in</span> self<span class="token punctuation">.</span>_token_freqs<span class="token punctuation">:</span>
            <span class="token keyword">if</span> freq <span class="token operator">&lt;</span> min_freq<span class="token punctuation">:</span>
                <span class="token keyword">break</span>
            <span class="token keyword">if</span> token <span class="token keyword">not</span> <span class="token keyword">in</span> self<span class="token punctuation">.</span>token_to_idx<span class="token punctuation">:</span>
                self<span class="token punctuation">.</span>idx_to_token<span class="token punctuation">.</span>append<span class="token punctuation">(</span>token<span class="token punctuation">)</span>
                self<span class="token punctuation">.</span>token_to_idx<span class="token punctuation">[</span>token<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>idx_to_token<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span>

    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>idx_to_token<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> tokens<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token keyword">not</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>tokens<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">,</span> <span class="token builtin">tuple</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">return</span> self<span class="token punctuation">.</span>token_to_idx<span class="token punctuation">.</span>get<span class="token punctuation">(</span>tokens<span class="token punctuation">,</span> self<span class="token punctuation">.</span>unk<span class="token punctuation">)</span>
        <span class="token keyword">return</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>__getitem__<span class="token punctuation">(</span>token<span class="token punctuation">)</span> <span class="token keyword">for</span> token <span class="token keyword">in</span> tokens<span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">to_tokens</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> indices<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token keyword">not</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>indices<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">,</span> <span class="token builtin">tuple</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">return</span> self<span class="token punctuation">.</span>idx_to_token<span class="token punctuation">[</span>indices<span class="token punctuation">]</span>
        <span class="token keyword">return</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>idx_to_token<span class="token punctuation">[</span>index<span class="token punctuation">]</span> <span class="token keyword">for</span> index <span class="token keyword">in</span> indices<span class="token punctuation">]</span>

    <span class="token decorator annotation punctuation">@property</span>
    <span class="token keyword">def</span> <span class="token function">unk</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 未知词元的索引为0</span>
        <span class="token keyword">return</span> <span class="token number">0</span>

    <span class="token decorator annotation punctuation">@property</span>
    <span class="token keyword">def</span> <span class="token function">token_freqs</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>_token_freqs    
    



<span class="token keyword">def</span> <span class="token function">truncate_pad</span><span class="token punctuation">(</span>line<span class="token punctuation">,</span> num_steps<span class="token punctuation">,</span> padding_token<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""截断或填充文本序列"""</span>
    <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>line<span class="token punctuation">)</span> <span class="token operator">></span> num_steps<span class="token punctuation">:</span>
        <span class="token keyword">return</span> line<span class="token punctuation">[</span><span class="token punctuation">:</span>num_steps<span class="token punctuation">]</span>  <span class="token comment"># 截断</span>
    <span class="token keyword">return</span> line <span class="token operator">+</span> <span class="token punctuation">[</span>padding_token<span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token punctuation">(</span>num_steps <span class="token operator">-</span> <span class="token builtin">len</span><span class="token punctuation">(</span>line<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 填充</span>


<span class="token keyword">def</span> <span class="token function">build_array</span><span class="token punctuation">(</span>lines<span class="token punctuation">,</span> vocab<span class="token punctuation">,</span> num_steps<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""将机器翻译的文本序列转换成小批量"""</span>
    lines <span class="token operator">=</span> <span class="token punctuation">[</span>vocab<span class="token punctuation">[</span>l<span class="token punctuation">]</span> <span class="token keyword">for</span> l <span class="token keyword">in</span> lines<span class="token punctuation">]</span> <span class="token comment"># 每行的token转换为其id</span>
    lines <span class="token operator">=</span> <span class="token punctuation">[</span>l <span class="token operator">+</span> <span class="token punctuation">[</span>vocab<span class="token punctuation">[</span><span class="token string">'&lt;eos>'</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token keyword">for</span> l <span class="token keyword">in</span> lines<span class="token punctuation">]</span> <span class="token comment"># 每行的token后加上eos的id</span>
    array <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>truncate_pad<span class="token punctuation">(</span>
        l<span class="token punctuation">,</span> num_steps<span class="token punctuation">,</span> vocab<span class="token punctuation">[</span><span class="token string">'&lt;pad>'</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> l <span class="token keyword">in</span> lines<span class="token punctuation">]</span><span class="token punctuation">)</span>
    valid_len <span class="token operator">=</span> <span class="token punctuation">(</span>array <span class="token operator">!=</span> vocab<span class="token punctuation">[</span><span class="token string">'&lt;pad>'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>int32<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> array<span class="token punctuation">,</span> valid_len

<span class="token keyword">def</span> <span class="token function">load_array</span><span class="token punctuation">(</span>data_arrays<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> is_train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""构造一个PyTorch数据迭代器

    Defined in :numref:`sec_linear_concise`"""</span>
    dataset <span class="token operator">=</span> data<span class="token punctuation">.</span>TensorDataset<span class="token punctuation">(</span><span class="token operator">*</span>data_arrays<span class="token punctuation">)</span>
    <span class="token keyword">return</span> data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> shuffle<span class="token operator">=</span>is_train<span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">load_data</span><span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> num_steps<span class="token punctuation">,</span> num_examples<span class="token operator">=</span><span class="token number">600</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""返回翻译数据集的迭代器和词表"""</span>

    text <span class="token operator">=</span> read_data<span class="token punctuation">(</span><span class="token punctuation">)</span>

    source<span class="token punctuation">,</span> target<span class="token punctuation">,</span> src_line<span class="token punctuation">,</span> tgt_line <span class="token operator">=</span> tokenize<span class="token punctuation">(</span>text<span class="token punctuation">)</span>
    <span class="token comment"># 返回类似&#123;'l': 3, 'o': 2, 'h': 1, 'e': 1, ' ': 1, 'w': 1, 'r': 1, 'd': 1&#125;的一个字典</span>
    src_vocab <span class="token operator">=</span> Vocab<span class="token punctuation">(</span>source<span class="token punctuation">,</span> min_freq<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>
                          reserved_tokens<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'&lt;pad>'</span><span class="token punctuation">,</span> <span class="token string">'&lt;bos>'</span><span class="token punctuation">,</span> <span class="token string">'&lt;eos>'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    tgt_vocab <span class="token operator">=</span> Vocab<span class="token punctuation">(</span>target<span class="token punctuation">,</span> min_freq<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>
                          reserved_tokens<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'&lt;pad>'</span><span class="token punctuation">,</span> <span class="token string">'&lt;bos>'</span><span class="token punctuation">,</span> <span class="token string">'&lt;eos>'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token comment"># 首先把每行的词转换为了其对应的id，然后给每一行的末尾添加token &lt;eos>, 然后根据num_steps，如果line长度不足，补&lt;pad>，如果长度超出，截断</span>
    <span class="token comment"># 一种类型的输出是：</span>
    <span class="token comment"># [</span>
    <span class="token comment">#     [line0-char0-id, line0-char1-id, line0-char2-id, ...., eos-id],</span>
    <span class="token comment">#     [line1-char0-id, line1-char1-id, line1-char2-id, ...., eos-id], 注意，最后的末尾可能没有eos</span>
    <span class="token comment">#     .....</span>
    <span class="token comment"># ]</span>
    src_array<span class="token punctuation">,</span> src_valid_len <span class="token operator">=</span> build_array<span class="token punctuation">(</span>source<span class="token punctuation">,</span> src_vocab<span class="token punctuation">,</span> num_steps<span class="token punctuation">)</span>
    tgt_array<span class="token punctuation">,</span> tgt_valid_len <span class="token operator">=</span> build_array<span class="token punctuation">(</span>target<span class="token punctuation">,</span> tgt_vocab<span class="token punctuation">,</span> num_steps<span class="token punctuation">)</span>

    data_arrays <span class="token operator">=</span> <span class="token punctuation">(</span>src_array<span class="token punctuation">,</span> src_valid_len<span class="token punctuation">,</span> tgt_array<span class="token punctuation">,</span> tgt_valid_len<span class="token punctuation">)</span>
    data_iter <span class="token operator">=</span> load_array<span class="token punctuation">(</span>data_arrays<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>
    <span class="token keyword">return</span> data_iter<span class="token punctuation">,</span> src_vocab<span class="token punctuation">,</span> tgt_vocab<span class="token punctuation">,</span> src_line<span class="token punctuation">,</span> tgt_line<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>  上面代码做了如下事情：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>根据数据集的格式，读取每一行，只提前每行前面2个字符串。</p>
</li>
<li class="lvl-2">
<p>然后我们对每一行进行文字切割，得到了一个二维列表，列表中的每一行又被分割为一个个中文文字和一个个英文的词，也就得到了一个个token。(特别注意，站在当前的时刻，这里的token和现在主流的大语言模型的token概念是一样的，但是不是一样的实现。)</p>
</li>
<li class="lvl-2">
<p>由于模型不能直接处理文字，我们需要将文字转换为数字，那么直接的做法就是将一个个token编号即可，这个时候我们得到了词表（vocabulary）。</p>
</li>
<li class="lvl-2">
<p>然后我们根据我们得到的词表，对原始数据集进行数字化，得到一个列表，列表中每个元素就是一个个token对应的索引。</p>
</li>
<li class="lvl-2">
<p>最后得到：基于pytorch的DataLoader、原文词表、译文词表、原文文字列表、译文文字列表</p>
</li>
</ul>
<p>  此外，在这里出现了几个在后面的大语言模型中也会出现的词：BOS/EOS。这两个分别代表一次对话的起始、结尾，这里直接记住就行。</p>
<br/>
<br/>
<h5 id="搭建seq2seq训练框架">搭建seq2seq训练框架</h5>
<p>  首先引用一些包和一些辅助class</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> os
<span class="token keyword">import</span> random
<span class="token keyword">import</span> torch
<span class="token keyword">import</span> math
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> functional <span class="token keyword">as</span> F
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> time
<span class="token keyword">import</span> visdom
<span class="token keyword">import</span> collections

<span class="token keyword">import</span> dataset
<span class="token keyword">class</span> <span class="token class-name">Accumulator</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""在n个变量上累加"""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Defined in :numref:`sec_softmax_scratch`"""</span>
        self<span class="token punctuation">.</span>data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0.0</span><span class="token punctuation">]</span> <span class="token operator">*</span> n

    <span class="token keyword">def</span> <span class="token function">add</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>data <span class="token operator">=</span> <span class="token punctuation">[</span>a <span class="token operator">+</span> <span class="token builtin">float</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span> <span class="token keyword">for</span> a<span class="token punctuation">,</span> b <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>data<span class="token punctuation">,</span> args<span class="token punctuation">)</span><span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">reset</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0.0</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>data<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> idx<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>data<span class="token punctuation">[</span>idx<span class="token punctuation">]</span>
    
<span class="token keyword">class</span> <span class="token class-name">Timer</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""记录多次运行时间"""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Defined in :numref:`subsec_linear_model`"""</span>
        self<span class="token punctuation">.</span>times <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">start</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""启动计时器"""</span>
        self<span class="token punctuation">.</span>tik <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">stop</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""停止计时器并将时间记录在列表中"""</span>
        self<span class="token punctuation">.</span>times<span class="token punctuation">.</span>append<span class="token punctuation">(</span>time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> self<span class="token punctuation">.</span>tik<span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>times<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">avg</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""返回平均时间"""</span>
        <span class="token keyword">return</span> <span class="token builtin">sum</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>times<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>times<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">sum</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""返回时间总和"""</span>
        <span class="token keyword">return</span> <span class="token builtin">sum</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>times<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">cumsum</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""返回累计时间"""</span>
        <span class="token keyword">return</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>self<span class="token punctuation">.</span>times<span class="token punctuation">)</span><span class="token punctuation">.</span>cumsum<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>  然后我们根据编码器、解码器架构，设计seq2seq的网络主干</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Encoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""编码器-解码器架构的基本编码器接口"""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 调用父类nn.Module的构造函数，确保正确初始化</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Encoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 抛出未实现错误，意味着该方法需要在子类中具体实现</span>
        <span class="token keyword">raise</span> NotImplementedError

<span class="token keyword">class</span> <span class="token class-name">Decoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""编码器-解码器架构的基本解码器接口

    Defined in :numref:`sec_encoder-decoder`"""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 调用父类nn.Module的构造函数，确保正确初始化</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Decoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">init_state</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> enc_outputs<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 抛出未实现错误，意味着该方法需要在子类中具体实现</span>
        <span class="token keyword">raise</span> NotImplementedError

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> state<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 抛出未实现错误，意味着该方法需要在子类中具体实现</span>
        <span class="token keyword">raise</span> NotImplementedError

<span class="token keyword">class</span> <span class="token class-name">EncoderDecoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""编码器-解码器架构的基类

    Defined in :numref:`sec_encoder-decoder`"""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> encoder<span class="token punctuation">,</span> decoder<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 调用父类nn.Module的构造函数，确保正确初始化</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>EncoderDecoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
        <span class="token comment"># 将传入的编码器实例赋值给类的属性</span>
        self<span class="token punctuation">.</span>encoder <span class="token operator">=</span> encoder
        <span class="token comment"># 将传入的解码器实例赋值给类的属性</span>
        self<span class="token punctuation">.</span>decoder <span class="token operator">=</span> decoder

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> enc_X<span class="token punctuation">,</span> dec_X<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 调用编码器的前向传播方法，处理输入的编码器输入数据enc_X</span>
        enc_outputs <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>enc_X<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span>
        <span class="token comment"># 调用解码器的init_state方法，根据编码器的输出初始化解码器的状态</span>
        dec_state <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">.</span>init_state<span class="token punctuation">(</span>enc_outputs<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span>
        <span class="token comment"># 调用解码器的前向传播方法，处理输入的解码器输入数据dec_X和初始化后的状态</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>dec_X<span class="token punctuation">,</span> dec_state<span class="token punctuation">)</span>
    
<span class="token comment">#@save</span>
<span class="token keyword">class</span> <span class="token class-name">Seq2SeqEncoder</span><span class="token punctuation">(</span>Encoder<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""用于序列到序列学习的循环神经网络编码器"""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> embed_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> num_layers<span class="token punctuation">,</span>
                 dropout<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Seq2SeqEncoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
        <span class="token comment"># 嵌入层</span>
        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embed_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>rnn <span class="token operator">=</span> nn<span class="token punctuation">.</span>GRU<span class="token punctuation">(</span>embed_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> num_layers<span class="token punctuation">,</span>
                          dropout<span class="token operator">=</span>dropout<span class="token punctuation">)</span>
        <span class="token comment"># self.lstm = nn.LSTM(embed_size, num_hiddens, num_layers)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 输入X.shape = (batch_size,num_steps)</span>
        <span class="token comment"># 输出'X'的形状：(batch_size,num_steps,embed_size)</span>
        X <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
        <span class="token comment"># 在循环神经网络模型中，第一个轴对应于时间步</span>
        X <span class="token operator">=</span> X<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
        <span class="token comment"># 如果未提及状态，则默认为0</span>
        output<span class="token punctuation">,</span> state <span class="token operator">=</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
        <span class="token comment"># output : 这个返回值是所有时间步的隐藏状态序列</span>
        <span class="token comment"># output的形状:(num_steps,batch_size,num_hiddens)</span>
        <span class="token comment"># hn (hidden) : 这是每一层rnn的最后一个时间步的隐藏状态</span>
        <span class="token comment"># state的形状:(num_layers,batch_size,num_hiddens)</span>
        <span class="token keyword">return</span> output<span class="token punctuation">,</span> state

<span class="token keyword">class</span> <span class="token class-name">Seq2SeqDecoder</span><span class="token punctuation">(</span>Decoder<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""用于序列到序列学习的循环神经网络解码器"""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> embed_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> num_layers<span class="token punctuation">,</span>
                 dropout<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Seq2SeqDecoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embed_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>rnn <span class="token operator">=</span> nn<span class="token punctuation">.</span>GRU<span class="token punctuation">(</span>embed_size <span class="token operator">+</span> num_hiddens<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> num_layers<span class="token punctuation">,</span>
                          dropout<span class="token operator">=</span>dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dense <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> vocab_size<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">init_state</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> enc_outputs<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> enc_outputs<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> state<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 输出'X'的形状：(batch_size,num_steps,embed_size)</span>
        X <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
        <span class="token comment"># 广播context，使其具有与X相同的num_steps</span>
        context <span class="token operator">=</span> state<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        X_and_context <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> context<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
        output<span class="token punctuation">,</span> state <span class="token operator">=</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">(</span>X_and_context<span class="token punctuation">,</span> state<span class="token punctuation">)</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>dense<span class="token punctuation">(</span>output<span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
        <span class="token comment"># output的形状:(batch_size,num_steps,vocab_size)</span>
        <span class="token comment"># state的形状:(num_layers,batch_size,num_hiddens)</span>
        <span class="token keyword">return</span> output<span class="token punctuation">,</span> state<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>  我们结合上面的架构图对比着看，首先声明一下decoder/encoder的接口类：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>声明了Encoder(nn.Module)，Encoder(nn.Module)其输入是原文，输出是中间状态。</p>
</li>
<li class="lvl-2">
<p>声明了Decoder(nn.Module)，Decoder(nn.Module)的输入是BOS和中间状态，输出是译文。</p>
</li>
<li class="lvl-2">
<p>声明了EncoderDecoder(nn.Module)类，串联Encoder(nn.Module)/Decoder(nn.Module)进行运行。</p>
</li>
</ul>
<p>  然后声明实际的Seq2SeqEncoder部分：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>声明了Seq2SeqEncoder(Encoder)，其是seq2seq编码器部分的实际定义，其输入是一串原文，然后经过了nn.Embedding，将输入的token序列转换为token-embedding，然后送入nn.GRU，得到了两个值：最后一层rnn的所有时间步的隐藏状态output（shape=num_steps,batch_size,num_hiddens），所有层rnn的最后一个时间步的隐藏状态h_n（shape=num_layers,batch_size,num_hiddens）</p>
</li>
<li class="lvl-2">
<p>从Seq2SeqEncoder(Encoder)上面的分析可知：rnn的输出output代表的是每一个时间步，当前序列的总结信息，h_n encoder的隐藏态参数。</p>
</li>
</ul>
<p>  最后声明实际的Seq2SeqDecoder部分：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>声明了Seq2SeqDecoder(Decoder)，输入是：一个是bos，一个是Seq2SeqEncoder(Encoder)输出的隐藏态state(output,h_n)。首先将bos转换为embedding向量，然后将h_n的最后一个数据（也就是原文的总结，rnn最后一层最后一个时间步的隐藏态）和embedding组合在一起(注意：这里已经将原文的语义已经和bos输入混合在一起了)，和Seq2SeqEncoder(Encoder) state作为的隐藏状态初始值，一起传入rnn，然后经过nn.Linear的映射，得到了decoder的输出。</p>
</li>
<li class="lvl-2">
<p>从Seq2SeqDecoder(Decoder)的分析可知，经过了nn.Linear映射之后，我们将decoder层的rnn的output转换为词表大小的一个向量，这个向量我们可以看做下一个字的分数Logits（注意：这个概念在后续大语言模型中，有比较大的作用）。</p>
</li>
</ul>
<p>  这里，nn.RNN等pytorch层的输出，可以结合下面这个图来理解（图来自于参考文献相关链接）：</p>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/blog_idx_142/stacked_rnn.png" alt="rep_img"/></center>
    </div>
</div>   
<p>   下面给出的就是训练、预测部分的代码：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">try_gpu</span><span class="token punctuation">(</span>i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""如果存在，则返回gpu(i)，否则返回cpu()

    Defined in :numref:`sec_use_gpu`"""</span>
    <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>device_count<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">>=</span> i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'cuda:</span><span class="token interpolation"><span class="token punctuation">&#123;</span>i<span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cpu'</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">sequence_mask</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> valid_len<span class="token punctuation">,</span> value<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""在序列中屏蔽不相关的项"""</span>
    maxlen <span class="token operator">=</span> X<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
    mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token punctuation">(</span>maxlen<span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">,</span>
                        device<span class="token operator">=</span>X<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">&lt;</span> valid_len<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span>
    X<span class="token punctuation">[</span><span class="token operator">~</span>mask<span class="token punctuation">]</span> <span class="token operator">=</span> value
    <span class="token keyword">return</span> X

<span class="token keyword">class</span> <span class="token class-name">MaskedSoftmaxCELoss</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""带遮蔽的softmax交叉熵损失函数"""</span>
    <span class="token comment"># pred的形状：(batch_size,num_steps,vocab_size)</span>
    <span class="token comment"># label的形状：(batch_size,num_steps)</span>
    <span class="token comment"># valid_len的形状：(batch_size,)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> pred<span class="token punctuation">,</span> label<span class="token punctuation">,</span> valid_len<span class="token punctuation">)</span><span class="token punctuation">:</span>
        weights <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span>label<span class="token punctuation">)</span>
        weights <span class="token operator">=</span> sequence_mask<span class="token punctuation">(</span>weights<span class="token punctuation">,</span> valid_len<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>reduction<span class="token operator">=</span><span class="token string">'none'</span>
        unweighted_loss <span class="token operator">=</span> <span class="token builtin">super</span><span class="token punctuation">(</span>MaskedSoftmaxCELoss<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>forward<span class="token punctuation">(</span>
            pred<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> label<span class="token punctuation">)</span>
        weighted_loss <span class="token operator">=</span> <span class="token punctuation">(</span>unweighted_loss <span class="token operator">*</span> weights<span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> weighted_loss
    
<span class="token keyword">def</span> <span class="token function">grad_clipping</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> theta<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""裁剪梯度"""</span>
    <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
        params <span class="token operator">=</span> <span class="token punctuation">[</span>p <span class="token keyword">for</span> p <span class="token keyword">in</span> net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> p<span class="token punctuation">.</span>requires_grad<span class="token punctuation">]</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        params <span class="token operator">=</span> net<span class="token punctuation">.</span>params
    norm <span class="token operator">=</span> torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token builtin">sum</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">(</span>p<span class="token punctuation">.</span>grad <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">for</span> p <span class="token keyword">in</span> params<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> norm <span class="token operator">></span> theta<span class="token punctuation">:</span>
        <span class="token keyword">for</span> param <span class="token keyword">in</span> params<span class="token punctuation">:</span>
            param<span class="token punctuation">.</span>grad<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">*=</span> theta <span class="token operator">/</span> norm

<span class="token keyword">def</span> <span class="token function">train_seq2seq</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> data_iter<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> tgt_vocab<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""训练序列到序列模型"""</span>
    <span class="token keyword">def</span> <span class="token function">xavier_init_weights</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token builtin">type</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span> <span class="token operator">==</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">:</span>
            nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>xavier_uniform_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>
        <span class="token keyword">if</span> <span class="token builtin">type</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span> <span class="token operator">==</span> nn<span class="token punctuation">.</span>GRU<span class="token punctuation">:</span>
            <span class="token keyword">for</span> param <span class="token keyword">in</span> m<span class="token punctuation">.</span>_flat_weights_names<span class="token punctuation">:</span>
                <span class="token keyword">if</span> <span class="token string">"weight"</span> <span class="token keyword">in</span> param<span class="token punctuation">:</span>
                    nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>xavier_uniform_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>_parameters<span class="token punctuation">[</span>param<span class="token punctuation">]</span><span class="token punctuation">)</span>

    net<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>xavier_init_weights<span class="token punctuation">)</span>
    net<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
    optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">)</span>
    loss <span class="token operator">=</span> MaskedSoftmaxCELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
    net<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
    vis <span class="token operator">=</span> visdom<span class="token punctuation">.</span>Visdom<span class="token punctuation">(</span>env<span class="token operator">=</span><span class="token string">u'test1'</span><span class="token punctuation">,</span> server<span class="token operator">=</span><span class="token string">"http://127.0.0.1"</span><span class="token punctuation">,</span> port<span class="token operator">=</span><span class="token number">8097</span><span class="token punctuation">)</span>
    animator <span class="token operator">=</span> vis
    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        timer <span class="token operator">=</span> Timer<span class="token punctuation">(</span><span class="token punctuation">)</span>
        metric <span class="token operator">=</span> Accumulator<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># 训练损失总和，词元数量</span>
        <span class="token keyword">for</span> batch <span class="token keyword">in</span> data_iter<span class="token punctuation">:</span>
            <span class="token comment">#清零（reset）优化器中的梯度缓存</span>
            optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token comment"># x.shape = [batch_size, num_steps]</span>
            X<span class="token punctuation">,</span> X_valid_len<span class="token punctuation">,</span> Y<span class="token punctuation">,</span> Y_valid_len <span class="token operator">=</span> <span class="token punctuation">[</span>x<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> batch<span class="token punctuation">]</span>
            <span class="token comment"># bos.shape = batch_size 个 bos-id</span>
            bos <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>tgt_vocab<span class="token punctuation">[</span><span class="token string">'&lt;bos>'</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">*</span> Y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                          device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token comment"># dec_input.shape = (batch_size, num_steps)</span>
            <span class="token comment"># 解码器的输入通常由序列的起始标志 bos 和目标序列（去掉末尾的部分 Y[:, :-1]）组成。</span>
            dec_input <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>bos<span class="token punctuation">,</span> Y<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 强制教学</span>
            <span class="token comment"># Y_hat的形状:(batch_size,num_steps,vocab_size)</span>
            Y_hat<span class="token punctuation">,</span> _ <span class="token operator">=</span> net<span class="token punctuation">(</span>X<span class="token punctuation">,</span> dec_input<span class="token punctuation">,</span> X_valid_len<span class="token punctuation">)</span>
            l <span class="token operator">=</span> loss<span class="token punctuation">(</span>Y_hat<span class="token punctuation">,</span> Y<span class="token punctuation">,</span> Y_valid_len<span class="token punctuation">)</span>
            l<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>      <span class="token comment"># 损失函数的标量进行“反向传播”</span>
            grad_clipping<span class="token punctuation">(</span>net<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
            num_tokens <span class="token operator">=</span> Y_valid_len<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
            optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                metric<span class="token punctuation">.</span>add<span class="token punctuation">(</span>l<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> num_tokens<span class="token punctuation">)</span>

        <span class="token keyword">if</span> <span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">10</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            <span class="token comment"># print(predict('你是？'))</span>
            <span class="token comment"># print(epoch)</span>
            <span class="token comment"># animator.add(epoch + 1, )</span>

            <span class="token keyword">if</span> epoch <span class="token operator">==</span> <span class="token number">9</span><span class="token punctuation">:</span>
                <span class="token comment"># 清空图表：使用空数组来替换现有内容</span>
                vis<span class="token punctuation">.</span>line<span class="token punctuation">(</span>X<span class="token operator">=</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> Y<span class="token operator">=</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> win<span class="token operator">=</span><span class="token string">'train_ch8'</span><span class="token punctuation">,</span> update<span class="token operator">=</span><span class="token string">'replace'</span><span class="token punctuation">)</span>
            <span class="token comment"># _loss_val = l</span>
            <span class="token comment"># _loss_val = _loss_val.cpu().sum().detach().numpy()</span>
            vis<span class="token punctuation">.</span>line<span class="token punctuation">(</span>
                X<span class="token operator">=</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                Y<span class="token operator">=</span><span class="token punctuation">[</span> metric<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">/</span> metric<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                win<span class="token operator">=</span><span class="token string">'train_ch8'</span><span class="token punctuation">,</span>
                update<span class="token operator">=</span><span class="token string">'append'</span><span class="token punctuation">,</span>
                opts<span class="token operator">=</span><span class="token punctuation">&#123;</span>
                    <span class="token string">'title'</span><span class="token punctuation">:</span> <span class="token string">'train_ch8'</span><span class="token punctuation">,</span>
                    <span class="token string">'xlabel'</span><span class="token punctuation">:</span> <span class="token string">'epoch'</span><span class="token punctuation">,</span>
                    <span class="token string">'ylabel'</span><span class="token punctuation">:</span> <span class="token string">'loss'</span><span class="token punctuation">,</span>
                    <span class="token string">'linecolor'</span><span class="token punctuation">:</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">255</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment"># 蓝色线条</span>
                <span class="token punctuation">&#125;</span>
            <span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'loss </span><span class="token interpolation"><span class="token punctuation">&#123;</span>metric<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">/</span> metric<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">&#125;</span></span><span class="token string">, </span><span class="token interpolation"><span class="token punctuation">&#123;</span>metric<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">/</span> timer<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.1f</span><span class="token punctuation">&#125;</span></span><span class="token string"> '</span></span>
        <span class="token string-interpolation"><span class="token string">f'tokens/sec on </span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">str</span><span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
    torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>net<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'model_h.pt'</span><span class="token punctuation">)</span>  <span class="token comment"># [[6]]</span>
    torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>net<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'model.pt'</span><span class="token punctuation">)</span>  <span class="token comment"># [[6]]</span>

<span class="token keyword">def</span> <span class="token function">predict_seq2seq</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> src_sentence<span class="token punctuation">,</span> src_vocab<span class="token punctuation">,</span> tgt_vocab<span class="token punctuation">,</span> num_steps<span class="token punctuation">,</span>
                    device<span class="token punctuation">,</span> save_attention_weights<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""序列到序列模型的预测"""</span>
    <span class="token comment"># 在预测时将net设置为评估模式</span>
    net<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    src_tokens <span class="token operator">=</span> src_vocab<span class="token punctuation">[</span>src_sentence<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">[</span>
        src_vocab<span class="token punctuation">[</span><span class="token string">'&lt;eos>'</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
    enc_valid_len <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>src_tokens<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>
    src_tokens <span class="token operator">=</span> dataset<span class="token punctuation">.</span>truncate_pad<span class="token punctuation">(</span>src_tokens<span class="token punctuation">,</span> num_steps<span class="token punctuation">,</span> src_vocab<span class="token punctuation">[</span><span class="token string">'&lt;pad>'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token comment"># 添加批量轴</span>
    enc_X <span class="token operator">=</span> torch<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>
        torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>src_tokens<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
    enc_outputs <span class="token operator">=</span> net<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>enc_X<span class="token punctuation">,</span> enc_valid_len<span class="token punctuation">)</span>
    dec_state <span class="token operator">=</span> net<span class="token punctuation">.</span>decoder<span class="token punctuation">.</span>init_state<span class="token punctuation">(</span>enc_outputs<span class="token punctuation">,</span> enc_valid_len<span class="token punctuation">)</span>
    <span class="token comment"># 添加批量轴</span>
    dec_X <span class="token operator">=</span> torch<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>
        <span class="token punctuation">[</span>tgt_vocab<span class="token punctuation">[</span><span class="token string">'&lt;bos>'</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
    output_seq<span class="token punctuation">,</span> attention_weight_seq <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_steps<span class="token punctuation">)</span><span class="token punctuation">:</span>
        Y<span class="token punctuation">,</span> dec_state <span class="token operator">=</span> net<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>dec_X<span class="token punctuation">,</span> dec_state<span class="token punctuation">)</span>
        <span class="token comment"># 我们使用具有预测最高可能性的词元，作为解码器在下一时间步的输入</span>
        dec_X <span class="token operator">=</span> Y<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        pred <span class="token operator">=</span> dec_X<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>int32<span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 保存注意力权重（稍后讨论）</span>
        <span class="token keyword">if</span> save_attention_weights<span class="token punctuation">:</span>
            attention_weight_seq<span class="token punctuation">.</span>append<span class="token punctuation">(</span>net<span class="token punctuation">.</span>decoder<span class="token punctuation">.</span>attention_weights<span class="token punctuation">)</span>
        <span class="token comment"># 一旦序列结束词元被预测，输出序列的生成就完成了</span>
        <span class="token keyword">if</span> pred <span class="token operator">==</span> tgt_vocab<span class="token punctuation">[</span><span class="token string">'&lt;eos>'</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
            <span class="token keyword">break</span>
        output_seq<span class="token punctuation">.</span>append<span class="token punctuation">(</span>pred<span class="token punctuation">)</span>
    <span class="token keyword">return</span> <span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>tgt_vocab<span class="token punctuation">.</span>to_tokens<span class="token punctuation">(</span>output_seq<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> attention_weight_seq


<span class="token keyword">def</span> <span class="token function">bleu</span><span class="token punctuation">(</span>pred_seq<span class="token punctuation">,</span> label_seq<span class="token punctuation">,</span> k<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""计算BLEU"""</span>
    pred_tokens<span class="token punctuation">,</span> label_tokens <span class="token operator">=</span> pred_seq<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>i <span class="token keyword">for</span> i <span class="token keyword">in</span> label_seq<span class="token punctuation">]</span>
    len_pred<span class="token punctuation">,</span> len_label <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>pred_tokens<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>label_tokens<span class="token punctuation">)</span>
    score <span class="token operator">=</span> math<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token builtin">min</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span> <span class="token operator">-</span> len_label <span class="token operator">/</span> len_pred<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> n <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> k <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        num_matches<span class="token punctuation">,</span> label_subs <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> collections<span class="token punctuation">.</span>defaultdict<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>len_label <span class="token operator">-</span> n <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            label_subs<span class="token punctuation">[</span><span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>label_tokens<span class="token punctuation">[</span>i<span class="token punctuation">:</span> i <span class="token operator">+</span> n<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">1</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>len_pred <span class="token operator">-</span> n <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> label_subs<span class="token punctuation">[</span><span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>pred_tokens<span class="token punctuation">[</span>i<span class="token punctuation">:</span> i <span class="token operator">+</span> n<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>
                num_matches <span class="token operator">+=</span> <span class="token number">1</span>
                label_subs<span class="token punctuation">[</span><span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>pred_tokens<span class="token punctuation">[</span>i<span class="token punctuation">:</span> i <span class="token operator">+</span> n<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">-=</span> <span class="token number">1</span>
        score <span class="token operator">*=</span> math<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>num_matches <span class="token operator">/</span> <span class="token punctuation">(</span>len_pred <span class="token operator">-</span> n <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> math<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> score<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>  这里首先要介绍一下其损失函数，核心两个：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>通过交叉熵计算真实分布、预测分布的差异性，差异性越小，意味着我们的模型越好</p>
</li>
<li class="lvl-2">
<p>由于我们是序列模型，可能涉及pad项，这些pad项的位置是无意义的，但是对模型有影响，我们需要再loss中剔除掉这种无意义的位置，我们用mask来屏蔽。</p>
</li>
</ul>
<p>  然后训练过程的核心就是：从数据集中获取 训练数据、验证数据，通过训练数据得到预测数据，预测数据和验证数据进行loss计算，然后进行反向传播，找到loss最小化的方向，然后最小化loss，模型就会越来越好。</p>
<p>  然后就是介绍预测部分的内容：先将原文输入到seq的encoder，然后将bos序列 + seq的encoder的隐藏态传给seq的decoder，就可以得到下一个字的输出，直到我们遇到eos，预测结束。</p>
<p>  我们虽然预测完毕了，得到了原文对应的译文，但是我们需要一种方法来评估我们翻译的是不是正确，这里用的方法是bleu，它的作用就是评估输出序列与目标序列的精确度。</p>
<p>  最后，我们开始训练过程，注意，下面的例子是先进行训练，然后保存pt模型，然后加载模型进行预测推理。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">
<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    embed_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> num_layers<span class="token punctuation">,</span> dropout <span class="token operator">=</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0.1</span>
    batch_size<span class="token punctuation">,</span> num_steps <span class="token operator">=</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">10</span>
    lr<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> device <span class="token operator">=</span> <span class="token number">0.005</span><span class="token punctuation">,</span> <span class="token number">2000</span><span class="token punctuation">,</span> try_gpu<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># train_iter 每个迭代输出：(batch_size, num_steps)</span>
    train_iter<span class="token punctuation">,</span> src_vocab<span class="token punctuation">,</span> tgt_vocab<span class="token punctuation">,</span> source<span class="token punctuation">,</span> target <span class="token operator">=</span> dataset<span class="token punctuation">.</span>load_data<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> num_steps<span class="token punctuation">)</span>
    encoder <span class="token operator">=</span> Seq2SeqEncoder<span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>src_vocab<span class="token punctuation">)</span><span class="token punctuation">,</span> embed_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> num_layers<span class="token punctuation">,</span>
                        dropout<span class="token punctuation">)</span>
    decoder <span class="token operator">=</span> Seq2SeqDecoder<span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>tgt_vocab<span class="token punctuation">)</span><span class="token punctuation">,</span> embed_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> num_layers<span class="token punctuation">,</span>
                            dropout<span class="token punctuation">)</span>
    net <span class="token operator">=</span> EncoderDecoder<span class="token punctuation">(</span>encoder<span class="token punctuation">,</span> decoder<span class="token punctuation">)</span>

    is_train <span class="token operator">=</span> <span class="token boolean">False</span>
    <span class="token keyword">if</span> is_train<span class="token punctuation">:</span>
        train_seq2seq<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> tgt_vocab<span class="token punctuation">,</span> device<span class="token punctuation">)</span>

    <span class="token keyword">else</span><span class="token punctuation">:</span>
        state_dict <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'model_h.pt'</span><span class="token punctuation">)</span>
        net<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>state_dict<span class="token punctuation">)</span>
        net<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
        C <span class="token operator">=</span> <span class="token number">0</span>
        C1 <span class="token operator">=</span> <span class="token number">0</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">2000</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># print(source[i])</span>
            <span class="token comment"># print(target[i])</span>
            translation<span class="token punctuation">,</span> attention_weight_seq <span class="token operator">=</span> predict_seq2seq<span class="token punctuation">(</span>
                net<span class="token punctuation">,</span> source<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> src_vocab<span class="token punctuation">,</span> tgt_vocab<span class="token punctuation">,</span> num_steps<span class="token punctuation">,</span> device<span class="token punctuation">)</span>
            
            score <span class="token operator">=</span> bleu<span class="token punctuation">(</span>translation<span class="token punctuation">,</span> target<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> k<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
            <span class="token keyword">if</span> score <span class="token operator">></span> <span class="token number">0.0</span><span class="token punctuation">:</span>
                C <span class="token operator">=</span> C <span class="token operator">+</span> <span class="token number">1</span>
                <span class="token keyword">if</span> score <span class="token operator">></span> <span class="token number">0.8</span><span class="token punctuation">:</span>
                    C1 <span class="token operator">=</span> C1 <span class="token operator">+</span> <span class="token number">1</span>
                <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'</span><span class="token interpolation"><span class="token punctuation">&#123;</span>source<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">&#125;</span></span><span class="token string"> => </span><span class="token interpolation"><span class="token punctuation">&#123;</span>translation<span class="token punctuation">&#125;</span></span><span class="token string">, bleu </span><span class="token interpolation"><span class="token punctuation">&#123;</span>score<span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>

        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Counter(bleu > 0) = </span><span class="token interpolation"><span class="token punctuation">&#123;</span>C<span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Valid-Counter(bleu > 0.8) = </span><span class="token interpolation"><span class="token punctuation">&#123;</span>C1<span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/blog_idx_142/train.png" alt="rep_img"/></center>
    </div>
</div>   
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/blog_idx_142/ret.png" alt="rep_img"/></center>
    </div>
</div>   
<p>  从上面的图可以看到，这个模型有一定的翻译效果。</p>
<p>  此外，我这里计算了非零的bleu以及大于0.8的bleu的个数，这个个数勉强可以评估，我们对现在这个seq2seq模型优化的效果，为后面的文章提前做一些准备工作。</p>
<br/>
<br/>
<br/>
<br/>
<h3 id="后记">后记</h3>
<hr>
<p>  本文出现了bos/eos/logits等一些概念的应用，这些应用在大语言模型中也有体现。</p>
<p>  此外，我们从当前的模型结构也可以知道，当前并没有解决输入序列过长时，序列前面部分信息可能丢失，序列中的重点信息没有动态突出的问题。</p>
<h3 id="参考文献">参考文献</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://zh.d2l.ai/chapter_recurrent-modern/encoder-decoder.html">https://zh.d2l.ai/chapter_recurrent-modern/encoder-decoder.html</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://zh.d2l.ai/chapter_recurrent-modern/seq2seq.html">https://zh.d2l.ai/chapter_recurrent-modern/seq2seq.html</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://www.geeksforgeeks.org/nlp/stacked-rnns-in-nlp/">https://www.geeksforgeeks.org/nlp/stacked-rnns-in-nlp/</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.RNN.html">https://docs.pytorch.org/docs/stable/generated/torch.nn.RNN.html</a></p>
</li>
</ul>
<br/>
<br/>
<div style="margin:50px auto;">
</div>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <hr/>
        <center><font color = #91e0b0 size = 5>打赏、订阅、收藏、丢香蕉、硬币，请关注公众号（攻城狮的搬砖之路）</font></center>
    </div>
</div>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/qrcode_for_wx_official_account.jpg" alt="qrc_img"/></center>
    </div>
</div>
<!-- ![alt 公众号图片](https://flyinskyin2013.github.io/ImageBed0/blogs/qrcode_for_wx_official_account.jpg "公众号图片") -->
<p><font color="red" size="7">PS: 请尊重原创，不喜勿喷。</font><br/><br>
<font color="red" size="7">PS: 要转载请注明出处，本人版权所有。</font><br/><br>
<font color="red" size="7">PS: 有问题请留言，看到后我会第一时间回复。</font><br/></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://e-x.top/2025/09/14/blog_idx_141/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Sky">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sky's Blogs">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | Sky's Blogs">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/09/14/blog_idx_141/" class="post-title-link" itemprop="url">大模型基础补全计划(四)---LSTM的实例与测试(RNN的改进)</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-09-14 10:55:00 / 修改时间：10:59:03" itemprop="dateCreated datePublished" datetime="2025-09-14T10:55:00+08:00">2025-09-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/LLM/" itemprop="url" rel="index"><span itemprop="name">LLM</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/LLM/LM/" itemprop="url" rel="index"><span itemprop="name">LM</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.5k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>13 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <script src="\assets\js\APlayer.min.js"> </script><!--
 * @Description: 
 * @Author: Sky
 * @Date: 2020-08-24 16:37:34
 * @LastEditors: Sky
 * @LastEditTime: 2021-06-29 15:04:48
 * @Github: https://github.com/flyinskyin2013/
-->
<p><font color="red" size="7">PS：要转载请注明出处，本人版权所有。</font><br/><br>
<font color="red" size="7">PS: 这个只是基于《我自己》的理解，</font><br/><font color="red" size="7">如果和你的原则及想法相冲突，请谅解，勿喷。</font><br/></p>
<!-- ###### 前置说明
&emsp;&emsp;本文作为本人github blog/cnblog的主站的备份。（BlogID=141） 
&emsp;&emsp;本文发布于 2025-09-14 10:55:00             （BlogID=141） 
-->
<h6 id="环境说明">环境说明</h6>
<p>  无</p>
<h3 id="前言">前言</h3>
<hr>
<p>   本文是这个系列第四篇，它们是：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>《大模型基础补全计划(一)—重温一些深度学习相关的数学知识》 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/Iflyinsky/p/18717317">https://www.cnblogs.com/Iflyinsky/p/18717317</a></p>
</li>
<li class="lvl-2">
<p>《大模型基础补全计划(二)—词嵌入(word embedding) 》 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/Iflyinsky/p/18775451">https://www.cnblogs.com/Iflyinsky/p/18775451</a></p>
</li>
<li class="lvl-2">
<p>《大模型基础补全计划(三)—RNN实例与测试》 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/Iflyinsky/p/18967569">https://www.cnblogs.com/Iflyinsky/p/18967569</a></p>
</li>
</ul>
<p>   上文我们提到了RNN这种处理序列信息的网络结构，今天我们将会提到RNN的改进版本之一的网络结构：LSTM。注意在transformer结构出来之前，RNN还有很多的改进结构,毕竟这是一个大的研究方向。</p>
<br/>
<br/>
<br/>
<br/>
<h3 id="LSTM-long-short-term-memory-简介">LSTM (long short-term memory) 简介</h3>
<hr>
<p>  </p>
<br/>
<br/>
<h5 id="LSTM的意义">LSTM的意义</h5>
<p>  我们首先来想一想RNN的结构，很朴素的理解：RNN有两个输入，一个是当前输入，一个是上一次隐藏参数输入。如果我们从时间线来看，对于早期的输入$X_{t-n}$来说，由于隐藏参数一层层迭代和传递，对于$X_t$的影响非常的弱。此外，相对的，对于$X_{t-1}$来说，其对$X_t$的影响非常的强，如果$X_{t-1}$信息不完整，可能会影响输出。</p>
<p>  为了解决上面RNN结构遇到的问题，提出了LSTM结构。</p>
<br/>
<br/>
<h5 id="LSTM的结构介绍">LSTM的结构介绍</h5>
<p>  首先我们来看看其结构图如下：</p>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/blog_idx_141/lstm.png" alt="rep_img"/></center>
    </div>
</div>    
<p>注:此图来自于 <a target="_blank" rel="noopener" href="https://zh.d2l.ai/chapter_recurrent-modern/lstm.html">https://zh.d2l.ai/chapter_recurrent-modern/lstm.html</a> ，若侵权，联系删之。</p>
<p>  其有如下的一些内容：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>有三个输入：输入$X_t$，隐藏参数$H_{t-1}$，记忆$C_{t-1}$。</p>
</li>
<li class="lvl-2">
<p>有三个门：输入门 $I_t = \sigma(X_tW_{xi} + H_{t-1}W_{hi} + b_i)$ ， 遗忘门 $F_t = \sigma(X_tW_{xf} + H_{t-1}W_{hf} + b_f)$，输出门 $O_t = \sigma(X_tW_{xo} + H_{t-1}W_{ho} + b_o)$</p>
</li>
<li class="lvl-2">
<p>有一个候选记忆元 $\widetilde{C_t} = tanh(X_tW_{xc} + H_{t-1}W_{hc} + b_c)$。</p>
</li>
<li class="lvl-2">
<p>有一个记忆元$C_t$，其含义很简单，有多少记忆来自于$\widetilde{C_t}$ ，然后由输入门 $I_t$控制多少候选记忆元进入新记忆中，由 遗忘门 遗忘门 $F_t$ 来控制多少以前的记忆$C_{t-1}$进入新的记忆中。其公式为：$C_t = F_t \odot C_{t-1} + I_t \odot \widetilde{C_t}$</p>
</li>
<li class="lvl-2">
<p>有三个输出：输出门 $O_t$，记忆元 $C_t$，隐藏态$H_t = O_t \odot tanh(C_t)$</p>
</li>
</ul>
<p>  总的来说，就是给隐藏参数加入了记忆参数，并可以通过记忆影响隐藏参数。</p>
<br/>
<br/>
<br/>
<br/>
<h3 id="基于LSTM训练一个简单的文字序列输出模型">基于LSTM训练一个简单的文字序列输出模型</h3>
<hr>
<p>  对于文本预处理、数据集构造、训练框架搭建详见前文《大模型基础补全计划(三)—RNN实例与测试》</p>
<p>  下面是构建LSTM的网络结构，首先我们手动来构建网络：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">
<span class="token keyword">def</span> <span class="token function">get_lstm_params</span><span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span class="token punctuation">:</span>
    num_inputs <span class="token operator">=</span> num_outputs <span class="token operator">=</span> vocab_size

    <span class="token keyword">def</span> <span class="token function">normal</span><span class="token punctuation">(</span>shape<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>size<span class="token operator">=</span>shape<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token operator">*</span><span class="token number">0.01</span>

    <span class="token keyword">def</span> <span class="token function">three</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token punctuation">(</span>normal<span class="token punctuation">(</span><span class="token punctuation">(</span>num_inputs<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                normal<span class="token punctuation">(</span><span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">)</span>

    W_xi<span class="token punctuation">,</span> W_hi<span class="token punctuation">,</span> b_i <span class="token operator">=</span> three<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 输入门参数</span>
    W_xf<span class="token punctuation">,</span> W_hf<span class="token punctuation">,</span> b_f <span class="token operator">=</span> three<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 遗忘门参数</span>
    W_xo<span class="token punctuation">,</span> W_ho<span class="token punctuation">,</span> b_o <span class="token operator">=</span> three<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 输出门参数</span>
    W_xc<span class="token punctuation">,</span> W_hc<span class="token punctuation">,</span> b_c <span class="token operator">=</span> three<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 候选记忆元参数</span>

    <span class="token comment"># 输出层参数</span>
    W_hq <span class="token operator">=</span> normal<span class="token punctuation">(</span><span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> num_outputs<span class="token punctuation">)</span><span class="token punctuation">)</span>
    b_q <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>num_outputs<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>

    <span class="token comment"># 附加梯度</span>
    params <span class="token operator">=</span> <span class="token punctuation">[</span>W_xi<span class="token punctuation">,</span> W_hi<span class="token punctuation">,</span> b_i<span class="token punctuation">,</span> W_xf<span class="token punctuation">,</span> W_hf<span class="token punctuation">,</span> b_f<span class="token punctuation">,</span> W_xo<span class="token punctuation">,</span> W_ho<span class="token punctuation">,</span> b_o<span class="token punctuation">,</span> W_xc<span class="token punctuation">,</span> W_hc<span class="token punctuation">,</span>
              b_c<span class="token punctuation">,</span> W_hq<span class="token punctuation">,</span> b_q<span class="token punctuation">]</span>
    <span class="token keyword">for</span> param <span class="token keyword">in</span> params<span class="token punctuation">:</span>
        param<span class="token punctuation">.</span>requires_grad_<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> params

<span class="token keyword">def</span> <span class="token function">init_lstm_state</span><span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span>
            torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">lstm</span><span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> state<span class="token punctuation">,</span> params<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token punctuation">[</span>W_xi<span class="token punctuation">,</span> W_hi<span class="token punctuation">,</span> b_i<span class="token punctuation">,</span> W_xf<span class="token punctuation">,</span> W_hf<span class="token punctuation">,</span> b_f<span class="token punctuation">,</span> W_xo<span class="token punctuation">,</span> W_ho<span class="token punctuation">,</span> b_o<span class="token punctuation">,</span> W_xc<span class="token punctuation">,</span> W_hc<span class="token punctuation">,</span> b_c<span class="token punctuation">,</span>
     W_hq<span class="token punctuation">,</span> b_q<span class="token punctuation">]</span> <span class="token operator">=</span> params
    <span class="token punctuation">(</span>H<span class="token punctuation">,</span> C<span class="token punctuation">)</span> <span class="token operator">=</span> state
    outputs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> X <span class="token keyword">in</span> inputs<span class="token punctuation">:</span>
        I <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">(</span>X @ W_xi<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span>H @ W_hi<span class="token punctuation">)</span> <span class="token operator">+</span> b_i<span class="token punctuation">)</span>
        F <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">(</span>X @ W_xf<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span>H @ W_hf<span class="token punctuation">)</span> <span class="token operator">+</span> b_f<span class="token punctuation">)</span>
        O <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">(</span>X @ W_xo<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span>H @ W_ho<span class="token punctuation">)</span> <span class="token operator">+</span> b_o<span class="token punctuation">)</span>
        C_tilda <span class="token operator">=</span> torch<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span><span class="token punctuation">(</span>X @ W_xc<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span>H @ W_hc<span class="token punctuation">)</span> <span class="token operator">+</span> b_c<span class="token punctuation">)</span>
        C <span class="token operator">=</span> F <span class="token operator">*</span> C <span class="token operator">+</span> I <span class="token operator">*</span> C_tilda
        H <span class="token operator">=</span> O <span class="token operator">*</span> torch<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>C<span class="token punctuation">)</span>
        Y <span class="token operator">=</span> <span class="token punctuation">(</span>H @ W_hq<span class="token punctuation">)</span> <span class="token operator">+</span> b_q
        outputs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>Y<span class="token punctuation">)</span>
    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>H<span class="token punctuation">,</span> C<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>  然后是通过torch框架来设计网络：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">
lstm_layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span>num_inputs<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>  最后是完整的训练代码：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> os
<span class="token keyword">import</span> random
<span class="token keyword">import</span> torch
<span class="token keyword">import</span> math
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> functional <span class="token keyword">as</span> F
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> time
<span class="token keyword">import</span> visdom
<span class="token keyword">import</span> sys

sys<span class="token punctuation">.</span>path<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token string">'.'</span><span class="token punctuation">)</span>
<span class="token keyword">import</span> dateset
<span class="token keyword">class</span> <span class="token class-name">Accumulator</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""在n个变量上累加"""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Defined in :numref:`sec_softmax_scratch`"""</span>
        self<span class="token punctuation">.</span>data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0.0</span><span class="token punctuation">]</span> <span class="token operator">*</span> n

    <span class="token keyword">def</span> <span class="token function">add</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>data <span class="token operator">=</span> <span class="token punctuation">[</span>a <span class="token operator">+</span> <span class="token builtin">float</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span> <span class="token keyword">for</span> a<span class="token punctuation">,</span> b <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>data<span class="token punctuation">,</span> args<span class="token punctuation">)</span><span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">reset</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0.0</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>data<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> idx<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>data<span class="token punctuation">[</span>idx<span class="token punctuation">]</span>
    
<span class="token keyword">class</span> <span class="token class-name">Timer</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""记录多次运行时间"""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Defined in :numref:`subsec_linear_model`"""</span>
        self<span class="token punctuation">.</span>times <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">start</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""启动计时器"""</span>
        self<span class="token punctuation">.</span>tik <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">stop</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""停止计时器并将时间记录在列表中"""</span>
        self<span class="token punctuation">.</span>times<span class="token punctuation">.</span>append<span class="token punctuation">(</span>time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> self<span class="token punctuation">.</span>tik<span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>times<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">avg</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""返回平均时间"""</span>
        <span class="token keyword">return</span> <span class="token builtin">sum</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>times<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>times<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">sum</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""返回时间总和"""</span>
        <span class="token keyword">return</span> <span class="token builtin">sum</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>times<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">cumsum</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""返回累计时间"""</span>
        <span class="token keyword">return</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>self<span class="token punctuation">.</span>times<span class="token punctuation">)</span><span class="token punctuation">.</span>cumsum<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
<span class="token comment"># 以num_steps为步长，从随机的起始位置开始，返回</span>
<span class="token comment"># x1=[ [random_offset1:random_offset1 + num_steps], ... , [random_offset_batchsize:random_offset_batchsize + num_steps] ]</span>
<span class="token comment"># y1=[ [random_offset1 + 1:random_offset1 + num_steps + 1], ... , [random_offset_batchsize + 1:random_offset_batchsize + num_steps + 1] ]</span>
<span class="token keyword">def</span> <span class="token function">seq_data_iter_random</span><span class="token punctuation">(</span>corpus<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> num_steps<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""使用随机抽样生成一个小批量子序列"""</span>
    <span class="token comment"># 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1</span>
    corpus <span class="token operator">=</span> corpus<span class="token punctuation">[</span>random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> num_steps <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
    <span class="token comment"># 减去1，是因为我们需要考虑标签</span>
    num_subseqs <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>corpus<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">//</span> num_steps
    <span class="token comment"># 长度为num_steps的子序列的起始索引</span>
    <span class="token comment"># [0, num_steps*1, num_steps*2, num_steps*3, ...]</span>
    initial_indices <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> num_subseqs <span class="token operator">*</span> num_steps<span class="token punctuation">,</span> num_steps<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token comment"># 在随机抽样的迭代过程中，</span>
    <span class="token comment"># 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻</span>
    random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>initial_indices<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">data</span><span class="token punctuation">(</span>pos<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 返回从pos位置开始的长度为num_steps的序列</span>
        <span class="token keyword">return</span> corpus<span class="token punctuation">[</span>pos<span class="token punctuation">:</span> pos <span class="token operator">+</span> num_steps<span class="token punctuation">]</span>

    num_batches <span class="token operator">=</span> num_subseqs <span class="token operator">//</span> batch_size
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> batch_size <span class="token operator">*</span> num_batches<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 在这里，initial_indices包含子序列的随机起始索引</span>
        initial_indices_per_batch <span class="token operator">=</span> initial_indices<span class="token punctuation">[</span>i<span class="token punctuation">:</span> i <span class="token operator">+</span> batch_size<span class="token punctuation">]</span>
        X <span class="token operator">=</span> <span class="token punctuation">[</span>data<span class="token punctuation">(</span>j<span class="token punctuation">)</span> <span class="token keyword">for</span> j <span class="token keyword">in</span> initial_indices_per_batch<span class="token punctuation">]</span>
        Y <span class="token operator">=</span> <span class="token punctuation">[</span>data<span class="token punctuation">(</span>j <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token keyword">for</span> j <span class="token keyword">in</span> initial_indices_per_batch<span class="token punctuation">]</span>
        <span class="token keyword">yield</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>Y<span class="token punctuation">)</span>

<span class="token comment"># 以num_steps为步长，从随机的起始位置开始，返回</span>
<span class="token comment"># x1=[:, random_offset1:random_offset1 + num_steps]</span>
<span class="token comment"># y1=[:, random_offset1 + 1:random_offset1 + num_steps + 1]</span>

<span class="token keyword">def</span> <span class="token function">seq_data_iter_sequential</span><span class="token punctuation">(</span>corpus<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> num_steps<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""使用顺序分区生成一个小批量子序列"""</span>
    <span class="token comment"># 从随机偏移量开始划分序列</span>
    offset <span class="token operator">=</span> random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> num_steps<span class="token punctuation">)</span>
    num_tokens <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>corpus<span class="token punctuation">)</span> <span class="token operator">-</span> offset <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">//</span> batch_size<span class="token punctuation">)</span> <span class="token operator">*</span> batch_size
    <span class="token comment"># 重新根据corpus建立X_corpus, Y_corpus，两者之间差一位。注意X_corpus, Y_corpus的长度是batch_size的整数倍</span>
    Xs <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>corpus<span class="token punctuation">[</span>offset<span class="token punctuation">:</span> offset <span class="token operator">+</span> num_tokens<span class="token punctuation">]</span><span class="token punctuation">)</span>
    Ys <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>corpus<span class="token punctuation">[</span>offset <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">:</span> offset <span class="token operator">+</span> <span class="token number">1</span> <span class="token operator">+</span> num_tokens<span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token comment"># 直接根据batchsize划分X_corpus, Y_corpus</span>
    Xs<span class="token punctuation">,</span> Ys <span class="token operator">=</span> Xs<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> Ys<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token comment"># 计算出需要多少次才能取完数据</span>
    num_batches <span class="token operator">=</span> Xs<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">//</span> num_steps
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> num_steps <span class="token operator">*</span> num_batches<span class="token punctuation">,</span> num_steps<span class="token punctuation">)</span><span class="token punctuation">:</span>
        X <span class="token operator">=</span> Xs<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> i<span class="token punctuation">:</span> i <span class="token operator">+</span> num_steps<span class="token punctuation">]</span>
        Y <span class="token operator">=</span> Ys<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> i<span class="token punctuation">:</span> i <span class="token operator">+</span> num_steps<span class="token punctuation">]</span>
        <span class="token keyword">yield</span> X<span class="token punctuation">,</span> Y


<span class="token keyword">class</span> <span class="token class-name">SeqDataLoader</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""加载序列数据的迭代器"""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> num_steps<span class="token punctuation">,</span> use_random_iter<span class="token punctuation">,</span> max_tokens<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> use_random_iter<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>data_iter_fn <span class="token operator">=</span> seq_data_iter_random
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>data_iter_fn <span class="token operator">=</span> seq_data_iter_sequential
        self<span class="token punctuation">.</span>corpus<span class="token punctuation">,</span> self<span class="token punctuation">.</span>vocab <span class="token operator">=</span> dateset<span class="token punctuation">.</span>load_dataset<span class="token punctuation">(</span>max_tokens<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_steps <span class="token operator">=</span> batch_size<span class="token punctuation">,</span> num_steps

    <span class="token keyword">def</span> <span class="token function">__iter__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>data_iter_fn<span class="token punctuation">(</span>self<span class="token punctuation">.</span>corpus<span class="token punctuation">,</span> self<span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_steps<span class="token punctuation">)</span>
    
<span class="token keyword">def</span> <span class="token function">load_data_epoch</span><span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> num_steps<span class="token punctuation">,</span>  <span class="token comment">#@save</span>
                           use_random_iter<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> max_tokens<span class="token operator">=</span><span class="token number">10000</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""返回时光机器数据集的迭代器和词表"""</span>
    data_iter <span class="token operator">=</span> SeqDataLoader<span class="token punctuation">(</span>
        batch_size<span class="token punctuation">,</span> num_steps<span class="token punctuation">,</span> use_random_iter<span class="token punctuation">,</span> max_tokens<span class="token punctuation">)</span>
    <span class="token keyword">return</span> data_iter<span class="token punctuation">,</span> data_iter<span class="token punctuation">.</span>vocab



<span class="token keyword">def</span> <span class="token function">get_lstm_params</span><span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span class="token punctuation">:</span>
    num_inputs <span class="token operator">=</span> num_outputs <span class="token operator">=</span> vocab_size

    <span class="token keyword">def</span> <span class="token function">normal</span><span class="token punctuation">(</span>shape<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>size<span class="token operator">=</span>shape<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token operator">*</span><span class="token number">0.01</span>

    <span class="token keyword">def</span> <span class="token function">three</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token punctuation">(</span>normal<span class="token punctuation">(</span><span class="token punctuation">(</span>num_inputs<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                normal<span class="token punctuation">(</span><span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">)</span>

    W_xi<span class="token punctuation">,</span> W_hi<span class="token punctuation">,</span> b_i <span class="token operator">=</span> three<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 输入门参数</span>
    W_xf<span class="token punctuation">,</span> W_hf<span class="token punctuation">,</span> b_f <span class="token operator">=</span> three<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 遗忘门参数</span>
    W_xo<span class="token punctuation">,</span> W_ho<span class="token punctuation">,</span> b_o <span class="token operator">=</span> three<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 输出门参数</span>
    W_xc<span class="token punctuation">,</span> W_hc<span class="token punctuation">,</span> b_c <span class="token operator">=</span> three<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 候选记忆元参数</span>

    <span class="token comment"># 输出层参数</span>
    W_hq <span class="token operator">=</span> normal<span class="token punctuation">(</span><span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> num_outputs<span class="token punctuation">)</span><span class="token punctuation">)</span>
    b_q <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>num_outputs<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>

    <span class="token comment"># 附加梯度</span>
    params <span class="token operator">=</span> <span class="token punctuation">[</span>W_xi<span class="token punctuation">,</span> W_hi<span class="token punctuation">,</span> b_i<span class="token punctuation">,</span> W_xf<span class="token punctuation">,</span> W_hf<span class="token punctuation">,</span> b_f<span class="token punctuation">,</span> W_xo<span class="token punctuation">,</span> W_ho<span class="token punctuation">,</span> b_o<span class="token punctuation">,</span> W_xc<span class="token punctuation">,</span> W_hc<span class="token punctuation">,</span>
              b_c<span class="token punctuation">,</span> W_hq<span class="token punctuation">,</span> b_q<span class="token punctuation">]</span>
    <span class="token keyword">for</span> param <span class="token keyword">in</span> params<span class="token punctuation">:</span>
        param<span class="token punctuation">.</span>requires_grad_<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> params

<span class="token keyword">def</span> <span class="token function">init_lstm_state</span><span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span>
            torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">lstm</span><span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> state<span class="token punctuation">,</span> params<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token punctuation">[</span>W_xi<span class="token punctuation">,</span> W_hi<span class="token punctuation">,</span> b_i<span class="token punctuation">,</span> W_xf<span class="token punctuation">,</span> W_hf<span class="token punctuation">,</span> b_f<span class="token punctuation">,</span> W_xo<span class="token punctuation">,</span> W_ho<span class="token punctuation">,</span> b_o<span class="token punctuation">,</span> W_xc<span class="token punctuation">,</span> W_hc<span class="token punctuation">,</span> b_c<span class="token punctuation">,</span>
     W_hq<span class="token punctuation">,</span> b_q<span class="token punctuation">]</span> <span class="token operator">=</span> params
    <span class="token punctuation">(</span>H<span class="token punctuation">,</span> C<span class="token punctuation">)</span> <span class="token operator">=</span> state
    outputs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> X <span class="token keyword">in</span> inputs<span class="token punctuation">:</span>
        I <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">(</span>X @ W_xi<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span>H @ W_hi<span class="token punctuation">)</span> <span class="token operator">+</span> b_i<span class="token punctuation">)</span>
        F <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">(</span>X @ W_xf<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span>H @ W_hf<span class="token punctuation">)</span> <span class="token operator">+</span> b_f<span class="token punctuation">)</span>
        O <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">(</span>X @ W_xo<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span>H @ W_ho<span class="token punctuation">)</span> <span class="token operator">+</span> b_o<span class="token punctuation">)</span>
        C_tilda <span class="token operator">=</span> torch<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span><span class="token punctuation">(</span>X @ W_xc<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span>H @ W_hc<span class="token punctuation">)</span> <span class="token operator">+</span> b_c<span class="token punctuation">)</span>
        C <span class="token operator">=</span> F <span class="token operator">*</span> C <span class="token operator">+</span> I <span class="token operator">*</span> C_tilda
        H <span class="token operator">=</span> O <span class="token operator">*</span> torch<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>C<span class="token punctuation">)</span>
        Y <span class="token operator">=</span> <span class="token punctuation">(</span>H @ W_hq<span class="token punctuation">)</span> <span class="token operator">+</span> b_q
        outputs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>Y<span class="token punctuation">)</span>
    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>H<span class="token punctuation">,</span> C<span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">try_gpu</span><span class="token punctuation">(</span>i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""如果存在，则返回gpu(i)，否则返回cpu()

    Defined in :numref:`sec_use_gpu`"""</span>
    <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>device_count<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">>=</span> i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'cuda:</span><span class="token interpolation"><span class="token punctuation">&#123;</span>i<span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cpu'</span><span class="token punctuation">)</span>


<span class="token comment">#@save</span>
<span class="token keyword">class</span> <span class="token class-name">RNNModel</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""循环神经网络模型"""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> rnn_layer<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> device<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>RNNModel<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>rnn <span class="token operator">=</span> rnn_layer
        self<span class="token punctuation">.</span>vocab_size <span class="token operator">=</span> vocab_size
        self<span class="token punctuation">.</span>num_hiddens <span class="token operator">=</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>hidden_size
        <span class="token comment"># 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1</span>
        <span class="token keyword">if</span> <span class="token keyword">not</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>bidirectional<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>num_directions <span class="token operator">=</span> <span class="token number">1</span>
            self<span class="token punctuation">.</span>linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_hiddens<span class="token punctuation">,</span> self<span class="token punctuation">.</span>vocab_size<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>num_directions <span class="token operator">=</span> <span class="token number">2</span>
            self<span class="token punctuation">.</span>linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_hiddens <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>vocab_size<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">,</span> state<span class="token punctuation">)</span><span class="token punctuation">:</span>
        X <span class="token operator">=</span> F<span class="token punctuation">.</span>one_hot<span class="token punctuation">(</span>inputs<span class="token punctuation">.</span>T<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>vocab_size<span class="token punctuation">)</span>
        X <span class="token operator">=</span> X<span class="token punctuation">.</span>to<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
        Y<span class="token punctuation">,</span> state <span class="token operator">=</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">(</span>X<span class="token punctuation">,</span> state<span class="token punctuation">)</span>
        <span class="token comment"># 全连接层首先将Y的形状改为(时间步数*批量大小,隐藏单元数)</span>
        <span class="token comment"># 它的输出形状是(时间步数*批量大小,词表大小)。</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>Y<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> Y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> output<span class="token punctuation">,</span> state

    <span class="token keyword">def</span> <span class="token function">begin_state</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> device<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token keyword">not</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>rnn<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>LSTM<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># nn.GRU以张量作为隐状态</span>
            <span class="token keyword">return</span>  torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_directions <span class="token operator">*</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>num_layers<span class="token punctuation">,</span>
                                 batch_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_hiddens<span class="token punctuation">)</span><span class="token punctuation">,</span>
                                device<span class="token operator">=</span>device<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token comment"># nn.LSTM以元组作为隐状态</span>
            <span class="token keyword">return</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>
                self<span class="token punctuation">.</span>num_directions <span class="token operator">*</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>num_layers<span class="token punctuation">,</span>
                batch_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_hiddens<span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span>
                    torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>
                        self<span class="token punctuation">.</span>num_directions <span class="token operator">*</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>num_layers<span class="token punctuation">,</span>
                        batch_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_hiddens<span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">)</span>


<span class="token keyword">class</span> <span class="token class-name">RNNModelScratch</span><span class="token punctuation">:</span> <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""从零开始实现的循环神经网络模型"""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> device<span class="token punctuation">,</span>
                 get_params<span class="token punctuation">,</span> init_state<span class="token punctuation">,</span> forward_fn<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>vocab_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_hiddens <span class="token operator">=</span> vocab_size<span class="token punctuation">,</span> num_hiddens
        <span class="token comment"># 初始化了隐藏参数 W_xh, W_hh, b_h,  W_hq, b_q</span>
        self<span class="token punctuation">.</span>params <span class="token operator">=</span> get_params<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> device<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>init_state<span class="token punctuation">,</span> self<span class="token punctuation">.</span>forward_fn <span class="token operator">=</span> init_state<span class="token punctuation">,</span> forward_fn

    <span class="token keyword">def</span> <span class="token function">__call__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> state<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># X的形状：(batch_size, num_steps)</span>
        <span class="token comment"># X one_hot之后的形状：(num_steps，batch_size，词表大小)</span>
        X <span class="token operator">=</span> F<span class="token punctuation">.</span>one_hot<span class="token punctuation">(</span>X<span class="token punctuation">.</span>T<span class="token punctuation">,</span> self<span class="token punctuation">.</span>vocab_size<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>forward_fn<span class="token punctuation">(</span>X<span class="token punctuation">,</span> state<span class="token punctuation">,</span> self<span class="token punctuation">.</span>params<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">begin_state</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>init_state<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_hiddens<span class="token punctuation">,</span> device<span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">predict_ch8</span><span class="token punctuation">(</span>prefix<span class="token punctuation">,</span> num_preds<span class="token punctuation">,</span> net<span class="token punctuation">,</span> vocab<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""在prefix后面生成新字符"""</span>
    state <span class="token operator">=</span> net<span class="token punctuation">.</span>begin_state<span class="token punctuation">(</span>batch_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>
    outputs <span class="token operator">=</span> <span class="token punctuation">[</span>vocab<span class="token punctuation">[</span>prefix<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
    get_input <span class="token operator">=</span> <span class="token keyword">lambda</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>outputs<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> y <span class="token keyword">in</span> prefix<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">:</span>  <span class="token comment"># 预热期</span>
        _<span class="token punctuation">,</span> state <span class="token operator">=</span> net<span class="token punctuation">(</span>get_input<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> state<span class="token punctuation">)</span>
        outputs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>vocab<span class="token punctuation">[</span>y<span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_preds<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 预测num_preds步</span>
        <span class="token comment"># y 包含从开始到现在的所有输出</span>
        <span class="token comment"># state是当前计算出来的隐藏参数</span>
        y<span class="token punctuation">,</span> state <span class="token operator">=</span> net<span class="token punctuation">(</span>get_input<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> state<span class="token punctuation">)</span>
        outputs<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">(</span>y<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> <span class="token string">''</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">[</span>vocab<span class="token punctuation">.</span>idx_to_token<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> outputs<span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">grad_clipping</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> theta<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""裁剪梯度"""</span>
    <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
        params <span class="token operator">=</span> <span class="token punctuation">[</span>p <span class="token keyword">for</span> p <span class="token keyword">in</span> net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> p<span class="token punctuation">.</span>requires_grad<span class="token punctuation">]</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        params <span class="token operator">=</span> net<span class="token punctuation">.</span>params
    norm <span class="token operator">=</span> torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token builtin">sum</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">(</span>p<span class="token punctuation">.</span>grad <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">for</span> p <span class="token keyword">in</span> params<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> norm <span class="token operator">></span> theta<span class="token punctuation">:</span>
        <span class="token keyword">for</span> param <span class="token keyword">in</span> params<span class="token punctuation">:</span>
            param<span class="token punctuation">.</span>grad<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">*=</span> theta <span class="token operator">/</span> norm


<span class="token keyword">def</span> <span class="token function">train_epoch_ch8</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> loss<span class="token punctuation">,</span> updater<span class="token punctuation">,</span> device<span class="token punctuation">,</span> use_random_iter<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""训练网络一个迭代周期（定义见第8章）"""</span>
    state<span class="token punctuation">,</span> timer <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span> Timer<span class="token punctuation">(</span><span class="token punctuation">)</span>
    metric <span class="token operator">=</span> Accumulator<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># 训练损失之和,词元数量</span>
    <span class="token comment"># X的形状：(batch_size, num_steps)</span>
    <span class="token comment"># Y的形状：(batch_size, num_steps)</span>
    <span class="token keyword">for</span> X<span class="token punctuation">,</span> Y <span class="token keyword">in</span> train_iter<span class="token punctuation">:</span>
        <span class="token keyword">if</span> state <span class="token keyword">is</span> <span class="token boolean">None</span> <span class="token keyword">or</span> use_random_iter<span class="token punctuation">:</span>
            <span class="token comment"># 在第一次迭代或使用随机抽样时初始化state</span>
            state <span class="token operator">=</span> net<span class="token punctuation">.</span>begin_state<span class="token punctuation">(</span>batch_size<span class="token operator">=</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span> <span class="token keyword">and</span> <span class="token keyword">not</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>state<span class="token punctuation">,</span> <span class="token builtin">tuple</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                <span class="token comment"># state对于nn.GRU是个张量</span>
                state<span class="token punctuation">.</span>detach_<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                <span class="token comment"># state对于nn.LSTM或对于我们从零开始实现的模型是个张量</span>
                <span class="token keyword">for</span> s <span class="token keyword">in</span> state<span class="token punctuation">:</span>
                    s<span class="token punctuation">.</span>detach_<span class="token punctuation">(</span><span class="token punctuation">)</span>
        y <span class="token operator">=</span> Y<span class="token punctuation">.</span>T<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        X<span class="token punctuation">,</span> y <span class="token operator">=</span> X<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
        <span class="token comment"># y_hat 包含从开始到现在的所有输出</span>
        <span class="token comment"># y_hat的形状：（batch_size * num_steps， 词表大小）</span>
        <span class="token comment"># state是当前计算出来的隐藏参数</span>
        y_hat<span class="token punctuation">,</span> state <span class="token operator">=</span> net<span class="token punctuation">(</span>X<span class="token punctuation">,</span> state<span class="token punctuation">)</span>
        <span class="token comment"># 交叉熵损失函数，传入预测值和标签值，并求平均值</span>
        l <span class="token operator">=</span> loss<span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span> y<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>updater<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Optimizer<span class="token punctuation">)</span><span class="token punctuation">:</span>
            updater<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
            l<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
            grad_clipping<span class="token punctuation">(</span>net<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
            updater<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            l<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
            grad_clipping<span class="token punctuation">(</span>net<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token comment"># 因为已经调用了mean函数</span>
            updater<span class="token punctuation">(</span>batch_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># 这里记录交叉熵损失的值的和，以及记录对应交叉熵损失值的样本个数</span>
        metric<span class="token punctuation">.</span>add<span class="token punctuation">(</span>l <span class="token operator">*</span> y<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token comment"># 求交叉熵损失的平均值，再求exp，即可得到困惑度</span>
    <span class="token keyword">return</span> math<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>metric<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">/</span> metric<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> metric<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">/</span> timer<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span>


<span class="token keyword">def</span> <span class="token function">sgd</span><span class="token punctuation">(</span>params<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""小批量随机梯度下降

    Defined in :numref:`sec_linear_scratch`"""</span>
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> param <span class="token keyword">in</span> params<span class="token punctuation">:</span>
            param <span class="token operator">-=</span> lr <span class="token operator">*</span> param<span class="token punctuation">.</span>grad <span class="token operator">/</span> batch_size
            param<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment">#@save</span>
<span class="token keyword">def</span> <span class="token function">train_ch8</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> vocab<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> device<span class="token punctuation">,</span>
              use_random_iter<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""训练模型（定义见第8章）"""</span>
    loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># 新建一个连接客户端</span>
    <span class="token comment"># 指定 env=u'test1'，默认端口为 8097，host 是 'localhost'</span>
    vis <span class="token operator">=</span> visdom<span class="token punctuation">.</span>Visdom<span class="token punctuation">(</span>env<span class="token operator">=</span><span class="token string">u'test1'</span><span class="token punctuation">,</span> server<span class="token operator">=</span><span class="token string">"http://10.88.88.136"</span><span class="token punctuation">,</span> port<span class="token operator">=</span><span class="token number">8097</span><span class="token punctuation">)</span>
    animator <span class="token operator">=</span> vis
    <span class="token comment"># 初始化</span>
    <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
        updater <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        updater <span class="token operator">=</span> <span class="token keyword">lambda</span> batch_size<span class="token punctuation">:</span> sgd<span class="token punctuation">(</span>net<span class="token punctuation">.</span>params<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>
    predict <span class="token operator">=</span> <span class="token keyword">lambda</span> prefix<span class="token punctuation">:</span> predict_ch8<span class="token punctuation">(</span>prefix<span class="token punctuation">,</span> <span class="token number">30</span><span class="token punctuation">,</span> net<span class="token punctuation">,</span> vocab<span class="token punctuation">,</span> device<span class="token punctuation">)</span>
    <span class="token comment"># 训练和预测</span>
    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        ppl<span class="token punctuation">,</span> speed <span class="token operator">=</span> train_epoch_ch8<span class="token punctuation">(</span>
            net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> loss<span class="token punctuation">,</span> updater<span class="token punctuation">,</span> device<span class="token punctuation">,</span> use_random_iter<span class="token punctuation">)</span>
        


        <span class="token keyword">if</span> <span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">10</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            <span class="token comment"># print(predict('你是？'))</span>
            <span class="token comment"># print(epoch)</span>
            <span class="token comment"># animator.add(epoch + 1, )</span>

            <span class="token keyword">if</span> epoch <span class="token operator">==</span> <span class="token number">9</span><span class="token punctuation">:</span>
                <span class="token comment"># 清空图表：使用空数组来替换现有内容</span>
                vis<span class="token punctuation">.</span>line<span class="token punctuation">(</span>X<span class="token operator">=</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> Y<span class="token operator">=</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> win<span class="token operator">=</span><span class="token string">'train_ch8'</span><span class="token punctuation">,</span> update<span class="token operator">=</span><span class="token string">'replace'</span><span class="token punctuation">)</span>

            vis<span class="token punctuation">.</span>line<span class="token punctuation">(</span>
                X<span class="token operator">=</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                Y<span class="token operator">=</span><span class="token punctuation">[</span>ppl<span class="token punctuation">]</span><span class="token punctuation">,</span>
                win<span class="token operator">=</span><span class="token string">'train_ch8'</span><span class="token punctuation">,</span>
                update<span class="token operator">=</span><span class="token string">'append'</span><span class="token punctuation">,</span>
                opts<span class="token operator">=</span><span class="token punctuation">&#123;</span>
                    <span class="token string">'title'</span><span class="token punctuation">:</span> <span class="token string">'train_ch8'</span><span class="token punctuation">,</span>
                    <span class="token string">'xlabel'</span><span class="token punctuation">:</span> <span class="token string">'epoch'</span><span class="token punctuation">,</span>
                    <span class="token string">'ylabel'</span><span class="token punctuation">:</span> <span class="token string">'ppl'</span><span class="token punctuation">,</span>
                    <span class="token string">'linecolor'</span><span class="token punctuation">:</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">255</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment"># 蓝色线条</span>
                <span class="token punctuation">&#125;</span>
            <span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'困惑度 </span><span class="token interpolation"><span class="token punctuation">&#123;</span>ppl<span class="token punctuation">:</span><span class="token format-spec">.1f</span><span class="token punctuation">&#125;</span></span><span class="token string">, </span><span class="token interpolation"><span class="token punctuation">&#123;</span>speed<span class="token punctuation">:</span><span class="token format-spec">.1f</span><span class="token punctuation">&#125;</span></span><span class="token string"> 词元/秒 </span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">str</span><span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>predict<span class="token punctuation">(</span><span class="token string">'你是'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>predict<span class="token punctuation">(</span><span class="token string">'我有一剑'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    batch_size<span class="token punctuation">,</span> num_steps <span class="token operator">=</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">35</span>
    train_iter<span class="token punctuation">,</span> vocab <span class="token operator">=</span> load_data_epoch<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> num_steps<span class="token punctuation">)</span>

    vocab_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> device <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>vocab<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> try_gpu<span class="token punctuation">(</span><span class="token punctuation">)</span>
    num_epochs<span class="token punctuation">,</span> lr <span class="token operator">=</span> <span class="token number">1000</span><span class="token punctuation">,</span> <span class="token number">1</span>
    model <span class="token operator">=</span> RNNModelScratch<span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>vocab<span class="token punctuation">)</span><span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> device<span class="token punctuation">,</span> get_lstm_params<span class="token punctuation">,</span> init_lstm_state<span class="token punctuation">,</span> lstm<span class="token punctuation">)</span>
    
    <span class="token comment"># num_inputs = vocab_size</span>
    <span class="token comment"># lstm_layer = nn.LSTM(num_inputs, num_hiddens)</span>
    <span class="token comment"># model = RNNModel(lstm_layer, len(vocab), device)</span>
    <span class="token comment"># model = model.to(device)</span>
    
    <span class="token keyword">print</span><span class="token punctuation">(</span>predict_ch8<span class="token punctuation">(</span><span class="token string">'你是'</span><span class="token punctuation">,</span> <span class="token number">30</span><span class="token punctuation">,</span> model<span class="token punctuation">,</span> vocab<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span class="token punctuation">)</span>
    train_ch8<span class="token punctuation">(</span>model<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> vocab<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>  我们分别使用手动构建的LSTM和框架构建的LSTM进行训练和测试，结果如下：</p>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/blog_idx_141/scratch_train.png" alt="rep_img"/></center>
    </div>
</div>   
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/blog_idx_141/scratch_ret.png" alt="rep_img"/></center>
    </div>
</div>   
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/blog_idx_141/torch_train.png" alt="rep_img"/></center>
    </div>
</div>   
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/blog_idx_141/torch_ret.png" alt="rep_img"/></center>
    </div>
</div>   
<p>  我们可以看到，模型未训练和训练后的对比，明显训练后的语句要通顺一点。</p>
<br/>
<br/>
<br/>
<br/>
<h3 id="后记">后记</h3>
<hr>
<p>  综合RNN和LSTM两篇文章的结论来看，其对序列数据确实有一定的效果。</p>
<p>  此外，当前我们用RNN/LSTM做了序列数据的后续模拟生成工作，但是由于网络深度、广度的问题，其效果也就比在词表中随机抽取字组成的序列看起来要好点。</p>
<h3 id="参考文献">参考文献</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://zh.d2l.ai/chapter_recurrent-modern/lstm.html">https://zh.d2l.ai/chapter_recurrent-modern/lstm.html</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://zh.d2l.ai/chapter_recurrent-neural-networks/rnn.html">https://zh.d2l.ai/chapter_recurrent-neural-networks/rnn.html</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://zh.d2l.ai/chapter_recurrent-neural-networks/text-preprocessing.html">https://zh.d2l.ai/chapter_recurrent-neural-networks/text-preprocessing.html</a></p>
</li>
</ul>
<br/>
<br/>
<div style="margin:50px auto;">
</div>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <hr/>
        <center><font color = #91e0b0 size = 5>打赏、订阅、收藏、丢香蕉、硬币，请关注公众号（攻城狮的搬砖之路）</font></center>
    </div>
</div>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/qrcode_for_wx_official_account.jpg" alt="qrc_img"/></center>
    </div>
</div>
<!-- ![alt 公众号图片](https://flyinskyin2013.github.io/ImageBed0/blogs/qrcode_for_wx_official_account.jpg "公众号图片") -->
<p><font color="red" size="7">PS: 请尊重原创，不喜勿喷。</font><br/><br>
<font color="red" size="7">PS: 要转载请注明出处，本人版权所有。</font><br/><br>
<font color="red" size="7">PS: 有问题请留言，看到后我会第一时间回复。</font><br/></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://e-x.top/2025/08/28/blog_idx_140/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Sky">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sky's Blogs">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | Sky's Blogs">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/08/28/blog_idx_140/" class="post-title-link" itemprop="url">-fno-rtti导致的惨案（object has invalid vptr）</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-08-28 13:11:00 / 修改时间：13:14:45" itemprop="dateCreated datePublished" datetime="2025-08-28T13:11:00+08:00">2025-08-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/sanitizer/" itemprop="url" rel="index"><span itemprop="name">sanitizer</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/sanitizer/C-C/" itemprop="url" rel="index"><span itemprop="name">C&C++</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.4k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>5 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <script src="\assets\js\APlayer.min.js"> </script><!--
 * @Description: 
 * @Author: Sky
 * @Date: 2020-08-24 16:37:34
 * @LastEditors: Sky
 * @LastEditTime: 2021-06-29 15:04:48
 * @Github: https://github.com/flyinskyin2013/
-->
<p><font color="red" size="7">PS：要转载请注明出处，本人版权所有。</font><br/><br>
<font color="red" size="7">PS: 这个只是基于《我自己》的理解，</font><br/><font color="red" size="7">如果和你的原则及想法相冲突，请谅解，勿喷。</font><br/></p>
<!-- ###### 前置说明
&emsp;&emsp;本文作为本人csdn blog的主站的备份。（BlogID=140） 
&emsp;&emsp;本文发布于 2025-08-28 13:11:00，现用MarkDown+图床做备份更新。blog原图已丢失，使用csdn所存的图进行更新。（BlogID=140） 
-->
<h6 id="环境说明">环境说明</h6>
<ul class="lvl-0">
<li class="lvl-2">
<p>Ubuntu 24.04.2 LTS \n \l</p>
</li>
<li class="lvl-2">
<p>gcc version 13.3.0 (Ubuntu 13.3.0-6ubuntu2~24.04)</p>
</li>
</ul>
<h3 id="前言">前言</h3>
<hr>
<p>  对于C++程序开发来说，MemoryLeek/UndefinedBehavior 等问题，简直就是大型开发过程必定会出现的问题。那么我们怎么尝试减少这些问题，在我的日常开发中，大概有以下方案：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>对于开发过程中来说，在c++11以后，标准库引入了智能指针，然后增强开发者内存所有权意识等，可以有效减少MemoryLeek问题。</p>
</li>
<li class="lvl-2">
<p>对于测试发布流程来说，我们常常引入了valgrind/sanitizer减少MemoryLeek/UndefinedBehavior 等问题。</p>
</li>
</ul>
<p>  尤其是对于新的编译器来说，sanitizer还是比较好用的。最近遇到了一个不是那么常见的sanitizer ub错误，我觉得非常有趣，可以分享一下。</p>
<br/>
<br/>
<br/>
<br/>
<h3 id="fno-rtti导致的惨案">-fno-rtti导致的惨案</h3>
<hr>
<p>  下面的图片是出现的问题现场截图：</p>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/blog_idx_140/invalid_ptr.png" alt="rep_img"/></center>
    </div>
</div>    
<p>  上图一看就是ub错误，具体是什么原因，还要分析一番。</p>
<br/>
<br/>
<h5 id="问题最小用例复现">问题最小用例复现</h5>
<p>  下面是最小的复现用例：</p>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;l.cpp
#include &lt;memory&gt;

#include &quot;A.hpp&quot;


        void B::p()&#123;printf(&quot;p(): class B\n&quot;);&#125;
        void B::p1()&#123;printf(&quot;p1(): class B\n&quot;);&#125;

        void A::p()&#123;printf(&quot;p(): class A\n&quot;);&#125;
        void A::p1()&#123;printf(&quot;p1(): class A\n&quot;);&#125;


std::shared_ptr&lt;A&gt; my_A(new A());

void i()&#123;

        my_A-&gt;p1();
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;l.hpp
void i();<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;l.hpp
void i();<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;A.hpp
#include &lt;cstdio&gt;
class B&#123;
        public:
        virtual void p();
        virtual void p1();

&#125;;
class A:public B&#123;
        public:
                void p();
                void p1();
&#125;;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;t.cpp
#include &lt;memory&gt;
#include &lt;cstdio&gt;

#include &quot;A.hpp&quot;
#include &quot;l.hpp&quot;

std::shared_ptr&lt;A&gt; my_AA(new A());

int main(int argc, const char* argv[])
&#123;
        my_AA-&gt;p();
        i();
        return 0;
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">g++ <span class="token parameter variable">-c</span> l.cpp <span class="token parameter variable">-O3</span> <span class="token parameter variable">-fPIC</span> <span class="token parameter variable">-fsanitize</span><span class="token operator">=</span>undefined

g++ <span class="token parameter variable">-shared</span> <span class="token parameter variable">-o</span> libA.so l.o <span class="token parameter variable">-O3</span>

g++ t.cpp <span class="token parameter variable">-o</span> t <span class="token parameter variable">-O3</span> -I. <span class="token parameter variable">-L</span> <span class="token builtin class-name">.</span> <span class="token parameter variable">-l</span> A -fno-rtti <span class="token parameter variable">-fsanitize</span><span class="token operator">=</span>undefined -Wl,-rpath<span class="token operator">=</span>. 

<span class="token comment"># ./t 运行就会得到如上的错误</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>注意上述例子用到了多态类，这和我原始工程中类似，但是实际情况中，一个普通的类也会有同样的问题，具体原因，见如下分析。</p>
<br/>
<br/>
<h5 id="问题分析">问题分析</h5>
<p>  首先我们看看出现的_Sp_counted_base/_Sp_counted_ptr是什么，这个通过报错，看起来像shared_ptr引用计数相关，我们看看其实际的代码大致关系如下：</p>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;typename _Tp&gt;
class shared_ptr : public __shared_ptr&lt;_Tp&gt;
&#123;
    &#x2F;&#x2F;...
&#125;


class __shared_ptr
: public __shared_ptr_access&lt;_Tp, _Lp&gt;
&#123;
    &#x2F;&#x2F;...
    __shared_count&lt;_Lp&gt;  _M_refcount;    &#x2F;&#x2F; Reference counter.
&#125;


template&lt;_Lock_policy _Lp&gt;
class __shared_count
&#123;
    template&lt;typename _Ptr&gt;
    explicit
    __shared_count(_Ptr __p) : _M_pi(0)
    &#123;
    __try
        &#123;
            _M_pi &#x3D; new _Sp_counted_ptr&lt;_Ptr, _Lp&gt;(__p);
        &#125;
    __catch(...)
        &#123;
            delete __p;
            __throw_exception_again;
        &#125;
    &#125;
    &#x2F;&#x2F;...
    _Sp_counted_base&lt;_Lp&gt;*  _M_pi;
&#125;


template&lt;typename _Ptr, _Lock_policy _Lp&gt;
class _Sp_counted_ptr final : public _Sp_counted_base&lt;_Lp&gt;
&#123;
    &#x2F;&#x2F;...
&#125;

template&lt;_Lock_policy _Lp &#x3D; __default_lock_policy&gt;
class _Sp_counted_base
: public _Mutex_base&lt;_Lp&gt;
&#123;
    &#x2F;&#x2F;...
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>  我们使用如下命令，看一下t和libA.so的_Sp_counted_ptr符号，我们发现对于相同的符号来说，其大小不一样。</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token comment"># readelf -sW libA.so |grep counted_ptr</span>
    <span class="token number">24</span>: 0000000000005160    <span class="token number">54</span> OBJECT  WEAK   DEFAULT   <span class="token number">16</span> _ZTSSt15_Sp_counted_ptrIP1ALN9__gnu_cxx12_Lock_policyE2EE
    <span class="token number">25</span>: 0000000000003970   <span class="token number">338</span> FUNC    WEAK   DEFAULT   <span class="token number">14</span> _ZNSt15_Sp_counted_ptrIP1ALN9__gnu_cxx12_Lock_policyE2EED1Ev
    <span class="token number">27</span>: 0000000000003970   <span class="token number">338</span> FUNC    WEAK   DEFAULT   <span class="token number">14</span> _ZNSt15_Sp_counted_ptrIP1ALN9__gnu_cxx12_Lock_policyE2EED2Ev
    <span class="token number">30</span>: 0000000000003790     <span class="token number">7</span> FUNC    WEAK   DEFAULT   <span class="token number">14</span> _ZNSt15_Sp_counted_ptrIP1ALN9__gnu_cxx12_Lock_policyE2EE14_M_get_deleterERKSt9type_info
    <span class="token number">33</span>: 0000000000006d80    <span class="token number">56</span> OBJECT  WEAK   DEFAULT   <span class="token number">22</span> _ZTVSt15_Sp_counted_ptrIP1ALN9__gnu_cxx12_Lock_policyE2EE
    <span class="token number">40</span>: 0000000000006cf0    <span class="token number">24</span> OBJECT  WEAK   DEFAULT   <span class="token number">22</span> _ZTISt15_Sp_counted_ptrIP1ALN9__gnu_cxx12_Lock_policyE2EE
    <span class="token number">44</span>: 0000000000003c30   <span class="token number">489</span> FUNC    WEAK   DEFAULT   <span class="token number">14</span> _ZNSt15_Sp_counted_ptrIP1ALN9__gnu_cxx12_Lock_policyE2EE10_M_destroyEv
    <span class="token number">48</span>: 0000000000003880   <span class="token number">225</span> FUNC    WEAK   DEFAULT   <span class="token number">14</span> _ZNSt15_Sp_counted_ptrIP1ALN9__gnu_cxx12_Lock_policyE2EE10_M_disposeEv
    <span class="token number">53</span>: 0000000000003ad0   <span class="token number">350</span> FUNC    WEAK   DEFAULT   <span class="token number">14</span> _ZNSt15_Sp_counted_ptrIP1ALN9__gnu_cxx12_Lock_policyE2EED0Ev<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token comment"># readelf -sW t |grep counted_ptr</span>
    <span class="token number">20</span>: 0000000000002700   <span class="token number">172</span> FUNC    WEAK   DEFAULT   <span class="token number">16</span> _ZNSt15_Sp_counted_ptrIP1ALN9__gnu_cxx12_Lock_policyE2EED2Ev
    <span class="token number">22</span>: 0000000000002700   <span class="token number">172</span> FUNC    WEAK   DEFAULT   <span class="token number">16</span> _ZNSt15_Sp_counted_ptrIP1ALN9__gnu_cxx12_Lock_policyE2EED1Ev
    <span class="token number">23</span>: 0000000000002870   <span class="token number">233</span> FUNC    WEAK   DEFAULT   <span class="token number">16</span> _ZNSt15_Sp_counted_ptrIP1ALN9__gnu_cxx12_Lock_policyE2EE10_M_destroyEv
    <span class="token number">24</span>: 00000000000027b0   <span class="token number">188</span> FUNC    WEAK   DEFAULT   <span class="token number">16</span> _ZNSt15_Sp_counted_ptrIP1ALN9__gnu_cxx12_Lock_policyE2EED0Ev
    <span class="token number">26</span>: 00000000000026a0    <span class="token number">92</span> FUNC    WEAK   DEFAULT   <span class="token number">16</span> _ZNSt15_Sp_counted_ptrIP1ALN9__gnu_cxx12_Lock_policyE2EE10_M_disposeEv
    <span class="token number">30</span>: 0000000000002610     <span class="token number">7</span> FUNC    WEAK   DEFAULT   <span class="token number">16</span> _ZNSt15_Sp_counted_ptrIP1ALN9__gnu_cxx12_Lock_policyE2EE14_M_get_deleterERKSt9type_info
    <span class="token number">32</span>: 0000000000005ca0    <span class="token number">56</span> OBJECT  WEAK   DEFAULT   <span class="token number">24</span> _ZTVSt15_Sp_counted_ptrIP1ALN9__gnu_cxx12_Lock_policyE2EE<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>  这个时候我们想一下-fno-rtti的作用，其作用是禁用typeinfo+dynamic_cast，某些情况下可以提升执行性能。然后根据错误中的提示（object has invalid vptr），必定和其虚表有关系，那就意味着_Sp_counted_base/_Sp_counted_ptr的虚表存在异常。</p>
<p>  用ida查看t和libA.so中std::_Sp_counted_base的虚表内容，他们如下图：</p>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/blog_idx_140/sp_counted_base_vtable.png" alt="rep_img"/></center>
    </div>
</div>    
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/blog_idx_140/sp_counted_base_vtable_with_typeinfo.png" alt="rep_img"/></center>
    </div>
</div>    
<p>  注意，一般的虚表结构如下：</p>
<pre class="line-numbers language-txt" data-language="txt"><code class="language-txt">+-------------------+
|  Offset-to-Top    | (通常为负数，用于多重继承)
+-------------------+
|  type_info 指针    | (用于 RTTI)
+-------------------+
|  虚函数1 的地址    |
+-------------------+
|  虚函数2 的地址    |
+-------------------+
|      ...          |<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>  从上面的图和虚表结构可知，就是两个同名的vtable内容不一样，导致了此问题。解决方法也很简单，大家使用同样的编译参数即可。</p>
<br/>
<br/>
<br/>
<br/>
<h3 id="后记">后记</h3>
<hr>
<p>  c++的一些错误是非常有趣的，值得细看。</p>
<h3 id="参考文献">参考文献</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p>无</p>
</li>
</ul>
<br/>
<br/>
<div style="margin:50px auto;">
</div>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <hr/>
        <center><font color = #91e0b0 size = 5>打赏、订阅、收藏、丢香蕉、硬币，请关注公众号（攻城狮的搬砖之路）</font></center>
    </div>
</div>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/qrcode_for_wx_official_account.jpg" alt="qrc_img"/></center>
    </div>
</div>
<!-- ![alt 公众号图片](https://flyinskyin2013.github.io/ImageBed0/blogs/qrcode_for_wx_official_account.jpg "公众号图片") -->
<p><font color="red" size="7">PS: 请尊重原创，不喜勿喷。</font><br/><br>
<font color="red" size="7">PS: 要转载请注明出处，本人版权所有。</font><br/><br>
<font color="red" size="7">PS: 有问题请留言，看到后我会第一时间回复。</font><br/></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://e-x.top/2025/07/05/blog_idx_139/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Sky">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sky's Blogs">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | Sky's Blogs">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/07/05/blog_idx_139/" class="post-title-link" itemprop="url">大模型基础补全计划(三)---RNN实例与测试</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-07-05 17:47:00 / 修改时间：17:52:32" itemprop="dateCreated datePublished" datetime="2025-07-05T17:47:00+08:00">2025-07-05</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/LLM/" itemprop="url" rel="index"><span itemprop="name">LLM</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/LLM/LM/" itemprop="url" rel="index"><span itemprop="name">LM</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.5k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>16 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <script src="\assets\js\APlayer.min.js"> </script><!--
 * @Description: 
 * @Author: Sky
 * @Date: 2020-08-24 16:37:34
 * @LastEditors: Sky
 * @LastEditTime: 2021-06-29 15:04:48
 * @Github: https://github.com/flyinskyin2013/
-->
<p><font color="red" size="7">PS：要转载请注明出处，本人版权所有。</font><br/><br>
<font color="red" size="7">PS: 这个只是基于《我自己》的理解，</font><br/><font color="red" size="7">如果和你的原则及想法相冲突，请谅解，勿喷。</font><br/></p>
<!-- ###### 前置说明
&emsp;&emsp;本文作为本人github blog/cnblog的主站的备份。（BlogID=139） 
&emsp;&emsp;本文发布于 2025-07-05 17:47:00           （BlogID=139） 
-->
<h6 id="环境说明">环境说明</h6>
<p>  无</p>
<h3 id="前言">前言</h3>
<hr>
<p>   本文是这个系列第三篇，它们是：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>《大模型基础补全计划(一)—重温一些深度学习相关的数学知识》 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/Iflyinsky/p/18717317">https://www.cnblogs.com/Iflyinsky/p/18717317</a></p>
</li>
<li class="lvl-2">
<p>《大模型基础补全计划(二)—词嵌入(word embedding) 》 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/Iflyinsky/p/18775451">https://www.cnblogs.com/Iflyinsky/p/18775451</a></p>
</li>
</ul>
<p>   在CV世界里，卷积神经网络一直是主流。在以前，NLP的世界里，循环神经网络是主流，站在今天大模型时代，Transformer 及相关变体，是当今的NLP的绝对主流。但是我们要了解Transformer提出的原因，还需要回到循环神经网络，了解其历史变迁。当然，在循环神经网络中，一些主流的概念当前也还在使用，例如：token、词表等等。</p>
<p>  因此，如本文题目所示，本文主要简单介绍一下RNN，并尝试用RNN训练一个简单的文本续写模型。</p>
<br/>
<br/>
<br/>
<br/>
<h3 id="RNN-（Recurrent-Neural-Network）">RNN （Recurrent Neural Network）</h3>
<hr>
<p>  </p>
<br/>
<br/>
<h5 id="RNN的意义">RNN的意义</h5>
<p>  在提到rnn之前，我们还是有必要先提一下cnn，cnn的应用目标是指定一个输入，获得一个模型输出，多次输入之间是没有必然联系。然而，在日常生活中，我们还有许多其他的任务是多个输入之间是有前后关系的。例如：机翻、对话模型等等，这些任务都有明显的特征，那就是输入数据是一个序列，前面输入的数据会对后面的输出产生了影响，因此有了rnn模型结构。</p>
<br/>
<br/>
<h5 id="RNN的结构">RNN的结构</h5>
<p>  如图（注意，此图找不到来源出处，看到网络大部分文章都引用了此图，若有侵权，联系删除）rnn的基础结构就三层：输入层、隐藏层、输出层，：</p>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/blog_idx_139/rnn_struct.jpg" alt="rep_img"/></center>
    </div>
</div>   
<p>  从图中可以知道，W是一个隐藏参数，是作为来至于上一次模型计算值$S_{t-1}$的参数。V是输出的参数，U是输入的参数。那么我们就可以简单定义模型结构是：$S_t = U<em>X_t + W</em>S_{t-1} + b_i$和  $O_t = V*S_t + b_o$</p>
<p>  对于输入层来说，其是一个输入序列，我们输出的内容也是一个序列。</p>
<p>  注意，这里的核心就是$S_t$，前面的输入$X_t$对应一个$S_t$，那么在计算$O_{t+1}$的时候，会用到$S_t$。这样对于这个模型来说，$X_t$对$O_{t+1}$是有影响的，也就意味着，模型可能可以学习到$X_t$和$X_{t+1}$的关系。</p>
<br/>
<br/>
<br/>
<br/>
<h3 id="基于RNN训练一个简单的文字序列输出模型">基于RNN训练一个简单的文字序列输出模型</h3>
<hr>
<p>  </p>
<br/>
<br/>
<h5 id="文本预处理">文本预处理</h5>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> collections
<span class="token comment"># [</span>
<span class="token comment">#     [line0],</span>
<span class="token comment">#     [line1],</span>
<span class="token comment">#     .....</span>
<span class="token comment"># ]</span>
<span class="token keyword">def</span> <span class="token function">read_data_from_txt</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'诛仙 (萧鼎).txt'</span><span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
        lines <span class="token operator">=</span> f<span class="token punctuation">.</span>readlines<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
    <span class="token keyword">return</span> <span class="token punctuation">[</span>line<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> line <span class="token keyword">in</span> lines<span class="token punctuation">]</span>

<span class="token comment"># 下面的tokenize函数将文本行列表（lines）作为输入， 列表中的每个元素是一个文本序列（如一条文本行）。 </span>
<span class="token comment"># 每个文本序列又被拆分成一个词元列表，词元（token）是文本的基本单位。 最后，返回一个由词元列表组成的列表，</span>
<span class="token comment"># 其中的每个词元都是一个字符串（string）。</span>
<span class="token comment"># [</span>
<span class="token comment">#     [line0-char0, line0-char1, line0-char2, ....],</span>
<span class="token comment">#     [line1-char0, line1-char1, line1-char2, ....],</span>
<span class="token comment">#     .....</span>
<span class="token comment"># ]</span>
<span class="token keyword">def</span> <span class="token function">tokenize</span><span class="token punctuation">(</span>lines<span class="token punctuation">,</span> token<span class="token operator">=</span><span class="token string">'char'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""将文本行拆分为单词或字符词元"""</span>
    <span class="token keyword">if</span> token <span class="token operator">==</span> <span class="token string">'word'</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token punctuation">[</span>line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> line <span class="token keyword">in</span> lines<span class="token punctuation">]</span>
    <span class="token keyword">elif</span> token <span class="token operator">==</span> <span class="token string">'char'</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token punctuation">[</span><span class="token builtin">list</span><span class="token punctuation">(</span>line<span class="token punctuation">)</span> <span class="token keyword">for</span> line <span class="token keyword">in</span> lines<span class="token punctuation">]</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'错误：未知词元类型：'</span> <span class="token operator">+</span> token<span class="token punctuation">)</span>


<span class="token comment"># 词元的类型是字符串，而模型需要的输入是数字，因此这种类型不方便模型使用。 现在，让我们构建一个字典，</span>
<span class="token comment"># 通常也叫做词表（vocabulary）， 用来将字符串类型的词元映射到从开始的数字索引中。</span>
<span class="token keyword">def</span> <span class="token function">count_corpus</span><span class="token punctuation">(</span>tokens<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""统计词元的频率"""</span>
    <span class="token comment"># 这里的tokens是1D列表或2D列表</span>
    <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>tokens<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">or</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>tokens<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">list</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 将词元列表展平成一个列表</span>
        tokens <span class="token operator">=</span> <span class="token punctuation">[</span>token <span class="token keyword">for</span> line <span class="token keyword">in</span> tokens <span class="token keyword">for</span> token <span class="token keyword">in</span> line<span class="token punctuation">]</span>
    <span class="token keyword">return</span> collections<span class="token punctuation">.</span>Counter<span class="token punctuation">(</span>tokens<span class="token punctuation">)</span>

<span class="token comment"># 返回类似&#123;'l': 3, 'o': 2, 'h': 1, 'e': 1, ' ': 1, 'w': 1, 'r': 1, 'd': 1&#125;的一个字典</span>
<span class="token keyword">class</span> <span class="token class-name">Vocab</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""文本词表"""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> tokens<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> min_freq<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> reserved_tokens<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> tokens <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            tokens <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">if</span> reserved_tokens <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            reserved_tokens <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token comment"># 按出现频率排序</span>
        <span class="token comment"># 对于Counter("hello world")，结果如下</span>
        <span class="token comment"># Counter(&#123;'l': 3, 'o': 2, 'h': 1, 'e': 1, ' ': 1, 'w': 1, 'r': 1, 'd': 1&#125;)</span>
        counter <span class="token operator">=</span> count_corpus<span class="token punctuation">(</span>tokens<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>_token_freqs <span class="token operator">=</span> <span class="token builtin">sorted</span><span class="token punctuation">(</span>counter<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> key<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                                   reverse<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token comment"># 未知词元的索引为0</span>
        self<span class="token punctuation">.</span>idx_to_token <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'&lt;unk>'</span><span class="token punctuation">]</span> <span class="token operator">+</span> reserved_tokens
        self<span class="token punctuation">.</span>token_to_idx <span class="token operator">=</span> <span class="token punctuation">&#123;</span>token<span class="token punctuation">:</span> idx
                             <span class="token keyword">for</span> idx<span class="token punctuation">,</span> token <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>idx_to_token<span class="token punctuation">)</span><span class="token punctuation">&#125;</span>
        <span class="token keyword">for</span> token<span class="token punctuation">,</span> freq <span class="token keyword">in</span> self<span class="token punctuation">.</span>_token_freqs<span class="token punctuation">:</span>
            <span class="token keyword">if</span> freq <span class="token operator">&lt;</span> min_freq<span class="token punctuation">:</span>
                <span class="token keyword">break</span>
            <span class="token keyword">if</span> token <span class="token keyword">not</span> <span class="token keyword">in</span> self<span class="token punctuation">.</span>token_to_idx<span class="token punctuation">:</span>
                self<span class="token punctuation">.</span>idx_to_token<span class="token punctuation">.</span>append<span class="token punctuation">(</span>token<span class="token punctuation">)</span>
                self<span class="token punctuation">.</span>token_to_idx<span class="token punctuation">[</span>token<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>idx_to_token<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span>

    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>idx_to_token<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> tokens<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token keyword">not</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>tokens<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">,</span> <span class="token builtin">tuple</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">return</span> self<span class="token punctuation">.</span>token_to_idx<span class="token punctuation">.</span>get<span class="token punctuation">(</span>tokens<span class="token punctuation">,</span> self<span class="token punctuation">.</span>unk<span class="token punctuation">)</span>
        <span class="token keyword">return</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>__getitem__<span class="token punctuation">(</span>token<span class="token punctuation">)</span> <span class="token keyword">for</span> token <span class="token keyword">in</span> tokens<span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">to_tokens</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> indices<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token keyword">not</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>indices<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">,</span> <span class="token builtin">tuple</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">return</span> self<span class="token punctuation">.</span>idx_to_token<span class="token punctuation">[</span>indices<span class="token punctuation">]</span>
        <span class="token keyword">return</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>idx_to_token<span class="token punctuation">[</span>index<span class="token punctuation">]</span> <span class="token keyword">for</span> index <span class="token keyword">in</span> indices<span class="token punctuation">]</span>

    <span class="token decorator annotation punctuation">@property</span>
    <span class="token keyword">def</span> <span class="token function">unk</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 未知词元的索引为0</span>
        <span class="token keyword">return</span> <span class="token number">0</span>

    <span class="token decorator annotation punctuation">@property</span>
    <span class="token keyword">def</span> <span class="token function">token_freqs</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>_token_freqs    

<span class="token comment"># 将传入的数据集映射为一个索引表</span>
<span class="token comment"># 返回传入文本的索引、词表</span>
<span class="token keyword">def</span> <span class="token function">load_dataset</span><span class="token punctuation">(</span>max_tokens<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>

    lines <span class="token operator">=</span> read_data_from_txt<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'# 文本总行数: </span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">len</span><span class="token punctuation">(</span>lines<span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
    <span class="token comment"># print(lines[0])</span>
    <span class="token comment"># print(lines[10])</span>

    tokens <span class="token operator">=</span> tokenize<span class="token punctuation">(</span>lines<span class="token punctuation">)</span>
    <span class="token comment"># for i in range(11):</span>
    <span class="token comment">#     print(tokens[i])</span>

    vocab <span class="token operator">=</span> Vocab<span class="token punctuation">(</span>tokens<span class="token punctuation">,</span> reserved_tokens<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'&lt;pad>'</span><span class="token punctuation">,</span> <span class="token string">'&lt;bos>'</span><span class="token punctuation">,</span> <span class="token string">'&lt;eos>'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token comment"># print(list(vocab.token_to_idx.items())[:10])</span>

    <span class="token comment"># for i in [0, 10]:</span>
    <span class="token comment">#     print('文本:', tokens[i])</span>
    <span class="token comment">#     print('索引:', vocab[tokens[i]])</span>


    corpus <span class="token operator">=</span> <span class="token punctuation">[</span>vocab<span class="token punctuation">[</span>token<span class="token punctuation">]</span> <span class="token keyword">for</span> line <span class="token keyword">in</span> tokens <span class="token keyword">for</span> token <span class="token keyword">in</span> line<span class="token punctuation">]</span>
    <span class="token keyword">if</span> max_tokens <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>
        corpus <span class="token operator">=</span> corpus<span class="token punctuation">[</span><span class="token punctuation">:</span>max_tokens<span class="token punctuation">]</span>
    <span class="token keyword">return</span> corpus<span class="token punctuation">,</span> vocab<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>  上面代码做了如下事情：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>首先我们随便找了一部中文小说，然后读取其所有的行，然后得到一个包含所有行的二维列表。</p>
</li>
<li class="lvl-2">
<p>然后我们对每一行进行文字切割，得到了一个二维列表，列表中的每一行又被分割为一个个中文文字，也就得到了一个个token。(特别注意，站在当前的时刻，这里的token和现在主流的大语言模型的token概念是一样的，但是不是一样的实现。)</p>
</li>
<li class="lvl-2">
<p>由于模型不能直接处理文字，我们需要将文字转换为数字，那么直接的做法就是将一个个token编号即可，这个时候我们得到了词表（vocabulary）。</p>
</li>
<li class="lvl-2">
<p>然后我们根据我们得到的词表，对原始数据集进行数字化，得到一个列表，列表中每个元素就是一个个token对应的索引。</p>
</li>
</ul>
<br/>
<br/>
<h5 id="构造数据集及加载器">构造数据集及加载器</h5>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 以num_steps为步长，从随机的起始位置开始，返回</span>
<span class="token comment"># x1=[ [random_offset1:random_offset1 + num_steps], ... , [random_offset_batchsize:random_offset_batchsize + num_steps] ]</span>
<span class="token comment"># y1=[ [random_offset1 + 1:random_offset1 + num_steps + 1], ... , [random_offset_batchsize + 1:random_offset_batchsize + num_steps + 1] ]</span>
<span class="token keyword">def</span> <span class="token function">seq_data_iter_random</span><span class="token punctuation">(</span>corpus<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> num_steps<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""使用随机抽样生成一个小批量子序列"""</span>
    <span class="token comment"># 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1</span>
    corpus <span class="token operator">=</span> corpus<span class="token punctuation">[</span>random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> num_steps <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
    <span class="token comment"># 减去1，是因为我们需要考虑标签</span>
    num_subseqs <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>corpus<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">//</span> num_steps
    <span class="token comment"># 长度为num_steps的子序列的起始索引</span>
    <span class="token comment"># [0, num_steps*1, num_steps*2, num_steps*3, ...]</span>
    initial_indices <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> num_subseqs <span class="token operator">*</span> num_steps<span class="token punctuation">,</span> num_steps<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token comment"># 在随机抽样的迭代过程中，</span>
    <span class="token comment"># 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻</span>
    random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>initial_indices<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">data</span><span class="token punctuation">(</span>pos<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 返回从pos位置开始的长度为num_steps的序列</span>
        <span class="token keyword">return</span> corpus<span class="token punctuation">[</span>pos<span class="token punctuation">:</span> pos <span class="token operator">+</span> num_steps<span class="token punctuation">]</span>

    num_batches <span class="token operator">=</span> num_subseqs <span class="token operator">//</span> batch_size
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> batch_size <span class="token operator">*</span> num_batches<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 在这里，initial_indices包含子序列的随机起始索引</span>
        initial_indices_per_batch <span class="token operator">=</span> initial_indices<span class="token punctuation">[</span>i<span class="token punctuation">:</span> i <span class="token operator">+</span> batch_size<span class="token punctuation">]</span>
        X <span class="token operator">=</span> <span class="token punctuation">[</span>data<span class="token punctuation">(</span>j<span class="token punctuation">)</span> <span class="token keyword">for</span> j <span class="token keyword">in</span> initial_indices_per_batch<span class="token punctuation">]</span>
        Y <span class="token operator">=</span> <span class="token punctuation">[</span>data<span class="token punctuation">(</span>j <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token keyword">for</span> j <span class="token keyword">in</span> initial_indices_per_batch<span class="token punctuation">]</span>
        <span class="token keyword">yield</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>Y<span class="token punctuation">)</span>

<span class="token comment"># 以num_steps为步长，从随机的起始位置开始，返回</span>
<span class="token comment"># x1=[:, random_offset1:random_offset1 + num_steps]</span>
<span class="token comment"># y1=[:, random_offset1 + 1:random_offset1 + num_steps + 1]</span>

<span class="token keyword">def</span> <span class="token function">seq_data_iter_sequential</span><span class="token punctuation">(</span>corpus<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> num_steps<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""使用顺序分区生成一个小批量子序列"""</span>
    <span class="token comment"># 从随机偏移量开始划分序列</span>
    offset <span class="token operator">=</span> random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> num_steps<span class="token punctuation">)</span>
    num_tokens <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>corpus<span class="token punctuation">)</span> <span class="token operator">-</span> offset <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">//</span> batch_size<span class="token punctuation">)</span> <span class="token operator">*</span> batch_size
    <span class="token comment"># 重新根据corpus建立X_corpus, Y_corpus，两者之间差一位。注意X_corpus, Y_corpus的长度是batch_size的整数倍</span>
    Xs <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>corpus<span class="token punctuation">[</span>offset<span class="token punctuation">:</span> offset <span class="token operator">+</span> num_tokens<span class="token punctuation">]</span><span class="token punctuation">)</span>
    Ys <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>corpus<span class="token punctuation">[</span>offset <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">:</span> offset <span class="token operator">+</span> <span class="token number">1</span> <span class="token operator">+</span> num_tokens<span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token comment"># 直接根据batchsize划分X_corpus, Y_corpus</span>
    Xs<span class="token punctuation">,</span> Ys <span class="token operator">=</span> Xs<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> Ys<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token comment"># 计算出需要多少次才能取完数据</span>
    num_batches <span class="token operator">=</span> Xs<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">//</span> num_steps
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> num_steps <span class="token operator">*</span> num_batches<span class="token punctuation">,</span> num_steps<span class="token punctuation">)</span><span class="token punctuation">:</span>
        X <span class="token operator">=</span> Xs<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> i<span class="token punctuation">:</span> i <span class="token operator">+</span> num_steps<span class="token punctuation">]</span>
        Y <span class="token operator">=</span> Ys<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> i<span class="token punctuation">:</span> i <span class="token operator">+</span> num_steps<span class="token punctuation">]</span>
        <span class="token keyword">yield</span> X<span class="token punctuation">,</span> Y


<span class="token keyword">class</span> <span class="token class-name">SeqDataLoader</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""加载序列数据的迭代器"""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> num_steps<span class="token punctuation">,</span> use_random_iter<span class="token punctuation">,</span> max_tokens<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> use_random_iter<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>data_iter_fn <span class="token operator">=</span> seq_data_iter_random
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>data_iter_fn <span class="token operator">=</span> seq_data_iter_sequential
        self<span class="token punctuation">.</span>corpus<span class="token punctuation">,</span> self<span class="token punctuation">.</span>vocab <span class="token operator">=</span> dateset<span class="token punctuation">.</span>load_dataset<span class="token punctuation">(</span>max_tokens<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_steps <span class="token operator">=</span> batch_size<span class="token punctuation">,</span> num_steps

    <span class="token keyword">def</span> <span class="token function">__iter__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>data_iter_fn<span class="token punctuation">(</span>self<span class="token punctuation">.</span>corpus<span class="token punctuation">,</span> self<span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_steps<span class="token punctuation">)</span>
    
<span class="token keyword">def</span> <span class="token function">load_data_epoch</span><span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> num_steps<span class="token punctuation">,</span>  <span class="token comment">#@save</span>
                           use_random_iter<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> max_tokens<span class="token operator">=</span><span class="token number">10000</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""返回时光机器数据集的迭代器和词表"""</span>
    data_iter <span class="token operator">=</span> SeqDataLoader<span class="token punctuation">(</span>
        batch_size<span class="token punctuation">,</span> num_steps<span class="token punctuation">,</span> use_random_iter<span class="token punctuation">,</span> max_tokens<span class="token punctuation">)</span>
    <span class="token keyword">return</span> data_iter<span class="token punctuation">,</span> data_iter<span class="token punctuation">.</span>vocab<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>  上面的代码主要作用是：在训练的时候，从我们在文本预处理数据中，以随机顺序或者相邻顺序抽取其中的部分数据作为随机批量数据。每次抽取的数据维度是：(batch_size, num_steps)</p>
<br/>
<br/>
<h5 id="搭建RNN训练框架">搭建RNN训练框架</h5>
<p>  按照原来的经验，我们要设计一个训练框架，第一步就要搭建网络，此网络用于接收一个输入，输出一个输出。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">rnn</span><span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> state<span class="token punctuation">,</span> params<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># inputs的形状：(时间步数量，批量大小，词表大小)</span>
    <span class="token comment"># inputs的形状：(num_steps，batch_size，词表大小)</span>
    <span class="token comment"># W_xh的形状: (词表大小, num_hiddens)</span>
    <span class="token comment"># W_hh的形状：(num_hiddens, num_hiddens)</span>
    <span class="token comment"># b_h 的形状：(num_hiddens)</span>
    <span class="token comment"># W_hq的形状：(num_hiddens, 词表大小)</span>
    <span class="token comment"># b_q 的形状：(词表大小)</span>
    W_xh<span class="token punctuation">,</span> W_hh<span class="token punctuation">,</span> b_h<span class="token punctuation">,</span> W_hq<span class="token punctuation">,</span> b_q <span class="token operator">=</span> params
    <span class="token comment"># H的形状：（batch_size, num_hiddens）</span>
    H<span class="token punctuation">,</span> <span class="token operator">=</span> state
    outputs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token comment"># X的形状：(批量大小，词表大小)</span>
    <span class="token comment"># X的形状：(batch_size，词表大小)</span>
    <span class="token keyword">for</span> X <span class="token keyword">in</span> inputs<span class="token punctuation">:</span>
        <span class="token comment"># H是上一次预测的一个参数，每次计算隐藏层值后，更新H的值</span>
        <span class="token comment"># H = tanh(X*W_xh + H*W_hh + b_h) </span>
        H <span class="token operator">=</span> torch<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>X<span class="token punctuation">,</span> W_xh<span class="token punctuation">)</span> <span class="token operator">+</span> torch<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>H<span class="token punctuation">,</span> W_hh<span class="token punctuation">)</span> <span class="token operator">+</span> b_h<span class="token punctuation">)</span>
        <span class="token comment"># Y是输出值，每次rnn输出的时候，都会输出从开始到当前的所有值，因此我们需要保存所有的输出值</span>
        <span class="token comment"># Y = H * W_hq + b_q</span>
        <span class="token comment"># Y的形状：(batch_size，词表大小)</span>
        Y <span class="token operator">=</span> torch<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>H<span class="token punctuation">,</span> W_hq<span class="token punctuation">)</span> <span class="token operator">+</span> b_q
        outputs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>Y<span class="token punctuation">)</span>
    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>H<span class="token punctuation">,</span><span class="token punctuation">)</span>

<span class="token keyword">class</span> <span class="token class-name">RNNModelScratch</span><span class="token punctuation">:</span> <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""从零开始实现的循环神经网络模型"""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> device<span class="token punctuation">,</span>
                 get_params<span class="token punctuation">,</span> init_state<span class="token punctuation">,</span> forward_fn<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>vocab_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_hiddens <span class="token operator">=</span> vocab_size<span class="token punctuation">,</span> num_hiddens
        <span class="token comment"># 初始化了隐藏参数 W_xh, W_hh, b_h,  W_hq, b_q</span>
        self<span class="token punctuation">.</span>params <span class="token operator">=</span> get_params<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> device<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>init_state<span class="token punctuation">,</span> self<span class="token punctuation">.</span>forward_fn <span class="token operator">=</span> init_state<span class="token punctuation">,</span> forward_fn

    <span class="token keyword">def</span> <span class="token function">__call__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> state<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># X的形状：(batch_size, num_steps)</span>
        <span class="token comment"># X one_hot之后的形状：(num_steps，batch_size，词表大小)</span>
        X <span class="token operator">=</span> F<span class="token punctuation">.</span>one_hot<span class="token punctuation">(</span>X<span class="token punctuation">.</span>T<span class="token punctuation">,</span> self<span class="token punctuation">.</span>vocab_size<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>forward_fn<span class="token punctuation">(</span>X<span class="token punctuation">,</span> state<span class="token punctuation">,</span> self<span class="token punctuation">.</span>params<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">begin_state</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>init_state<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_hiddens<span class="token punctuation">,</span> device<span class="token punctuation">)</span>

<span class="token comment"># 用框架</span>
<span class="token comment">#@save</span>
<span class="token keyword">class</span> <span class="token class-name">RNNModel</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""循环神经网络模型"""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> rnn_layer<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> device<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>RNNModel<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>rnn <span class="token operator">=</span> rnn_layer
        self<span class="token punctuation">.</span>vocab_size <span class="token operator">=</span> vocab_size
        self<span class="token punctuation">.</span>num_hiddens <span class="token operator">=</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>hidden_size
        <span class="token comment"># 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1</span>
        <span class="token keyword">if</span> <span class="token keyword">not</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>bidirectional<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>num_directions <span class="token operator">=</span> <span class="token number">1</span>
            self<span class="token punctuation">.</span>linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_hiddens<span class="token punctuation">,</span> self<span class="token punctuation">.</span>vocab_size<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>num_directions <span class="token operator">=</span> <span class="token number">2</span>
            self<span class="token punctuation">.</span>linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_hiddens <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>vocab_size<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">,</span> state<span class="token punctuation">)</span><span class="token punctuation">:</span>
        X <span class="token operator">=</span> F<span class="token punctuation">.</span>one_hot<span class="token punctuation">(</span>inputs<span class="token punctuation">.</span>T<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>vocab_size<span class="token punctuation">)</span>
        X <span class="token operator">=</span> X<span class="token punctuation">.</span>to<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
        Y<span class="token punctuation">,</span> state <span class="token operator">=</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">(</span>X<span class="token punctuation">,</span> state<span class="token punctuation">)</span>
        <span class="token comment"># 全连接层首先将Y的形状改为(时间步数*批量大小,隐藏单元数)</span>
        <span class="token comment"># 它的输出形状是(时间步数*批量大小,词表大小)。</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>Y<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> Y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> output<span class="token punctuation">,</span> state

    <span class="token keyword">def</span> <span class="token function">begin_state</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> device<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token keyword">not</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>rnn<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>LSTM<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># nn.GRU以张量作为隐状态</span>
            <span class="token keyword">return</span>  torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_directions <span class="token operator">*</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>num_layers<span class="token punctuation">,</span>
                                 batch_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_hiddens<span class="token punctuation">)</span><span class="token punctuation">,</span>
                                device<span class="token operator">=</span>device<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token comment"># nn.LSTM以元组作为隐状态</span>
            <span class="token keyword">return</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>
                self<span class="token punctuation">.</span>num_directions <span class="token operator">*</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>num_layers<span class="token punctuation">,</span>
                batch_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_hiddens<span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span>
                    torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>
                        self<span class="token punctuation">.</span>num_directions <span class="token operator">*</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>num_layers<span class="token punctuation">,</span>
                        batch_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_hiddens<span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>  上面主要是设计了两个网络类：RNNModelScratch、RNNModel。前者是手搓rnn实现。后者是借用torch框架来实现一个简单的rnn网络。他们的主要做了如下几个事情：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>接收(batch_size, num_steps)的输入，并将输入转换为one_hot向量模式，其shape是(num_steps，batch_size，词表大小)</p>
</li>
<li class="lvl-2">
<p>通过rnn的计算，然后通过变换，将最终输出映射到（batch_size * num_steps， 词表大小）</p>
</li>
</ul>
<p>  其实我们观察输入和输出，就可以理解一个事情：输入的内容就是输入序列所有的字符对应的one_hot向量。输出的内容就是batch_size * num_steps个向量，代表输出的文字序列信息，每个向量里面的最大值就代表了网络预测的文字id。</p>
<p>  有了网络，对于部署角度来说，我们只需要实现预测过程即可：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">predict_ch8</span><span class="token punctuation">(</span>prefix<span class="token punctuation">,</span> num_preds<span class="token punctuation">,</span> net<span class="token punctuation">,</span> vocab<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""在prefix后面生成新字符"""</span>
    state <span class="token operator">=</span> net<span class="token punctuation">.</span>begin_state<span class="token punctuation">(</span>batch_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>
    outputs <span class="token operator">=</span> <span class="token punctuation">[</span>vocab<span class="token punctuation">[</span>prefix<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
    get_input <span class="token operator">=</span> <span class="token keyword">lambda</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>outputs<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> y <span class="token keyword">in</span> prefix<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">:</span>  <span class="token comment"># 预热期</span>
        _<span class="token punctuation">,</span> state <span class="token operator">=</span> net<span class="token punctuation">(</span>get_input<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> state<span class="token punctuation">)</span>
        outputs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>vocab<span class="token punctuation">[</span>y<span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_preds<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 预测num_preds步</span>
        <span class="token comment"># y 包含从开始到现在的所有输出</span>
        <span class="token comment"># state是当前计算出来的隐藏参数</span>
        y<span class="token punctuation">,</span> state <span class="token operator">=</span> net<span class="token punctuation">(</span>get_input<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> state<span class="token punctuation">)</span>
        outputs<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">(</span>y<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> <span class="token string">''</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">[</span>vocab<span class="token punctuation">.</span>idx_to_token<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> outputs<span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>  由于输出的信息就是batch_size * num_steps个向量，那么只需要计算每一个向量的最大值id就得到了网络输出的tokenid，然后通过词表反向映射回词表，完成了预测文字输出的功能。</p>
<p>  有了网络、预测过程，然后就可以搭建训练过程，训练过程最重要的一步就是通过网络得到输入对应的输出，然后根据输出计算loss信息，然后根据loss信息进行梯度下降（这就是通用流程）</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">train_epoch_ch8</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> loss<span class="token punctuation">,</span> updater<span class="token punctuation">,</span> device<span class="token punctuation">,</span> use_random_iter<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""训练网络一个迭代周期（定义见第8章）"""</span>
    state<span class="token punctuation">,</span> timer <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span> Timer<span class="token punctuation">(</span><span class="token punctuation">)</span>
    metric <span class="token operator">=</span> Accumulator<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># 训练损失之和,词元数量</span>
    <span class="token comment"># X的形状：(batch_size, num_steps)</span>
    <span class="token comment"># Y的形状：(batch_size, num_steps)</span>
    <span class="token keyword">for</span> X<span class="token punctuation">,</span> Y <span class="token keyword">in</span> train_iter<span class="token punctuation">:</span>
        <span class="token keyword">if</span> state <span class="token keyword">is</span> <span class="token boolean">None</span> <span class="token keyword">or</span> use_random_iter<span class="token punctuation">:</span>
            <span class="token comment"># 在第一次迭代或使用随机抽样时初始化state</span>
            state <span class="token operator">=</span> net<span class="token punctuation">.</span>begin_state<span class="token punctuation">(</span>batch_size<span class="token operator">=</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span> <span class="token keyword">and</span> <span class="token keyword">not</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>state<span class="token punctuation">,</span> <span class="token builtin">tuple</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                <span class="token comment"># state对于nn.GRU是个张量</span>
                state<span class="token punctuation">.</span>detach_<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                <span class="token comment"># state对于nn.LSTM或对于我们从零开始实现的模型是个张量</span>
                <span class="token keyword">for</span> s <span class="token keyword">in</span> state<span class="token punctuation">:</span>
                    s<span class="token punctuation">.</span>detach_<span class="token punctuation">(</span><span class="token punctuation">)</span>
        y <span class="token operator">=</span> Y<span class="token punctuation">.</span>T<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        X<span class="token punctuation">,</span> y <span class="token operator">=</span> X<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
        <span class="token comment"># y_hat 包含从开始到现在的所有输出</span>
        <span class="token comment"># y_hat的形状：（batch_size * num_steps， 词表大小）</span>
        <span class="token comment"># state是当前计算出来的隐藏参数</span>
        y_hat<span class="token punctuation">,</span> state <span class="token operator">=</span> net<span class="token punctuation">(</span>X<span class="token punctuation">,</span> state<span class="token punctuation">)</span>
        <span class="token comment"># 交叉熵损失函数，传入预测值和标签值，并求平均值</span>
        l <span class="token operator">=</span> loss<span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span> y<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>updater<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Optimizer<span class="token punctuation">)</span><span class="token punctuation">:</span>
            updater<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
            l<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
            grad_clipping<span class="token punctuation">(</span>net<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
            updater<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            l<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
            grad_clipping<span class="token punctuation">(</span>net<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token comment"># 因为已经调用了mean函数</span>
            updater<span class="token punctuation">(</span>batch_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># 这里记录交叉熵损失的值的和，以及记录对应交叉熵损失值的样本个数</span>
        metric<span class="token punctuation">.</span>add<span class="token punctuation">(</span>l <span class="token operator">*</span> y<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token comment"># 求交叉熵损失的平均值，再求exp，即可得到困惑度</span>
    <span class="token keyword">return</span> math<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>metric<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">/</span> metric<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> metric<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">/</span> timer<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span>


<span class="token keyword">def</span> <span class="token function">sgd</span><span class="token punctuation">(</span>params<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""小批量随机梯度下降

    Defined in :numref:`sec_linear_scratch`"""</span>
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> param <span class="token keyword">in</span> params<span class="token punctuation">:</span>
            param <span class="token operator">-=</span> lr <span class="token operator">*</span> param<span class="token punctuation">.</span>grad <span class="token operator">/</span> batch_size
            param<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment">#@save</span>
<span class="token keyword">def</span> <span class="token function">train_ch8</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> vocab<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> device<span class="token punctuation">,</span>
              use_random_iter<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""训练模型（定义见第8章）"""</span>
    loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># 新建一个连接客户端</span>
    <span class="token comment"># 指定 env=u'test1'，默认端口为 8097，host 是 'localhost'</span>
    vis <span class="token operator">=</span> visdom<span class="token punctuation">.</span>Visdom<span class="token punctuation">(</span>env<span class="token operator">=</span><span class="token string">u'test1'</span><span class="token punctuation">,</span> server<span class="token operator">=</span><span class="token string">"http://127.0.0.1"</span><span class="token punctuation">,</span> port<span class="token operator">=</span><span class="token number">8097</span><span class="token punctuation">)</span>
    animator <span class="token operator">=</span> vis
    <span class="token comment"># 初始化</span>
    <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
        updater <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        updater <span class="token operator">=</span> <span class="token keyword">lambda</span> batch_size<span class="token punctuation">:</span> sgd<span class="token punctuation">(</span>net<span class="token punctuation">.</span>params<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>
    predict <span class="token operator">=</span> <span class="token keyword">lambda</span> prefix<span class="token punctuation">:</span> predict_ch8<span class="token punctuation">(</span>prefix<span class="token punctuation">,</span> <span class="token number">30</span><span class="token punctuation">,</span> net<span class="token punctuation">,</span> vocab<span class="token punctuation">,</span> device<span class="token punctuation">)</span>
    <span class="token comment"># 训练和预测</span>
    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        ppl<span class="token punctuation">,</span> speed <span class="token operator">=</span> train_epoch_ch8<span class="token punctuation">(</span>
            net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> loss<span class="token punctuation">,</span> updater<span class="token punctuation">,</span> device<span class="token punctuation">,</span> use_random_iter<span class="token punctuation">)</span>
        


        <span class="token keyword">if</span> <span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">10</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            <span class="token comment"># print(predict('你是？'))</span>
            <span class="token comment"># print(epoch)</span>
            <span class="token comment"># animator.add(epoch + 1, )</span>

            <span class="token keyword">if</span> epoch <span class="token operator">==</span> <span class="token number">9</span><span class="token punctuation">:</span>
                <span class="token comment"># 清空图表：使用空数组来替换现有内容</span>
                vis<span class="token punctuation">.</span>line<span class="token punctuation">(</span>X<span class="token operator">=</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> Y<span class="token operator">=</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> win<span class="token operator">=</span><span class="token string">'train_ch8'</span><span class="token punctuation">,</span> update<span class="token operator">=</span><span class="token string">'replace'</span><span class="token punctuation">)</span>

            vis<span class="token punctuation">.</span>line<span class="token punctuation">(</span>
                X<span class="token operator">=</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                Y<span class="token operator">=</span><span class="token punctuation">[</span>ppl<span class="token punctuation">]</span><span class="token punctuation">,</span>
                win<span class="token operator">=</span><span class="token string">'train_ch8'</span><span class="token punctuation">,</span>
                update<span class="token operator">=</span><span class="token string">'append'</span><span class="token punctuation">,</span>
                opts<span class="token operator">=</span><span class="token punctuation">&#123;</span>
                    <span class="token string">'title'</span><span class="token punctuation">:</span> <span class="token string">'train_ch8'</span><span class="token punctuation">,</span>
                    <span class="token string">'xlabel'</span><span class="token punctuation">:</span> <span class="token string">'epoch'</span><span class="token punctuation">,</span>
                    <span class="token string">'ylabel'</span><span class="token punctuation">:</span> <span class="token string">'ppl'</span><span class="token punctuation">,</span>
                    <span class="token string">'linecolor'</span><span class="token punctuation">:</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">255</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment"># 蓝色线条</span>
                <span class="token punctuation">&#125;</span>
            <span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'困惑度 </span><span class="token interpolation"><span class="token punctuation">&#123;</span>ppl<span class="token punctuation">:</span><span class="token format-spec">.1f</span><span class="token punctuation">&#125;</span></span><span class="token string">, </span><span class="token interpolation"><span class="token punctuation">&#123;</span>speed<span class="token punctuation">:</span><span class="token format-spec">.1f</span><span class="token punctuation">&#125;</span></span><span class="token string"> 词元/秒 </span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">str</span><span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>predict<span class="token punctuation">(</span><span class="token string">'你是'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>predict<span class="token punctuation">(</span><span class="token string">'我有一剑'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>  其实从上面的代码就可以看到，我们传入数据，得到输出，计算了交叉熵loss，然后使用sgd最小化loss，最终我们计算困惑度，得到了模型的质量。注意，这里面有关于梯度截断的计算，这个我们只需要它是避免梯度爆炸的一个方法即可。</p>
<p>  然后我们使用如下的代码就可以开始训练，注意使用net就是自定义rnn，net1就是使用框架的rnn。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">try_gpu</span><span class="token punctuation">(</span>i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""如果存在，则返回gpu(i)，否则返回cpu()

    Defined in :numref:`sec_use_gpu`"""</span>
    <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>device_count<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">>=</span> i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'cuda:</span><span class="token interpolation"><span class="token punctuation">&#123;</span>i<span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cpu'</span><span class="token punctuation">)</span>

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    num_epochs<span class="token punctuation">,</span> lr <span class="token operator">=</span> <span class="token number">1000</span><span class="token punctuation">,</span> <span class="token number">0.5</span>
    batch_size<span class="token punctuation">,</span> num_steps <span class="token operator">=</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">35</span>
    data_iter<span class="token punctuation">,</span> vocab  <span class="token operator">=</span> load_data_epoch<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> num_steps<span class="token punctuation">)</span>
    num_hiddens <span class="token operator">=</span> <span class="token number">512</span>
    device <span class="token operator">=</span> try_gpu<span class="token punctuation">(</span><span class="token punctuation">)</span>
    net <span class="token operator">=</span> RNNModelScratch<span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>vocab<span class="token punctuation">)</span><span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> device<span class="token punctuation">,</span> get_params<span class="token punctuation">,</span>
                        init_rnn_state<span class="token punctuation">,</span> rnn<span class="token punctuation">)</span>
    
    rnn_layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>RNN<span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>vocab<span class="token punctuation">)</span><span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>
    net1 <span class="token operator">=</span> RNNModel<span class="token punctuation">(</span>rnn_layer<span class="token punctuation">,</span> vocab_size<span class="token operator">=</span><span class="token builtin">len</span><span class="token punctuation">(</span>vocab<span class="token punctuation">)</span><span class="token punctuation">,</span>  device<span class="token operator">=</span>device<span class="token punctuation">)</span>
    
    <span class="token keyword">print</span><span class="token punctuation">(</span>predict_ch8<span class="token punctuation">(</span><span class="token string">'你是'</span><span class="token punctuation">,</span> <span class="token number">30</span><span class="token punctuation">,</span> net<span class="token punctuation">,</span> vocab<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span class="token punctuation">)</span>

    train_ch8<span class="token punctuation">(</span>net<span class="token punctuation">,</span> data_iter<span class="token punctuation">,</span> vocab<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>  我们分别使用手动构建的rnn和框架构建的rnn进行训练和测试，结果如下：</p>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/blog_idx_139/scratch_train.png" alt="rep_img"/></center>
    </div>
</div>   
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/blog_idx_139/scratch_ret.png" alt="rep_img"/></center>
    </div>
</div>   
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/blog_idx_139/torch_train.png" alt="rep_img"/></center>
    </div>
</div>   
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/blog_idx_139/torch_ret.png" alt="rep_img"/></center>
    </div>
</div>   
<p>  我们可以看到，模型未训练和训练后的对比，明显训练后能说两句人话，虽然感觉还是胡说八道，但是感觉还是有点效果。</p>
<br/>
<br/>
<br/>
<br/>
<h3 id="后记">后记</h3>
<hr>
<p>  总的来说，未训练的模型和已训练的模型的文字续写效果完全不一样，明显感觉训练之后的模型，文字续写给人一种可以读感觉。</p>
<h3 id="参考文献">参考文献</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/30844905">https://zhuanlan.zhihu.com/p/30844905</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://zh.d2l.ai/chapter_recurrent-neural-networks/rnn.html">https://zh.d2l.ai/chapter_recurrent-neural-networks/rnn.html</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://zh.d2l.ai/chapter_recurrent-neural-networks/text-preprocessing.html">https://zh.d2l.ai/chapter_recurrent-neural-networks/text-preprocessing.html</a></p>
</li>
</ul>
<br/>
<br/>
<div style="margin:50px auto;">
</div>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <hr/>
        <center><font color = #91e0b0 size = 5>打赏、订阅、收藏、丢香蕉、硬币，请关注公众号（攻城狮的搬砖之路）</font></center>
    </div>
</div>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/qrcode_for_wx_official_account.jpg" alt="qrc_img"/></center>
    </div>
</div>
<!-- ![alt 公众号图片](https://flyinskyin2013.github.io/ImageBed0/blogs/qrcode_for_wx_official_account.jpg "公众号图片") -->
<p><font color="red" size="7">PS: 请尊重原创，不喜勿喷。</font><br/><br>
<font color="red" size="7">PS: 要转载请注明出处，本人版权所有。</font><br/><br>
<font color="red" size="7">PS: 有问题请留言，看到后我会第一时间回复。</font><br/></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://e-x.top/2025/03/16/blog_idx_138/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Sky">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sky's Blogs">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | Sky's Blogs">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/03/16/blog_idx_138/" class="post-title-link" itemprop="url">大模型基础补全计划(二)---词嵌入(word embedding)</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-03-16 19:16:27 / 修改时间：19:20:49" itemprop="dateCreated datePublished" datetime="2025-03-16T19:16:27+08:00">2025-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/LLM/" itemprop="url" rel="index"><span itemprop="name">LLM</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/LLM/LM/" itemprop="url" rel="index"><span itemprop="name">LM</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.8k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>7 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <script src="\assets\js\APlayer.min.js"> </script><!--
 * @Description: 
 * @Author: Sky
 * @Date: 2020-08-24 16:37:34
 * @LastEditors: Sky
 * @LastEditTime: 2021-06-29 15:04:48
 * @Github: https://github.com/flyinskyin2013/
-->
<p><font color="red" size="7">PS：要转载请注明出处，本人版权所有。</font><br/><br>
<font color="red" size="7">PS: 这个只是基于《我自己》的理解，</font><br/><font color="red" size="7">如果和你的原则及想法相冲突，请谅解，勿喷。</font><br/></p>
<!-- ###### 前置说明
&emsp;&emsp;本文作为本人github blog/cnblog的主站的备份。（BlogID=138） 
&emsp;&emsp;本文发布于 2025-03-16 19:16:27             （BlogID=138） 
-->
<h6 id="环境说明">环境说明</h6>
<p>  无</p>
<h3 id="前言">前言</h3>
<hr>
<p>  本文是这个系列第二篇，它们是：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>《大模型基础补全计划(一)—重温一些深度学习相关的数学知识》<a target="_blank" rel="noopener" href="https://www.cnblogs.com/Iflyinsky/p/18717317">https://www.cnblogs.com/Iflyinsky/p/18717317</a></p>
</li>
</ul>
<p>  本系列虽然是大模型相关内容，但是主要还是大语言模型，对于大语言模型来说，其输入是我们值得关心的。</p>
<p>  自然语言的基本单位是词或者字，对于模型来说，是没有办法直接输入文字的，因此我们需要一种方法将文字转换为LLM所能接受的格式，我们将词转换为向量的表达这种技术叫做词嵌入（word embedding）。</p>
<p>  下面我们介绍一种在后续例子中会出现的一种直观词嵌入方法：one_hot向量。</p>
<br/>
<br/>
<br/>
<br/>
<h3 id="one-hot向量">one_hot向量</h3>
<hr>
<p>  其含义非常的简单，如果有5个不同的词，那么生成5个0，1的向量即可，例如：$[1,0,0,0,0]$或者$[0,0,0,0,1]$，通过这样简单的向量，就可以标识所有的字。</p>
<p>  虽然其看起来简单，但是有些缺陷，例如：现在我们来考虑一个问题，我们用one_hot向量A表示‘似’，用one_hot向量B表示‘像’，然后我们求其余弦相似度 $\cos{\theta} = \frac{A^TB}{|A||B|} = 0$ ，难道现在我们可以说‘似’和‘像’是无关联的吗？</p>
<p>  根据上面的这个疑问，很明显要解决这个问题，需要把词映射为像人脸识别中的人脸特征向量这样的特征向量。下面我们介绍word2vec这个工具，注意词向量的表达有很多越来越好的方法，这里我们只需要了解一个基本即可。</p>
<br/>
<br/>
<br/>
<br/>
<h3 id="word2vec">word2vec</h3>
<hr>
<p>  word2vec工具可以将每个词映射为固定长度的向量，这些向量能够表达词的相似性关系。如果大家做过人脸识别，那就对这个相似性概念一点也不陌生。</p>
<p>  word2vec工具包含了两个模型：跳元模型（skip-gram），连续词袋（CBOW）。这些模型的训练依赖于条件概率，且由于是不带标签的数据，他们是自监督模型。</p>
<p>  下面我们只简单分析一个简单的：跳元模型（skip-gram）。</p>
<br/>
<br/>
<h5 id="跳元模型（skip-gram）">跳元模型（skip-gram）</h5>
<p>  跳元模型假设一个词可以用来在文本序列中生成其周围的单词。我们以文本序列：“自太古以来”为例，给定中心词“古”，给定上下文窗口是2，跳元模型考虑生成上下文词“自”，“太”，“以”，“来”的条件概率是：$P(“自”，“太”，“以”，“来”|“古”) = P(“自”|“古”)*P(“太”|“古”)*P(“以”|“古”)*P(“来”|“古”)$。</p>
<p>  在跳元模型中，对于词表V中索引为i的的词$w_i$，其有两个向量$v_i$和$u_i$，他们分别表示为$w_i$做为中心词、上下文词时的向量。此时我们给定中心词$w_c$，生成上下文词$w_o$的条件概率可以使用u,v向量的点积和softmax来建模：$P(w_o|w_c) = \frac{exp(u_o^T v_c)}{\sum_{i\in{V}}  exp(u_i^T v_c)}$</p>
<p>  现在我们给定词表V，时间步t处的词表示为$w_t$，给定上下文窗口是m，跳元模型的似然函数是在给定任何中心词的情况下生成所有上下文词的概率：$\prod\limits_{t-1}^{T} \prod\limits_{-m\le j \le m, j \ne 0} P(w_{t+j}|w_{t})$</p>
<p>  然后我们就通过最大化似然函数来学习模型参数，相当于最小化负对数似然函数，然后得到损失函数是：$-\sum\limits_{t-1}^{T} \sum\limits_{-m\le j \le m, j \ne 0} log(P(w_{t+j}|w_{t}))$。</p>
<p>  最后当我们使用随机梯度下降来最小化损失时，我们选取一个短的序列来计算该序列的梯度。注意，我们的模型定义为：$P(w_o|w_c) = \frac{exp(u_o^T v_c)}{\sum_{i\in{V}}  exp(u_i^T v_c)}$，我们给定$v_c$，求$v_o$的梯度。</p>
<p>  为了方便计算，我们对模型取对数，可得到:$log(P(w_o|w_c)) = u_o^T v_c - log(\sum_{i\in{V}}  exp(u_i^T v_c))$</p>
<p>  然后我们求相对于$v_c$的微分，可以得到：$\frac{\partial}{\partial v_c}(log(P(w_o|w_c))) = \frac{\partial}{\partial v_c}(u_o^T v_c) - \frac{\partial}{\partial v_c}(log(\sum_{i\in{V}}  exp(u_i^T v_c)))  = u_o -  \frac{1}{\sum_{i\in{V}}  exp(u_i^T v_c)}  * \frac{\partial}{\partial v_c}(\sum_{i\in{V}}  exp(u_i^T v_c)) = u_o -  \frac{1}{\sum_{i\in{V}}  exp(u_i^T v_c)}  * (\sum_{j\in{V}}  exp(u_j^T v_c)*u_j) = u_o - \sum\limits_{j\in{V}}(\frac{exp(u_j^T v_c)*u_j}{\sum_{i\in{V}}  exp(u_i^T v_c)}) = u_o - \sum\limits_{j\in{V}}(\frac{exp(u_j^T v_c)}{\sum_{i\in{V}}  exp(u_i^T v_c)})*u_j = u_o - \sum\limits_{j\in{V}} P(w_j|w_c)u_j$</p>
<p>  注意中间关于对数的求导计算：令$f(v_c) = \sum_{i\in{V}}  exp(u_i^T v_c)$，可以通过<br>
$\frac{\partial}{\partial v_c}(log f(v_c)) = u_o -  \frac{1}{f(v_c)}  * \frac{\partial}{\partial v_c}(f(v_c))$得到上面的结果。</p>
<br/>
<br/>
<br/>
<br/>
<h3 id="Tokenizer">Tokenizer</h3>
<hr>
<p>  注意，在这些把词生成向量的过程，我们上文已经提到了。但是这里忽略了一个大问题，就是我们默认将语句拆分了词或者字。</p>
<p>  因此在做向量化之前，有一个关键的动作是分词，从2025/03现在来看，分词的主要作用是将字转换（浓缩为）为token id。现在简单理解就是：tokenid可能是一个字、词或者零点几个字、词。</p>
<p>  以后有机会再挖这个的坑吧，现在先简单这样理解。</p>
<br/>
<br/>
<br/>
<br/>
<h3 id="后记">后记</h3>
<hr>
<p>  虽然现在有更加先进的模型代替了这些基础的模型，但是对于我们初学者来说，可以通过这样的一个简单的模型来知道词嵌入过程做了什么是非常有意义的。<br>
  此外从上面的过程我们可以知道，我们在用大语言模型时，需要做预处理文字，非常的像使用CV模型前，对图像进行预处理。而这个预处理过程就是：分词+向量化。</p>
<h3 id="参考文献">参考文献</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="http://zh.gluon.ai/">http://zh.gluon.ai/</a></p>
</li>
</ul>
<br/>
<br/>
<div style="margin:50px auto;">
</div>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <hr/>
        <center><font color = #91e0b0 size = 5>打赏、订阅、收藏、丢香蕉、硬币，请关注公众号（攻城狮的搬砖之路）</font></center>
    </div>
</div>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/qrcode_for_wx_official_account.jpg" alt="qrc_img"/></center>
    </div>
</div>
<!-- ![alt 公众号图片](https://flyinskyin2013.github.io/ImageBed0/blogs/qrcode_for_wx_official_account.jpg "公众号图片") -->
<p><font color="red" size="7">PS: 请尊重原创，不喜勿喷。</font><br/><br>
<font color="red" size="7">PS: 要转载请注明出处，本人版权所有。</font><br/><br>
<font color="red" size="7">PS: 有问题请留言，看到后我会第一时间回复。</font><br/></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://e-x.top/2025/02/15/blog_idx_137/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Sky">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sky's Blogs">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | Sky's Blogs">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/02/15/blog_idx_137/" class="post-title-link" itemprop="url">大模型基础补全计划(一)---重温一些深度学习相关的数学知识</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-02-15 19:50:00 / 修改时间：19:54:55" itemprop="dateCreated datePublished" datetime="2025-02-15T19:50:00+08:00">2025-02-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/LLM/" itemprop="url" rel="index"><span itemprop="name">LLM</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/LLM/LM/" itemprop="url" rel="index"><span itemprop="name">LM</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>13 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <script src="\assets\js\APlayer.min.js"> </script><!--
 * @Description: 
 * @Author: Sky
 * @Date: 2020-08-24 16:37:34
 * @LastEditors: Sky
 * @LastEditTime: 2021-06-29 15:04:48
 * @Github: https://github.com/flyinskyin2013/
-->
<p><font color="red" size="7">PS：要转载请注明出处，本人版权所有。</font><br/><br>
<font color="red" size="7">PS: 这个只是基于《我自己》的理解，</font><br/><font color="red" size="7">如果和你的原则及想法相冲突，请谅解，勿喷。</font><br/></p>
<!-- ###### 前置说明
&emsp;&emsp;本文作为本人github blog/cnblog的主站的备份。（BlogID=137） 
&emsp;&emsp;本文发布于 2025-02-15 19:50:00             （BlogID=137） 
-->
<h6 id="环境说明">环境说明</h6>
<p>  无</p>
<h3 id="前言">前言</h3>
<hr>
<p>  遥记在2021年左右，我写了一系列关于深度学习视觉方向基础学习的文章，它们如下：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>DL基础补全计划(一)—线性回归及示例（Pytorch，平方损失） <a target="_blank" rel="noopener" href="https://githubio.e-x.top/2021/07/04/blog_idx_105/">https://githubio.e-x.top/2021/07/04/blog_idx_105/</a></p>
</li>
<li class="lvl-2">
<p>DL基础补全计划(二)—Softmax回归及示例（Pytorch，交叉熵损失） <a target="_blank" rel="noopener" href="https://githubio.e-x.top/2021/07/11/blog_idx_106/">https://githubio.e-x.top/2021/07/11/blog_idx_106/</a></p>
</li>
<li class="lvl-2">
<p>DL基础补全计划(三)—模型选择、欠拟合、过拟合 <a target="_blank" rel="noopener" href="https://githubio.e-x.top/2021/07/18/blog_idx_107/">https://githubio.e-x.top/2021/07/18/blog_idx_107/</a></p>
</li>
<li class="lvl-2">
<p>DL基础补全计划(四)—对抗过拟合：权重衰减、Dropout <a target="_blank" rel="noopener" href="https://githubio.e-x.top/2021/08/01/blog_idx_108/">https://githubio.e-x.top/2021/08/01/blog_idx_108/</a></p>
</li>
<li class="lvl-2">
<p>DL基础补全计划(五)—数值稳定性及参数初始化（梯度消失、梯度爆炸） <a target="_blank" rel="noopener" href="https://githubio.e-x.top/2021/08/08/blog_idx_109/">https://githubio.e-x.top/2021/08/08/blog_idx_109/</a></p>
</li>
<li class="lvl-2">
<p>DL基础补全计划(六)—卷积和池化 <a target="_blank" rel="noopener" href="https://githubio.e-x.top/2021/08/15/blog_idx_110/">https://githubio.e-x.top/2021/08/15/blog_idx_110/</a></p>
</li>
</ul>
<p>  那时候的我，还一心沉醉在视觉算法模型落地到侧端的各个场景，虽然对NLP有所了解，但是当时还未料想到，在后面的几年，由大语言模型引爆的大模型领域是如此的火爆。到了2023左右，开始逐渐的接触大模型，逐渐的将其应用到自己的工作中，逐渐在工作中将大模型迁移到侧端。从2024年开始，意识到如果要在以后将大模型应用的更好，急需要补充一些大模型及NLP相关的知识才能更好的理解它。因此有了从本文开始的一系列文章。</p>
<p>  从本文开始，预计从数学知识开始，到transformer及LLM结束（挖坑），挑选一些内容来学习记录。</p>
<br/>
<br/>
<br/>
<br/>
<h3 id="概率论、数理统计">概率论、数理统计</h3>
<hr>
<p>  </p>
<br/>
<br/>
<h5 id="基本概念">基本概念</h5>
<p>  在统计学中，把需要调查或者研究的某一现象或者事物的全部数据称为统计总体（简称 总体population)，其所属的数据分布称为 总体分布 (population distribution)， 单个数据称为个体(individual)。我们从统计总体中抽取样本的过程称为抽样（sampling），一次抽样的结果称为一份样本(sample)，一份样本中包含的个体数据的数量称为本容量(sample size)。</p>
<p>  可以把分布（distribution）看作对事件的概率分配，P(X)表示为随机变量X上的分布（distribution）, 分布告诉我们X获得某一值的概率</p>
<p>  概率（probability）在给定的样本空间中，A事件的发生的可信度， 表示为P(A)</p>
<p>  推断统计学（或称统计推断，英语：statistical inference）， 指统计学中，研究如何根据样本(sample)数据去推断总体(population)特征（或者参数）的方法， 比如根据样本的平均值去估计总体的均值参数。 它是在对样本数据进行描述的基础上，对统计总体的未知数量特征做出以概率形式表述的推断。</p>
<p>  通常我们会假设总体分布服从某种已知的概率分布，但是分布的某些参数是不确定的， 比如全国身高数据服从正态分布，但是期望和方差不知道， 这时我们期望能通过样本推断（估计）出总体正态分布的期望和方差参数。</p>
<p>  概率（probability）和统计（statistics）看似两个相近的概念，其实研究的问题刚好相反。</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>概率研究的是，已经知道了模型和参数后，给出一个事件发生的概率。θ是已知确定的，x是变量，这个函数叫做概率函数(probability function)，它描述对于不同的样本点x，其出现概率是多少（表示不同x出现的概率）。概率函数记作$P(X=x_i|\theta)$。</p>
</li>
<li class="lvl-2">
<p>统计是根据给出的观测数据，利用这些数据进行建模和参数的预测。x是已知确定的，θ是变量，这个函数叫做似然函数(likelihood function),  它描述对于不同的模型参数θ，出现x这个样本点的概率是多少(表示不同θ下，x出现的概率）。此时的函数也记作$L(\theta|X=x_i)$。</p>
</li>
</ul>
<br/>
<br/>
<h5 id="联合概率">联合概率</h5>
<p>  给定任意值a和b，联合概率可以回答：A=a和B=b同时满足的概率是多少，其表示为 $P(A=a,B=b)=P(A=a)*P(B=b)$</p>
<br/>
<br/>
<h5 id="条件概率">条件概率</h5>
<p>  联合概率的不等式带给我们一个有趣的比率：$0&lt;= \frac{P(A=a,B=b)}{P(A=a)} &lt;=1$，表示为$P(B=b|A=a)$, 代表在A=a已发生的情况下，B=b的概率。也就是 $P(B=b|A=a) = \frac{P(A=a,B=b)}{P(A=a)} $。</p>
<br/>
<br/>
<h5 id="贝叶斯定理">贝叶斯定理</h5>
<p>  因为条件概率公式：$P(B=b|A=a) = \frac{P(A=a,B=b)}{P(A=a)} ，P(A=a|B=b) = \frac{P(A=a,B=b)}{P(B=b)} $。然后我们观察两个条件概率表达式,可看到有公有的联合概率部分，改变方程即可得到：$P(A,B) = P(A|B) * P(B) = P(B|A) * P(A)$ ，这个就是贝叶斯公式。</p>
<p>此外此定理在深度学习中有一些特殊的解释：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>P(A|B)：在事件 B 已经发生的情况下，事件 A 发生的后验概率 。</p>
</li>
<li class="lvl-2">
<p>P(B|A)：在事件 A 已经发生的情况下，事件 B 发生的似然 。</p>
</li>
<li class="lvl-2">
<p>P(A)：事件 A 的先验概率（在没有额外信息时对 A 的概率估计）。</p>
</li>
<li class="lvl-2">
<p>P(B)：事件 B 的边缘概率（即 B 发生的总概率）。</p>
</li>
</ul>
<p>首先我们定义一个深度学习的模型为：$P(\theta,D) = \frac{P(D|\theta) * P(\theta)}{P(D)}$, 其参数为 θ，训练数据为 D。我们的目标是根据数据 D 来更新对参数 θ 的估计,这里我们来看一个例子：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>$P(\theta | D) $：在观察到数据 D 后，参数 $\theta $ 的后验分布 。</p>
</li>
<li class="lvl-2">
<p>$P(\theta) $：参数 $\theta $ 的先验分布 （在没有看到数据之前对参数的假设）。</p>
</li>
<li class="lvl-2">
<p>$P(D | \theta) $：在给定参数 $\theta$ 的情况下，生成数据 D  的似然函数 。</p>
</li>
<li class="lvl-2">
<p>$P(D) $：数据 D 的边缘概率 （归一化常数）</p>
</li>
</ul>
<br/>
<br/>
<h5 id="边际化">边际化</h5>
<p>  B的概率相当于计算A的所有可能选择，并将所有选择的联合概率聚合在一起：$P(B) = \sum\limits_{i=1}^{n} P(A_i,B) , P(A) = \sum\limits_{i=1}^{n} P(B_i,A)$</p>
<br/>
<br/>
<h5 id="期望、均值、方差、概率">期望、均值、方差、概率</h5>
<p>  一个随机变量X的期望（expectation，或平均值（average））表示为: $E(X) = \sum\limits_{x=1}^{n} x_i * P(X=x_i)$</p>
<p>  均值是一个统计量(基于样本构造的函数)，更偏统计学的概念；而期望完全由随机变量的概率分布所确定（更偏概率学的概念），类似于在“上帝视角”下去计算均值，所谓上帝视角是指你拥有的是总体并且知道总体所有取值出现的概率</p>
<p>  希望衡量随机变量X与其期望值的偏置。这可以通过方差来量化：$V(X) = \frac{\sum\limits_{i=1}<sup>{n}(x_i-E(X_i))</sup>2}{n}$</p>
<br/>
<br/>
<h5 id="似然函数及最大似然估计">似然函数及最大似然估计</h5>
<p>  似然估计函数,其可以解释为：假设我们有一个关于X的概率分布是$P(X=x|\theta)$，$\theta$是关于X的概率分布的参数。假如我们从关于X的概率分布是$P(X=x|\theta)$中抽取n个样本（这个时候往往我们是不知道其参数$\theta$的），这个时候我们想去估计$\theta$，因此我们可以定义似然函数为： $L(\theta|x_i,i\in{n}) = \prod\limits_{i=1}^{n} P(x_i|\theta)$。</p>
<p>  注意，我们可以知道由于有n个样本，每个样本都有一个$P(x_i|\theta)$，因此n个样本的联合概率就是似然函数，根据联合概率的定义，似然函数就是描述在$x_i$出现概率已知的情况下，出现$\theta$的概率。</p>
<p>  最大似然估计（Maximum Likelihood Estimation，MLE），又叫极大似然估计，是统计学中应用最广泛的一种未知参数估计方法。 它可以在已知随机变量属于哪种概率分布的前提下， 利用随机变量的一些观测值估计出分布的一些参数值。</p>
<p>  最大似然估计函数 就是 对似然估计函数 取对数，以简化乘积的计算。那么其定义是：$L(\theta|x_i,i\in{n}) = \sum\limits_{i=1}^{n} \ln{P(x_i|\theta)}$，对其求最大值就等于对其求 负最小值，其定义为：$-\ln_{}{L(\theta|x_i,i\in{n})} = -\sum\limits_{i=1}^{n} \ln(P(x_i|\theta))$</p>
<br/>
<br/>
<br/>
<br/>
<h3 id="信息论">信息论</h3>
<hr>
<p>  </p>
<br/>
<br/>
<h5 id="基本概念-2">基本概念</h5>
<p>  根据信息论中的定义：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>信息量，事件发生概率越大，所携带的信息量越小。定义为：$I(x)=\log_{2}(\frac{1}{P(x)})=-\log_{2}(P(x))=表示此事件的最少比特位数$ ,其也蕴含了我们需要使用多个比特才能表示信息量。</p>
</li>
<li class="lvl-2">
<p>信息熵，一个随机变量的熵是指该变量可能的结果所蕴含的不确定性的平均水平。可以类别期望的定义，这里得到信息量的均值：$H(X)=-\sum\limits_{i=0}^{n-1}P(Xi)\log_{2}(P(Xi))$</p>
</li>
<li class="lvl-2">
<p>KL差异：定义原概率分布为P(X),近似概率分布为Q(X)，假如X是离散随机变量，KL差异定义为：$D_{KL}(P(X)||Q(X))=\sum\limits_{i=0}<sup>{n-1}P(Xi)\log_{2}(P(Xi)/Q(Xi))=\sum\limits_{i=0}</sup>{n-1}P(Xi)[\log_{2}(P(Xi)) - \log_{2}(Q(Xi))]$</p>
</li>
<li class="lvl-2">
<p>交叉熵（cross-entropy），交叉熵定义为：$H(P,Q)=-\sum\limits_{i=0}^{n-1}P(Xi)\log_{2}(Q(Xi))$，我们可以看到$H(P,Q)=H(P)+D_{KL}(P||Q)$</p>
</li>
</ul>
<br/>
<br/>
<h5 id="深度学习中的交叉熵">深度学习中的交叉熵</h5>
<p>  假如: 数据集{X, Y}有n个样本，有特征向量$x_i \in X$，独热标签向量$y_i \in Y$，当前模型最终softmax输出的向量$\hat{y}_i$。首先，我们将$\hat{y}_i$当做给定特征向量$x_i$的每个类别的条件概率。因此我们可以得到：$\hat{y}_i=P(y_i|x_i)$</p>
<p>  根据似然函数定义：$L(\theta) = \prod_{i=1}^{n} P(y_i|x_i,\theta)$，其代表给定$x_i, \theta$情况下，观察到$y_i$的概率。</p>
<p>  根据似然函数，要使得其值为最大值，及 对其取负对数，此外根据我们之前的定义：我们将$\hat{y}<em>i$当做给定特征向量$x_i$的每个类别的条件概率。因此我们得到 $-log{L(\theta)} = -log{\prod\limits</em>{i=1}^{n} P(y_i|x_i,\theta)} = -\sum\limits_{i=1}^{n} log{P(y_i|x_i,\theta)}= -\sum\limits_{i=1}^{n} log{\hat{y_i}}$</p>
<p>  我们通过信息论中的定义得到交叉熵损失函数定义：$l(y,\hat{y}) = - \sum\limits_{j=1}^{q} y_j * log{\hat{y}_j}$ ，其描述的是标签$y_j$和预测值$\hat{y_j}$两个分布之间的差异值。</p>
<p>  我们对其交叉损失函数进行多个样本的求和可以得到：$\sum\limits_{i=1}^{n}{l(y,\hat{y})} = - \sum\limits_{i=1}<sup>{n}{\sum\limits_{j=1}</sup>{q} y_j * log{\hat{y}_j}}$</p>
<p>  由于$y_j$是标签的热独向量，只有对应类别概率为1，其他类别为0，因此$\sum\limits_{j=1}^{q} y_j * log{\hat{y}<em>j} = log{\hat{y}<em>j}$，因此可以得到交叉熵损失和似然函数之间的关系：$\sum\limits</em>{i=1}^{n}{l(y,\hat{y})} = - \sum\limits</em>{i=1}<sup>{n}{\sum\limits_{j=1}</sup>{q} y_j * log{\hat{y}<em>j}} = - \sum\limits</em>{i=1}^{n}{log{\hat{y}_j}} $</p>
<p>  从这里可以知道交叉熵损失函数与负对数似然函数的关系。</p>
<br/>
<br/>
<h5 id="困惑度（Perplexity）">困惑度（Perplexity）</h5>
<p>  这里我们讨论如何度量语言模型的质量。这里根据参考文件中的建议，不要直接使用交叉熵来理解，要从另外一个角度来理解。</p>
<p>  如果想要压缩文本，我们可以根据当前词元集预测的下一个词元。 一个更好的语言模型应该能让我们更准确地预测下一个词元。 因此，它应该允许我们在压缩序列时花费更少的比特。 所以我们可以通过一个序列中所有的个词元的交叉熵损失的平均值来衡量：<br>
$\frac{1}{n}\sum\limits_{t=1}^{n} -\log P(x_t|x_{t-1}, …, x_1)$，我们看这个公式的含义就是每个预测的词元信息量求和，然后再求平均值，最后平均值越小，意味着我们整个模型蕴含的信息量越小，我们要压缩这个文本需要的比特最少。</p>
<p>  由于历史原因，自然语言处理的科学家更喜欢使用一个叫做困惑度（perplexity）的量。 简而言之，它是上面提到的信息量均值的指数：$\exp(\frac{1}{n}\sum\limits_{t=1}^{n} -\log P(x_t|x_{t-1}, …, x_1))$</p>
<p>  我们来看一下困惑度的特性（信息量取值是[0,1]）：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>当最好情况下，因此当每个变量预测概率都是1，信息量为0，困惑度是$\exp^0 = 1$</p>
</li>
<li class="lvl-2">
<p>当最坏情况下，因此当每个变量预测概率都是0，信息量为无穷大，困惑度是$\exp^{\infty} = \infty$</p>
</li>
</ul>
<br/>
<br/>
<br/>
<br/>
<h3 id="后记">后记</h3>
<hr>
<p>  看了上面这些内容，有些是以前接触过的，有些是新的体验 总的来说，脑袋大了。</p>
<h3 id="参考文献">参考文献</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="http://zh.gluon.ai/">http://zh.gluon.ai/</a></p>
</li>
</ul>
<br/>
<br/>
<div style="margin:50px auto;">
</div>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <hr/>
        <center><font color = #91e0b0 size = 5>打赏、订阅、收藏、丢香蕉、硬币，请关注公众号（攻城狮的搬砖之路）</font></center>
    </div>
</div>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/qrcode_for_wx_official_account.jpg" alt="qrc_img"/></center>
    </div>
</div>
<!-- ![alt 公众号图片](https://flyinskyin2013.github.io/ImageBed0/blogs/qrcode_for_wx_official_account.jpg "公众号图片") -->
<p><font color="red" size="7">PS: 请尊重原创，不喜勿喷。</font><br/><br>
<font color="red" size="7">PS: 要转载请注明出处，本人版权所有。</font><br/><br>
<font color="red" size="7">PS: 有问题请留言，看到后我会第一时间回复。</font><br/></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/NO_EXSIT.XXXXXXX/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/NO_EXSIT.XXXXXXX/page/14/">14</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/NO_EXSIT.XXXXXXX/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2026</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Sky</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">286k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">17:19</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"position":"right","width":150,"height":250,"hOffset":50,"vOffset":5},"mobile":{"show":false,"scale":0.5},"react":{"opacityDefault":0.7,"opacityOnHover":0.8},"log":false});</script></body>
</html>
