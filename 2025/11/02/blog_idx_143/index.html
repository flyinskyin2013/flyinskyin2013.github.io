<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    
    <meta name="keywords" content="大模型基础补全计划(六)---带注意力机制的seq2seq实例与测试(Bahdanau Attention), Sky&#39;s Blogs">
    <meta name="description" content=" 
PS：要转载请注明出处，本人版权所有。
PS: 这个只是基于《我自己》的理解，如果和你的原则及想法相冲突，请谅解，勿喷。

环境说明
  无
前言

   本文是这个系列第六篇，它们是：


《大模型基础补全计划(一)—重温一些深度学习">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->

<script async src="https://www.googletagmanager.com/gtag/js?id=G-80RLMJ8DQP"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
        dataLayer.push(arguments);
    }

    gtag('js', new Date());
    gtag('config', 'G-80RLMJ8DQP');
</script>


    <title>大模型基础补全计划(六)---带注意力机制的seq2seq实例与测试(Bahdanau Attention) | Sky&#39;s Blogs</title>
    <link rel="icon" type="image/png" href="/favicon.png">
    


    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/css/matery.css">
<link rel="stylesheet" type="text/css" href="/css/my.css">
<link rel="stylesheet" type="text/css" href="/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/css/post.css">




    
        <link rel="stylesheet" type="text/css" href="/css/reward.css">
    



    <script src="/libs/jquery/jquery-3.6.0.min.js"></script>

    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8677552300382028" crossorigin="anonymous"></script>
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.jpg" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Sky&#39;s Blogs</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.jpg" class="logo-img circle responsive-img">
        
        <div class="logo-name">Sky&#39;s Blogs</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			留言板
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/flyinskyin2013" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Follow Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/flyinskyin2013" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Follow Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/12.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">大模型基础补全计划(六)---带注意力机制的seq2seq实例与测试(Bahdanau Attention)</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">
                                <span class="chip bg-color">深度学习</span>
                            </a>
                        
                            <a href="/tags/NLP/">
                                <span class="chip bg-color">NLP</span>
                            </a>
                        
                            <a href="/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                            <a href="/tags/LM/">
                                <span class="chip bg-color">LM</span>
                            </a>
                        
                            <a href="/tags/seq2seq/">
                                <span class="chip bg-color">seq2seq</span>
                            </a>
                        
                            <a href="/tags/attention/">
                                <span class="chip bg-color">attention</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">
                                深度学习
                            </a>
                        
                            <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/" class="post-category">
                                NLP
                            </a>
                        
                            <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/LLM/" class="post-category">
                                LLM
                            </a>
                        
                            <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/LLM/LM/" class="post-category">
                                LM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-11-02
                </div>
                

                

                

                

                
            </div>
        </div>
        <hr class="clearfix">

        
        <!-- 是否加载使用自带的 prismjs. -->
        <link rel="stylesheet" href="/libs/prism/prism.min.css">
        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <script src="\assets\js\APlayer.min.js"> </script><!--
 * @Description: 
 * @Author: Sky
 * @Date: 2020-08-24 16:37:34
 * @LastEditors: Sky
 * @LastEditTime: 2021-06-29 15:04:48
 * @Github: https://github.com/flyinskyin2013/
-->
<p><font color="red" size="7">PS：要转载请注明出处，本人版权所有。</font><br/><br>
<font color="red" size="7">PS: 这个只是基于《我自己》的理解，</font><br/><font color="red" size="7">如果和你的原则及想法相冲突，请谅解，勿喷。</font><br/></p>
<!-- ###### 前置说明
&emsp;&emsp;本文作为本人github blog/cnblog的主站的备份。（BlogID=143） 
&emsp;&emsp;本文发布于 2025-11-02 11:08:00             （BlogID=143） 
-->
<h6 id="环境说明">环境说明</h6>
<p>  无</p>
<h3 id="前言">前言</h3>
<hr>
<p>   本文是这个系列第六篇，它们是：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>《大模型基础补全计划(一)—重温一些深度学习相关的数学知识》 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/Iflyinsky/p/18717317">https://www.cnblogs.com/Iflyinsky/p/18717317</a></p>
</li>
<li class="lvl-2">
<p>《大模型基础补全计划(二)—词嵌入(word embedding) 》 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/Iflyinsky/p/18775451">https://www.cnblogs.com/Iflyinsky/p/18775451</a></p>
</li>
<li class="lvl-2">
<p>《大模型基础补全计划(三)—RNN实例与测试》 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/Iflyinsky/p/18967569">https://www.cnblogs.com/Iflyinsky/p/18967569</a></p>
</li>
<li class="lvl-2">
<p>《大模型基础补全计划(四)—LSTM的实例与测试(RNN的改进)》 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/Iflyinsky/p/19091089">https://www.cnblogs.com/Iflyinsky/p/19091089</a></p>
</li>
<li class="lvl-2">
<p>《大模型基础补全计划(五)—seq2seq实例与测试(编码器、解码器架构)》 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/Iflyinsky/p/19150535">https://www.cnblogs.com/Iflyinsky/p/19150535</a></p>
</li>
</ul>
<p>  本文，介绍一下注意力机制，并在上文的机翻模型seq2seq的实例中添加一个简单的注意力机制，并看看模型效果是否有提升。</p>
<br/>
<br/>
<br/>
<br/>
<h3 id="注意力机制（Bahdanau-Attention）">注意力机制（Bahdanau Attention）</h3>
<hr>
<p>   举一个例子：在日常生活中，比如我们看一幅黑白画（画中有一个红色的苹果，其他的都是黑白的物体，例如香蕉），这个时候我们无意识的看一眼画，很有可能第一个关注的就是这个红色的苹果，但是我有意识的控制眼睛集中去看香蕉，这个时候我关注的就是香蕉。</p>
<p>  在上面的例子中，我们的注意力，最开始是无意识的看苹果，后面有意识的注意香蕉，这里面的区别就是我们在这个动作里面加了：意识。当加了意识后，我们就可以有选择的根据条件来关注这幅画的我想关注的地方。</p>
<p>  然后我们可以对上面的现象进行建模：$R=Attention(Q,K)*V$，这里我们将Attention当作意识，V当作黑白画的特征，Q是画中是什么？K是V的标签（你可以把K当作是V有关联的部分，不同的K，对应的不同的V），如果没有Attention，R就是苹果，有了Attention，R就可以是香蕉。</p>
<p>  我们回头想一想上一篇文的seq2seq中，我们的encoder的output是最后一层rnn的所有时间步的隐藏状态（num_steps,batch_size,num_hiddens），这里包含了我们的序列数据在不同时间步的特征变化，当我们在做decoder的时候，我们是拿着这个encoder的最后一层rnn的最后一个时间步的隐藏状态（1,batch_size,num_hiddens）来作为context的，是一个固定的值，这样有几个问题：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>对于长序列来说，context可能丢失信息。</p>
</li>
<li class="lvl-2">
<p>我们从固定context中解码信息，导致了我们对序列在特定解码步骤中，对context关注重点是一样的。</p>
</li>
</ul>
<p>  针对上面seq2seq的问题，Bahdanau设计了一种模型，可以解决我们遇到的问题，其定义如下：$$c_{t’} = \sum_{t=1}^{T} \alpha(s_{t’-1}, h_{t})h_{t}$$，看公式我们可以知道，这里定义了Q（decoder的上一次隐藏态$s_{t’-1}$）/K（encoder的output的部分$h_{t}$）/V（encoder的output的部分$h_{t}$）三个概念，含义就是通过Q+K来计算一个权重矩阵W（通过softmax归一化），然后然后将W和V进行计算，得到了我们通过W关注到的新的$V_{new}$，这里的W就是我们的注意力矩阵，代表我们关注V中的哪些部分。整个计算过程，就相当于我们生成了新成context具备了注意力机制。</p>
<br/>
<br/>
<br/>
<br/>
<h3 id="带注意力机制的seq2seq-英文翻译中文-的实例">带注意力机制的seq2seq  英文翻译中文  的实例</h3>
<hr>
<p>   下面的代码和上一篇文章的代码只有decoder部分有比较大的差别，其他的基本类似。如dataset部分的内容，请参考上一篇文章。</p>
<br/>
<br/>
<h5 id="seq2seq完整代码如下">seq2seq完整代码如下</h5>
<p>  </p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> os
<span class="token keyword">import</span> random
<span class="token keyword">import</span> torch
<span class="token keyword">import</span> math
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> functional <span class="token keyword">as</span> F
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> time
<span class="token keyword">import</span> visdom
<span class="token keyword">import</span> collections

<span class="token keyword">import</span> dataset
<span class="token keyword">class</span> <span class="token class-name">Accumulator</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""在n个变量上累加"""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Defined in :numref:`sec_softmax_scratch`"""</span>
        self<span class="token punctuation">.</span>data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0.0</span><span class="token punctuation">]</span> <span class="token operator">*</span> n

    <span class="token keyword">def</span> <span class="token function">add</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>data <span class="token operator">=</span> <span class="token punctuation">[</span>a <span class="token operator">+</span> <span class="token builtin">float</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span> <span class="token keyword">for</span> a<span class="token punctuation">,</span> b <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>data<span class="token punctuation">,</span> args<span class="token punctuation">)</span><span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">reset</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0.0</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>data<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> idx<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>data<span class="token punctuation">[</span>idx<span class="token punctuation">]</span>
    
<span class="token keyword">class</span> <span class="token class-name">Timer</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""记录多次运行时间"""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Defined in :numref:`subsec_linear_model`"""</span>
        self<span class="token punctuation">.</span>times <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">start</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""启动计时器"""</span>
        self<span class="token punctuation">.</span>tik <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">stop</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""停止计时器并将时间记录在列表中"""</span>
        self<span class="token punctuation">.</span>times<span class="token punctuation">.</span>append<span class="token punctuation">(</span>time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> self<span class="token punctuation">.</span>tik<span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>times<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">avg</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""返回平均时间"""</span>
        <span class="token keyword">return</span> <span class="token builtin">sum</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>times<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>times<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">sum</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""返回时间总和"""</span>
        <span class="token keyword">return</span> <span class="token builtin">sum</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>times<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">cumsum</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""返回累计时间"""</span>
        <span class="token keyword">return</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>self<span class="token punctuation">.</span>times<span class="token punctuation">)</span><span class="token punctuation">.</span>cumsum<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
<span class="token keyword">class</span> <span class="token class-name">Encoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""编码器-解码器架构的基本编码器接口"""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 调用父类nn.Module的构造函数，确保正确初始化</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Encoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 抛出未实现错误，意味着该方法需要在子类中具体实现</span>
        <span class="token keyword">raise</span> NotImplementedError

<span class="token keyword">class</span> <span class="token class-name">Decoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""编码器-解码器架构的基本解码器接口

    Defined in :numref:`sec_encoder-decoder`"""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 调用父类nn.Module的构造函数，确保正确初始化</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Decoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">init_state</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> enc_outputs<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 抛出未实现错误，意味着该方法需要在子类中具体实现</span>
        <span class="token keyword">raise</span> NotImplementedError

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> state<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 抛出未实现错误，意味着该方法需要在子类中具体实现</span>
        <span class="token keyword">raise</span> NotImplementedError

<span class="token keyword">class</span> <span class="token class-name">EncoderDecoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""编码器-解码器架构的基类

    Defined in :numref:`sec_encoder-decoder`"""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> encoder<span class="token punctuation">,</span> decoder<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 调用父类nn.Module的构造函数，确保正确初始化</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>EncoderDecoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
        <span class="token comment"># 将传入的编码器实例赋值给类的属性</span>
        self<span class="token punctuation">.</span>encoder <span class="token operator">=</span> encoder
        <span class="token comment"># 将传入的解码器实例赋值给类的属性</span>
        self<span class="token punctuation">.</span>decoder <span class="token operator">=</span> decoder

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> enc_X<span class="token punctuation">,</span> dec_X<span class="token punctuation">,</span> enc_X_valid_len<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 调用编码器的前向传播方法，处理输入的编码器输入数据enc_X</span>
        enc_outputs <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>enc_X<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span>
        <span class="token comment"># 调用解码器的init_state方法，根据编码器的输出初始化解码器的状态</span>
        dec_state <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">.</span>init_state<span class="token punctuation">(</span>enc_outputs<span class="token punctuation">,</span> enc_X_valid_len<span class="token punctuation">)</span>
        <span class="token comment"># 调用解码器的前向传播方法，处理输入的解码器输入数据dec_X和初始化后的状态</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>dec_X<span class="token punctuation">,</span> dec_state<span class="token punctuation">)</span>
    

<span class="token keyword">def</span> <span class="token function">masked_softmax</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> valid_lens<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""
    执行带掩码的 Softmax 操作。
    
    参数:
        X (torch.Tensor): 待 Softmax 的张量，通常是注意力机制中的“分数”（scores）。
                          其形状通常为 (批量大小, 查询数量/序列长度, 键值对数量/序列长度)。
        valid_lens (torch.Tensor): 序列的有效长度张量。
                                   形状可以是 (批量大小,) 或 (批量大小, 键值对数量)。
                                   用于指示每个序列的哪个部分是有效的（非填充）。
    
    返回:
        torch.Tensor: 经过 Softmax 归一化且填充部分被忽略的概率分布张量。
    """</span>
    
    <span class="token comment"># 辅助函数：创建一个序列掩码，并用特定值覆盖被掩码（填充）的元素</span>
    <span class="token keyword">def</span> <span class="token function">_sequence_mask</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> valid_len<span class="token punctuation">,</span> value<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        根据有效长度（valid_len）创建掩码，并应用于张量 X。
        
        参数:
            X (torch.Tensor): 形状为 (批量大小 * 查询数量, 最大长度) 的张量。
            valid_len (torch.Tensor): 形状为 (批量大小 * 查询数量,) 的有效长度向量。
            value (float): 用于替换被掩码元素的填充值。
            
        返回:
            torch.Tensor: 被填充值覆盖后的张量 X。
        """</span>
        <span class="token comment"># 获取序列的最大长度（张量的第二个维度）</span>
        maxlen <span class="token operator">=</span> X<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        
        <span class="token comment"># 核心掩码逻辑：</span>
        <span class="token comment"># 1. torch.arange((maxlen), ...) 创建一个从 0 到 maxlen-1 的序列（代表时间步索引）</span>
        <span class="token comment"># 2. [None, :] 使其形状变为 (1, maxlen)，用于广播</span>
        <span class="token comment"># 3. valid_len[:, None] 使有效长度形状变为 (批量大小 * 查询数量, 1)，用于广播</span>
        <span class="token comment"># 4. &lt; 比较操作：当索引 &lt; 有效长度时，结果为 True（有效元素），否则为 False（填充元素）</span>
        mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token punctuation">(</span>maxlen<span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">,</span>
                            device<span class="token operator">=</span>X<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">&lt;</span> valid_len<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span>
        
        <span class="token comment"># 逻辑非 ~mask 得到填充部分的掩码（True 表示填充部分）</span>
        <span class="token comment"># 使用填充值（value，通常是 -1e6）覆盖填充元素</span>
        X<span class="token punctuation">[</span><span class="token operator">~</span>mask<span class="token punctuation">]</span> <span class="token operator">=</span> value
        <span class="token keyword">return</span> X

    <span class="token comment"># 1. 处理无需掩码的情况</span>
    <span class="token keyword">if</span> valid_lens <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        <span class="token comment"># 如果未提供有效长度，则执行标准 Softmax</span>
        <span class="token keyword">return</span> nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>X<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 2. 处理需要掩码的情况</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        <span class="token comment"># 备份原始形状，用于后续重塑</span>
        shape <span class="token operator">=</span> X<span class="token punctuation">.</span>shape
        
        <span class="token comment"># 统一 valid_lens 的形状，使其与 X 的前两个维度相匹配</span>
        <span class="token keyword">if</span> valid_lens<span class="token punctuation">.</span>dim<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
            <span class="token comment"># 适用于批量中每个序列只有一个有效长度的情况（例如，K-V 序列是等长的）</span>
            <span class="token comment"># 将 valid_lens 重复 shape[1] 次，匹配 X 的查询/序列长度维度</span>
            valid_lens <span class="token operator">=</span> torch<span class="token punctuation">.</span>repeat_interleave<span class="token punctuation">(</span>valid_lens<span class="token punctuation">,</span> shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token comment"># 适用于每个查询-键值对的有效长度都不同的情况</span>
            <span class="token comment"># 将 2D 张量展平为 1D 向量</span>
            valid_lens <span class="token operator">=</span> valid_lens<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
            
        <span class="token comment"># 预处理 Softmax 输入：将 X 调整为 2D 矩阵 (批量*查询数量, 键值对数量)</span>
        <span class="token comment"># 并在最后一个轴（Softmax 轴）上，用一个非常大的负值替换被掩码的元素</span>
        <span class="token comment"># Softmax 时 exp(-1e6) 趋近于 0，从而忽略填充部分。</span>
        X <span class="token operator">=</span> _sequence_mask<span class="token punctuation">(</span>X<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> valid_lens<span class="token punctuation">,</span> value<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1e6</span><span class="token punctuation">)</span>
        
        <span class="token comment"># 对经过掩码处理的 X 执行 Softmax</span>
        <span class="token comment"># 结果张量 X 被重塑回原始的 3D 形状 (批量大小, 查询数量, 键值对数量)</span>
        <span class="token comment"># 并在最后一个维度（dim=-1）上进行归一化，得到注意力权重</span>
        <span class="token keyword">return</span> nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>X<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>shape<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    

    
<span class="token keyword">class</span> <span class="token class-name">AdditiveAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""
    加性注意力（Additive Attention）模块。
    通过将 Query 和 Key 投影到相同的维度后相加，再通过 tanh 激活和线性层计算注意力分数。
    
    公式核心：score(Q, K) = w_v^T * tanh(W_q*Q + W_k*K)
    """</span>
    
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> dropout<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        初始化加性注意力模块。
        
        参数:
            num_hiddens (int): 隐藏层维度，Q 和 K 投影后的维度。
            dropout (float): Dropout 率。
        """</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>AdditiveAttention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
        
        <span class="token comment"># W_k：将 Key (K) 向量投影到 num_hiddens 维度的线性层</span>
        <span class="token comment"># 使用 nn.LazyLinear 延迟初始化，直到第一次 forward 传入 K 的维度</span>
        self<span class="token punctuation">.</span>W_k <span class="token operator">=</span> nn<span class="token punctuation">.</span>LazyLinear<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        
        <span class="token comment"># W_q：将 Query (Q) 向量投影到 num_hiddens 维度的线性层</span>
        <span class="token comment"># 使用 nn.LazyLinear 延迟初始化</span>
        self<span class="token punctuation">.</span>W_q <span class="token operator">=</span> nn<span class="token punctuation">.</span>LazyLinear<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        
        <span class="token comment"># w_v：将激活后的特征向量 (W_q*Q + W_k*K) 投影成一个标量分数（维度为 1）</span>
        <span class="token comment"># 使用 nn.LazyLinear 延迟初始化</span>
        self<span class="token punctuation">.</span>w_v <span class="token operator">=</span> nn<span class="token punctuation">.</span>LazyLinear<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        
        <span class="token comment"># Dropout 层，用于防止过拟合</span>
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> queries<span class="token punctuation">,</span> keys<span class="token punctuation">,</span> values<span class="token punctuation">,</span> valid_lens<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        执行前向传播计算。
        
        参数:
            queries (torch.Tensor): 查询向量 Q。形状通常为 (批量大小, 查询数量, 查询维度)。
            keys (torch.Tensor): 键向量 K。形状通常为 (批量大小, 键值对数量, 键维度)。
            values (torch.Tensor): 值向量 V。形状通常为 (批量大小, 键值对数量, 值维度)。
            valid_lens (torch.Tensor): 键值对序列的有效长度，用于掩盖填充部分。
            
        返回:
            torch.Tensor: 注意力加权后的值向量。形状为 (批量大小, 查询数量, 值维度)。
        """</span>
        <span class="token comment"># 1. 线性变换：分别对 Q 和 K 进行投影</span>
        queries<span class="token punctuation">,</span> keys <span class="token operator">=</span> self<span class="token punctuation">.</span>W_q<span class="token punctuation">(</span>queries<span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>W_k<span class="token punctuation">(</span>keys<span class="token punctuation">)</span>
        
        <span class="token comment"># 2. 维度扩展与相加（Attention Scoring 的核心步骤）</span>
        <span class="token comment"># queries.unsqueeze(2): 形状从 (批量大小, 查询数量, num_hiddens) </span>
        <span class="token comment">#                       变为 (批量大小, 查询数量, 1, num_hiddens)。</span>
        <span class="token comment"># keys.unsqueeze(1): 形状从 (批量大小, 键值对数量, num_hiddens) </span>
        <span class="token comment">#                     变为 (批量大小, 1, 键值对数量, num_hiddens)。</span>
        <span class="token comment"># 两个张量通过广播机制相加，得到 features，形状为：</span>
        <span class="token comment"># (批量大小, 查询数量, 键值对数量, num_hiddens)</span>
        features <span class="token operator">=</span> queries<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">+</span> keys<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        
        <span class="token comment"># 3. 激活函数：应用 tanh 激活（加性注意力机制的要求）</span>
        features <span class="token operator">=</span> torch<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>features<span class="token punctuation">)</span>
        
        <span class="token comment"># 4. 投影到标量分数</span>
        <span class="token comment"># self.w_v(features): 将 features 的最后一个维度（num_hiddens）投影成 1。</span>
        <span class="token comment"># scores.squeeze(-1): 移除最后一个单维度 (1)，得到最终的注意力分数张量。</span>
        <span class="token comment"># 形状为：(批量大小, 查询数量, 键值对数量)</span>
        scores <span class="token operator">=</span> self<span class="token punctuation">.</span>w_v<span class="token punctuation">(</span>features<span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        
        <span class="token comment"># 5. 归一化（Softmax）：使用带掩码的 Softmax 得到注意力权重</span>
        <span class="token comment"># 填充部分的得分会被设置为一个极小的负值，Softmax 后权重趋近于 0。</span>
        self<span class="token punctuation">.</span>attention_weights <span class="token operator">=</span> masked_softmax<span class="token punctuation">(</span>scores<span class="token punctuation">,</span> valid_lens<span class="token punctuation">)</span>
        
        <span class="token comment"># 6. 加权求和</span>
        <span class="token comment"># torch.bmm: 批量矩阵乘法 (Batch Matrix Multiplication)。</span>
        <span class="token comment"># 将 [注意力权重] (批量, Q数量, K数量) 与 [值向量] (批量, K数量, V维度) 相乘</span>
        <span class="token comment"># 得到最终的注意力输出，形状为：(批量大小, 查询数量, 值维度)</span>
        <span class="token comment"># 在 BMM 之前，对注意力权重应用 Dropout。</span>
        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>self<span class="token punctuation">.</span>attention_weights<span class="token punctuation">)</span><span class="token punctuation">,</span> values<span class="token punctuation">)</span>


<span class="token comment">#@save</span>
<span class="token keyword">class</span> <span class="token class-name">Seq2SeqEncoder</span><span class="token punctuation">(</span>Encoder<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""用于序列到序列学习的循环神经网络编码器"""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> embed_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> num_layers<span class="token punctuation">,</span>
                 dropout<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Seq2SeqEncoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
        <span class="token comment"># 嵌入层</span>
        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embed_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>rnn <span class="token operator">=</span> nn<span class="token punctuation">.</span>GRU<span class="token punctuation">(</span>embed_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> num_layers<span class="token punctuation">,</span>
                          dropout<span class="token operator">=</span>dropout<span class="token punctuation">)</span>
        <span class="token comment"># self.lstm = nn.LSTM(embed_size, num_hiddens, num_layers)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 输入X.shape = (batch_size,num_steps)</span>
        <span class="token comment"># 输出'X'的形状：(batch_size,num_steps,embed_size)</span>
        X <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
        <span class="token comment"># 在循环神经网络模型中，第一个轴对应于时间步</span>
        X <span class="token operator">=</span> X<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
        <span class="token comment"># 如果未提及状态，则默认为0</span>
        output<span class="token punctuation">,</span> state <span class="token operator">=</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
        <span class="token comment"># output : 这个返回值是所有时间步的隐藏状态序列</span>
        <span class="token comment"># output的形状:(num_steps,batch_size,num_hiddens)</span>
        <span class="token comment"># hn (hidden) : 这是每一层rnn的最后一个时间步的隐藏状态</span>
        <span class="token comment"># state的形状:(num_layers,batch_size,num_hiddens)</span>
        <span class="token keyword">return</span> output<span class="token punctuation">,</span> state
    
<span class="token keyword">class</span> <span class="token class-name">AttentionDecoder</span><span class="token punctuation">(</span>Decoder<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""The base attention-based decoder interface."""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token decorator annotation punctuation">@property</span>
    <span class="token keyword">def</span> <span class="token function">attention_weights</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">raise</span> NotImplementedError
    
<span class="token keyword">class</span> <span class="token class-name">Seq2SeqAttentionDecoder</span><span class="token punctuation">(</span>AttentionDecoder<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> embed_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> num_layers<span class="token punctuation">,</span>
                 dropout<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>attention <span class="token operator">=</span> AdditiveAttention<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embed_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>rnn <span class="token operator">=</span> nn<span class="token punctuation">.</span>GRU<span class="token punctuation">(</span>
            embed_size <span class="token operator">+</span> num_hiddens<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> num_layers<span class="token punctuation">,</span>
            dropout<span class="token operator">=</span>dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dense <span class="token operator">=</span> nn<span class="token punctuation">.</span>LazyLinear<span class="token punctuation">(</span>vocab_size<span class="token punctuation">)</span>
        <span class="token comment"># self.apply(d2l.init_seq2seq)</span>

    <span class="token keyword">def</span> <span class="token function">init_state</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> enc_outputs<span class="token punctuation">,</span> enc_valid_lens<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># Shape of outputs: (num_steps, batch_size, num_hiddens).</span>
        <span class="token comment"># Shape of hidden_state: (num_layers, batch_size, num_hiddens)</span>
        outputs<span class="token punctuation">,</span> hidden_state <span class="token operator">=</span> enc_outputs
        <span class="token keyword">return</span> <span class="token punctuation">(</span>outputs<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> hidden_state<span class="token punctuation">,</span> enc_valid_lens<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> state<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># Shape of enc_outputs: (batch_size, num_steps, num_hiddens).</span>
        <span class="token comment"># Shape of hidden_state: (num_layers, batch_size, num_hiddens)</span>
        enc_outputs<span class="token punctuation">,</span> hidden_state<span class="token punctuation">,</span> enc_valid_lens <span class="token operator">=</span> state
        <span class="token comment"># Shape of the output X: (num_steps, batch_size, embed_size)</span>
        X <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
        outputs<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_attention_weights <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> x <span class="token keyword">in</span> X<span class="token punctuation">:</span>
            <span class="token comment"># Shape of query: (batch_size, 1, num_hiddens)</span>
            query <span class="token operator">=</span> torch<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>hidden_state<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token comment"># Shape of context: (batch_size, 1, num_hiddens)</span>
            context  <span class="token operator">=</span> self<span class="token punctuation">.</span>attention<span class="token punctuation">(</span>
                query<span class="token punctuation">,</span> enc_outputs<span class="token punctuation">,</span> enc_outputs<span class="token punctuation">,</span> enc_valid_lens<span class="token punctuation">)</span>
            <span class="token comment"># Concatenate on the feature dimension</span>
            x <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>context<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>x<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token comment"># Reshape x as (1, batch_size, embed_size + num_hiddens)</span>
            out<span class="token punctuation">,</span> hidden_state <span class="token operator">=</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">(</span>x<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> hidden_state<span class="token punctuation">)</span>
            outputs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>out<span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>_attention_weights<span class="token punctuation">.</span>append<span class="token punctuation">(</span>self<span class="token punctuation">.</span>attention<span class="token punctuation">.</span>attention_weights<span class="token punctuation">)</span>
        <span class="token comment"># After fully connected layer transformation, shape of outputs:</span>
        <span class="token comment"># (num_steps, batch_size, vocab_size)</span>
        outputs <span class="token operator">=</span> self<span class="token punctuation">.</span>dense<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> outputs<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>enc_outputs<span class="token punctuation">,</span> hidden_state<span class="token punctuation">,</span>
                                          enc_valid_lens<span class="token punctuation">]</span>

    <span class="token decorator annotation punctuation">@property</span>
    <span class="token keyword">def</span> <span class="token function">attention_weights</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>_attention_weights

    
<span class="token keyword">def</span> <span class="token function">try_gpu</span><span class="token punctuation">(</span>i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""如果存在，则返回gpu(i)，否则返回cpu()

    Defined in :numref:`sec_use_gpu`"""</span>
    <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>device_count<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">>=</span> i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'cuda:</span><span class="token interpolation"><span class="token punctuation">&#123;</span>i<span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cpu'</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">sequence_mask</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> valid_len<span class="token punctuation">,</span> value<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""在序列中屏蔽不相关的项"""</span>
    maxlen <span class="token operator">=</span> X<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
    mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token punctuation">(</span>maxlen<span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">,</span>
                        device<span class="token operator">=</span>X<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">&lt;</span> valid_len<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span>
    X<span class="token punctuation">[</span><span class="token operator">~</span>mask<span class="token punctuation">]</span> <span class="token operator">=</span> value
    <span class="token keyword">return</span> X

<span class="token keyword">class</span> <span class="token class-name">MaskedSoftmaxCELoss</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""带遮蔽的softmax交叉熵损失函数"""</span>
    <span class="token comment"># pred的形状：(batch_size,num_steps,vocab_size)</span>
    <span class="token comment"># label的形状：(batch_size,num_steps)</span>
    <span class="token comment"># valid_len的形状：(batch_size,)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> pred<span class="token punctuation">,</span> label<span class="token punctuation">,</span> valid_len<span class="token punctuation">)</span><span class="token punctuation">:</span>
        weights <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span>label<span class="token punctuation">)</span>
        weights <span class="token operator">=</span> sequence_mask<span class="token punctuation">(</span>weights<span class="token punctuation">,</span> valid_len<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>reduction<span class="token operator">=</span><span class="token string">'none'</span>
        unweighted_loss <span class="token operator">=</span> <span class="token builtin">super</span><span class="token punctuation">(</span>MaskedSoftmaxCELoss<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>forward<span class="token punctuation">(</span>
            pred<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> label<span class="token punctuation">)</span>
        weighted_loss <span class="token operator">=</span> <span class="token punctuation">(</span>unweighted_loss <span class="token operator">*</span> weights<span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> weighted_loss
    
<span class="token keyword">def</span> <span class="token function">grad_clipping</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> theta<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""裁剪梯度"""</span>
    <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
        params <span class="token operator">=</span> <span class="token punctuation">[</span>p <span class="token keyword">for</span> p <span class="token keyword">in</span> net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> p<span class="token punctuation">.</span>requires_grad<span class="token punctuation">]</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        params <span class="token operator">=</span> net<span class="token punctuation">.</span>params
    norm <span class="token operator">=</span> torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token builtin">sum</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">(</span>p<span class="token punctuation">.</span>grad <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">for</span> p <span class="token keyword">in</span> params<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> norm <span class="token operator">></span> theta<span class="token punctuation">:</span>
        <span class="token keyword">for</span> param <span class="token keyword">in</span> params<span class="token punctuation">:</span>
            param<span class="token punctuation">.</span>grad<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">*=</span> theta <span class="token operator">/</span> norm

<span class="token keyword">def</span> <span class="token function">train_seq2seq</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> data_iter<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> tgt_vocab<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""训练序列到序列模型"""</span>
    <span class="token keyword">def</span> <span class="token function">xavier_init_weights</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token builtin">type</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span> <span class="token operator">==</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">:</span>
            nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>xavier_uniform_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>
        <span class="token keyword">if</span> <span class="token builtin">type</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span> <span class="token operator">==</span> nn<span class="token punctuation">.</span>GRU<span class="token punctuation">:</span>
            <span class="token keyword">for</span> param <span class="token keyword">in</span> m<span class="token punctuation">.</span>_flat_weights_names<span class="token punctuation">:</span>
                <span class="token keyword">if</span> <span class="token string">"weight"</span> <span class="token keyword">in</span> param<span class="token punctuation">:</span>
                    nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>xavier_uniform_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>_parameters<span class="token punctuation">[</span>param<span class="token punctuation">]</span><span class="token punctuation">)</span>

    net<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>xavier_init_weights<span class="token punctuation">)</span>
    net<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
    optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">)</span>
    loss <span class="token operator">=</span> MaskedSoftmaxCELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
    net<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
    vis <span class="token operator">=</span> visdom<span class="token punctuation">.</span>Visdom<span class="token punctuation">(</span>env<span class="token operator">=</span><span class="token string">u'test1'</span><span class="token punctuation">,</span> server<span class="token operator">=</span><span class="token string">"http://127.0.0.1"</span><span class="token punctuation">,</span> port<span class="token operator">=</span><span class="token number">8097</span><span class="token punctuation">)</span>
    animator <span class="token operator">=</span> vis
    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        timer <span class="token operator">=</span> Timer<span class="token punctuation">(</span><span class="token punctuation">)</span>
        metric <span class="token operator">=</span> Accumulator<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># 训练损失总和，词元数量</span>
        <span class="token keyword">for</span> batch <span class="token keyword">in</span> data_iter<span class="token punctuation">:</span>
            <span class="token comment">#清零（reset）优化器中的梯度缓存</span>
            optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token comment"># x.shape = [batch_size, num_steps]</span>
            X<span class="token punctuation">,</span> X_valid_len<span class="token punctuation">,</span> Y<span class="token punctuation">,</span> Y_valid_len <span class="token operator">=</span> <span class="token punctuation">[</span>x<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> batch<span class="token punctuation">]</span>
            <span class="token comment"># bos.shape = batch_size 个 bos-id</span>
            bos <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>tgt_vocab<span class="token punctuation">[</span><span class="token string">'&lt;bos>'</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">*</span> Y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                          device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token comment"># dec_input.shape = (batch_size, num_steps)</span>
            <span class="token comment"># 解码器的输入通常由序列的起始标志 bos 和目标序列（去掉末尾的部分 Y[:, :-1]）组成。</span>
            dec_input <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>bos<span class="token punctuation">,</span> Y<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 强制教学</span>
            <span class="token comment"># Y_hat的形状:(batch_size,num_steps,vocab_size)</span>
            Y_hat<span class="token punctuation">,</span> _ <span class="token operator">=</span> net<span class="token punctuation">(</span>X<span class="token punctuation">,</span> dec_input<span class="token punctuation">,</span> X_valid_len<span class="token punctuation">)</span>
            l <span class="token operator">=</span> loss<span class="token punctuation">(</span>Y_hat<span class="token punctuation">,</span> Y<span class="token punctuation">,</span> Y_valid_len<span class="token punctuation">)</span>
            l<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>      <span class="token comment"># 损失函数的标量进行“反向传播”</span>
            grad_clipping<span class="token punctuation">(</span>net<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
            num_tokens <span class="token operator">=</span> Y_valid_len<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
            optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                metric<span class="token punctuation">.</span>add<span class="token punctuation">(</span>l<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> num_tokens<span class="token punctuation">)</span>

        <span class="token keyword">if</span> <span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">10</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            <span class="token comment"># print(predict('你是？'))</span>
            <span class="token comment"># print(epoch)</span>
            <span class="token comment"># animator.add(epoch + 1, )</span>

            <span class="token keyword">if</span> epoch <span class="token operator">==</span> <span class="token number">9</span><span class="token punctuation">:</span>
                <span class="token comment"># 清空图表：使用空数组来替换现有内容</span>
                vis<span class="token punctuation">.</span>line<span class="token punctuation">(</span>X<span class="token operator">=</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> Y<span class="token operator">=</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> win<span class="token operator">=</span><span class="token string">'train_ch8'</span><span class="token punctuation">,</span> update<span class="token operator">=</span><span class="token string">'replace'</span><span class="token punctuation">)</span>
            <span class="token comment"># _loss_val = l</span>
            <span class="token comment"># _loss_val = _loss_val.cpu().sum().detach().numpy()</span>
            vis<span class="token punctuation">.</span>line<span class="token punctuation">(</span>
                X<span class="token operator">=</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                Y<span class="token operator">=</span><span class="token punctuation">[</span> metric<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">/</span> metric<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                win<span class="token operator">=</span><span class="token string">'train_ch8'</span><span class="token punctuation">,</span>
                update<span class="token operator">=</span><span class="token string">'append'</span><span class="token punctuation">,</span>
                opts<span class="token operator">=</span><span class="token punctuation">&#123;</span>
                    <span class="token string">'title'</span><span class="token punctuation">:</span> <span class="token string">'train_ch8'</span><span class="token punctuation">,</span>
                    <span class="token string">'xlabel'</span><span class="token punctuation">:</span> <span class="token string">'epoch'</span><span class="token punctuation">,</span>
                    <span class="token string">'ylabel'</span><span class="token punctuation">:</span> <span class="token string">'loss'</span><span class="token punctuation">,</span>
                    <span class="token string">'linecolor'</span><span class="token punctuation">:</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">255</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment"># 蓝色线条</span>
                <span class="token punctuation">&#125;</span>
            <span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'loss </span><span class="token interpolation"><span class="token punctuation">&#123;</span>metric<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">/</span> metric<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">&#125;</span></span><span class="token string">, </span><span class="token interpolation"><span class="token punctuation">&#123;</span>metric<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">/</span> timer<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.1f</span><span class="token punctuation">&#125;</span></span><span class="token string"> '</span></span>
        <span class="token string-interpolation"><span class="token string">f'tokens/sec on </span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">str</span><span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
    torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>net<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'model_h.pt'</span><span class="token punctuation">)</span>  <span class="token comment"># [[6]]</span>
    torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>net<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'model.pt'</span><span class="token punctuation">)</span>  <span class="token comment"># [[6]]</span>

<span class="token keyword">def</span> <span class="token function">predict_seq2seq</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> src_sentence<span class="token punctuation">,</span> src_vocab<span class="token punctuation">,</span> tgt_vocab<span class="token punctuation">,</span> num_steps<span class="token punctuation">,</span>
                    device<span class="token punctuation">,</span> save_attention_weights<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""序列到序列模型的预测"""</span>
    <span class="token comment"># 在预测时将net设置为评估模式</span>
    net<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    src_tokens <span class="token operator">=</span> src_vocab<span class="token punctuation">[</span>src_sentence<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">[</span>
        src_vocab<span class="token punctuation">[</span><span class="token string">'&lt;eos>'</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
    enc_valid_len <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>src_tokens<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>
    src_tokens <span class="token operator">=</span> dataset<span class="token punctuation">.</span>truncate_pad<span class="token punctuation">(</span>src_tokens<span class="token punctuation">,</span> num_steps<span class="token punctuation">,</span> src_vocab<span class="token punctuation">[</span><span class="token string">'&lt;pad>'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token comment"># 添加批量轴</span>
    enc_X <span class="token operator">=</span> torch<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>
        torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>src_tokens<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
    enc_outputs <span class="token operator">=</span> net<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>enc_X<span class="token punctuation">,</span> enc_valid_len<span class="token punctuation">)</span>
    dec_state <span class="token operator">=</span> net<span class="token punctuation">.</span>decoder<span class="token punctuation">.</span>init_state<span class="token punctuation">(</span>enc_outputs<span class="token punctuation">,</span> enc_valid_len<span class="token punctuation">)</span>
    <span class="token comment"># 添加批量轴</span>
    dec_X <span class="token operator">=</span> torch<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>
        <span class="token punctuation">[</span>tgt_vocab<span class="token punctuation">[</span><span class="token string">'&lt;bos>'</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
    output_seq<span class="token punctuation">,</span> attention_weight_seq <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_steps<span class="token punctuation">)</span><span class="token punctuation">:</span>
        Y<span class="token punctuation">,</span> dec_state <span class="token operator">=</span> net<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>dec_X<span class="token punctuation">,</span> dec_state<span class="token punctuation">)</span>
        <span class="token comment"># 我们使用具有预测最高可能性的词元，作为解码器在下一时间步的输入</span>
        dec_X <span class="token operator">=</span> Y<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        pred <span class="token operator">=</span> dec_X<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>int32<span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 保存注意力权重（稍后讨论）</span>
        <span class="token keyword">if</span> save_attention_weights<span class="token punctuation">:</span>
            attention_weight_seq<span class="token punctuation">.</span>append<span class="token punctuation">(</span>net<span class="token punctuation">.</span>decoder<span class="token punctuation">.</span>attention_weights<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>num_steps<span class="token punctuation">)</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># 一旦序列结束词元被预测，输出序列的生成就完成了</span>
        <span class="token keyword">if</span> pred <span class="token operator">==</span> tgt_vocab<span class="token punctuation">[</span><span class="token string">'&lt;eos>'</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
            <span class="token keyword">break</span>
        output_seq<span class="token punctuation">.</span>append<span class="token punctuation">(</span>pred<span class="token punctuation">)</span>
    <span class="token keyword">return</span> <span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>tgt_vocab<span class="token punctuation">.</span>to_tokens<span class="token punctuation">(</span>output_seq<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> attention_weight_seq


<span class="token keyword">def</span> <span class="token function">bleu</span><span class="token punctuation">(</span>pred_seq<span class="token punctuation">,</span> label_seq<span class="token punctuation">,</span> k<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""计算BLEU"""</span>
    pred_tokens<span class="token punctuation">,</span> label_tokens <span class="token operator">=</span> pred_seq<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>i <span class="token keyword">for</span> i <span class="token keyword">in</span> label_seq<span class="token punctuation">]</span>
    len_pred<span class="token punctuation">,</span> len_label <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>pred_tokens<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>label_tokens<span class="token punctuation">)</span>
    score <span class="token operator">=</span> math<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token builtin">min</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span> <span class="token operator">-</span> len_label <span class="token operator">/</span> len_pred<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> n <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> k <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        num_matches<span class="token punctuation">,</span> label_subs <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> collections<span class="token punctuation">.</span>defaultdict<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>len_label <span class="token operator">-</span> n <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            label_subs<span class="token punctuation">[</span><span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>label_tokens<span class="token punctuation">[</span>i<span class="token punctuation">:</span> i <span class="token operator">+</span> n<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">1</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>len_pred <span class="token operator">-</span> n <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> label_subs<span class="token punctuation">[</span><span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>pred_tokens<span class="token punctuation">[</span>i<span class="token punctuation">:</span> i <span class="token operator">+</span> n<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>
                num_matches <span class="token operator">+=</span> <span class="token number">1</span>
                label_subs<span class="token punctuation">[</span><span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>pred_tokens<span class="token punctuation">[</span>i<span class="token punctuation">:</span> i <span class="token operator">+</span> n<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">-=</span> <span class="token number">1</span>
        score <span class="token operator">*=</span> math<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>num_matches <span class="token operator">/</span> <span class="token punctuation">(</span>len_pred <span class="token operator">-</span> n <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> math<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> score


<span class="token keyword">from</span> matplotlib <span class="token keyword">import</span> pyplot <span class="token keyword">as</span> plt
<span class="token keyword">import</span> matplotlib
<span class="token comment"># from matplotlib_inline import backend_inline</span>
<span class="token keyword">def</span> <span class="token function">show_heatmaps</span><span class="token punctuation">(</span>matrices<span class="token punctuation">,</span> xlabel<span class="token punctuation">,</span> ylabel<span class="token punctuation">,</span> titles<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2.5</span><span class="token punctuation">,</span> <span class="token number">2.5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                  cmap<span class="token operator">=</span><span class="token string">'Reds'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    显示矩阵的热图（Heatmaps）。
    这个函数旨在以子图网格的形式绘制多个矩阵，通常用于可视化注意力权重等。

    参数:
        matrices (numpy.ndarray 或 torch.Tensor 数组): 
            一个四维数组，形状应为 (num_rows, num_cols, height, width)。
            其中，num_rows 和 num_cols 决定了子图网格的布局，
            height 和 width 是每个热图（即每个矩阵）的维度。
        xlabel (str): 
            所有最底行子图的 x 轴标签。
        ylabel (str): 
            所有最左列子图的 y 轴标签。
        titles (list of str, optional): 
            一个包含 num_cols 个标题的列表，用于设置每一列子图的标题。默认 None。
        figsize (tuple, optional): 
            整个图形（figure）的大小。默认 (2.5, 2.5)。
        cmap (str, optional): 
            用于绘制热图的颜色映射（colormap）。默认 'Reds'。
    """</span>
    <span class="token comment"># 导入所需的 matplotlib 模块，确保图形在 Jupyter/IPython 环境中正确显示为 SVG 格式</span>
    <span class="token comment"># （假设在包含这个函数的环境中已经导入了 matplotlib 的 backend_inline）</span>
    <span class="token comment"># backend_inline.set_matplotlib_formats('svg')</span>
    matplotlib<span class="token punctuation">.</span>use<span class="token punctuation">(</span><span class="token string">'TkAgg'</span><span class="token punctuation">)</span>
    <span class="token comment"># 从输入的 matrices 形状中解构出子图网格的行数和列数</span>
    <span class="token comment"># 假设 matrices 的形状是 (num_rows, num_cols, height, width)</span>
    num_rows<span class="token punctuation">,</span> num_cols<span class="token punctuation">,</span> _<span class="token punctuation">,</span> _ <span class="token operator">=</span> matrices<span class="token punctuation">.</span>shape
    
    <span class="token comment"># 创建一个包含多个子图（axes）的图形（fig）</span>
    <span class="token comment"># fig: 整个图形对象</span>
    <span class="token comment"># axes: 一个 num_rows x num_cols 的子图对象数组</span>
    fig<span class="token punctuation">,</span> axes <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span>
        num_rows<span class="token punctuation">,</span> num_cols<span class="token punctuation">,</span> 
        figsize<span class="token operator">=</span>figsize<span class="token punctuation">,</span>
        sharex<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    <span class="token comment"># 所有子图共享 x 轴刻度</span>
        sharey<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    <span class="token comment"># 所有子图共享 y 轴刻度</span>
        squeeze<span class="token operator">=</span><span class="token boolean">False</span>   <span class="token comment"># 即使只有一行或一列，也强制返回二维数组的 axes，方便后续循环</span>
    <span class="token punctuation">)</span>
    
    <span class="token comment"># 遍历子图的行和对应的矩阵行</span>
    <span class="token comment"># i 是行索引, row_axes 是当前行的子图数组, row_matrices 是当前行的矩阵数组</span>
    <span class="token keyword">for</span> i<span class="token punctuation">,</span> <span class="token punctuation">(</span>row_axes<span class="token punctuation">,</span> row_matrices<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span><span class="token builtin">zip</span><span class="token punctuation">(</span>axes<span class="token punctuation">,</span> matrices<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 遍历当前行中的子图和对应的矩阵</span>
        <span class="token comment"># j 是列索引, ax 是当前的子图对象, matrix 是当前的待绘矩阵</span>
        <span class="token keyword">for</span> j<span class="token punctuation">,</span> <span class="token punctuation">(</span>ax<span class="token punctuation">,</span> matrix<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span><span class="token builtin">zip</span><span class="token punctuation">(</span>row_axes<span class="token punctuation">,</span> row_matrices<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            
            <span class="token comment"># 使用 ax.imshow() 绘制热图</span>
            <span class="token comment"># matrix.detach().numpy()：将 PyTorch Tensor 转换为 numpy 数组，并从计算图中分离（如果它是 Tensor）</span>
            <span class="token comment"># cmap：指定颜色映射</span>
            pcm <span class="token operator">=</span> ax<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>matrix<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cmap<span class="token operator">=</span>cmap<span class="token punctuation">)</span>
            
            <span class="token comment"># --- 设置轴标签和标题 ---</span>
            
            <span class="token comment"># 只有最底行 (i == num_rows - 1) 的子图才显示 x 轴标签</span>
            <span class="token keyword">if</span> i <span class="token operator">==</span> num_rows <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">:</span>
                ax<span class="token punctuation">.</span>set_xlabel<span class="token punctuation">(</span>xlabel<span class="token punctuation">)</span>
                
            <span class="token comment"># 只有最左列 (j == 0) 的子图才显示 y 轴标签</span>
            <span class="token keyword">if</span> j <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
                ax<span class="token punctuation">.</span>set_ylabel<span class="token punctuation">(</span>ylabel<span class="token punctuation">)</span>
                
            <span class="token comment"># 如果提供了标题列表，则设置当前列的子图标题（所有行共享列标题）</span>
            <span class="token keyword">if</span> titles<span class="token punctuation">:</span>
                ax<span class="token punctuation">.</span>set_title<span class="token punctuation">(</span>titles<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">)</span>
                
    <span class="token comment"># --- 添加颜色条（Colorbar） ---</span>
    
    <span class="token comment"># 为整个图形添加一个颜色条，用于表示数值和颜色的对应关系</span>
    <span class="token comment"># pcm: 之前绘制的第一个热图返回的 Colormap </span>
    <span class="token comment"># ax=axes: 颜色条将参照整个子图网格进行定位和缩放</span>
    <span class="token comment"># shrink=0.6: 缩小颜色条的高度/长度，使其只占图形高度的 60%</span>
    fig<span class="token punctuation">.</span>colorbar<span class="token punctuation">(</span>pcm<span class="token punctuation">,</span> ax<span class="token operator">=</span>axes<span class="token punctuation">,</span> shrink<span class="token operator">=</span><span class="token number">0.6</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    embed_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> num_layers<span class="token punctuation">,</span> dropout <span class="token operator">=</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0.1</span>
    batch_size<span class="token punctuation">,</span> num_steps <span class="token operator">=</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">10</span>
    lr<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> device <span class="token operator">=</span> <span class="token number">0.005</span><span class="token punctuation">,</span> <span class="token number">2000</span><span class="token punctuation">,</span> try_gpu<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># train_iter 每个迭代输出：(batch_size, num_steps)</span>
    train_iter<span class="token punctuation">,</span> src_vocab<span class="token punctuation">,</span> tgt_vocab<span class="token punctuation">,</span> source<span class="token punctuation">,</span> target <span class="token operator">=</span> dataset<span class="token punctuation">.</span>load_data<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> num_steps<span class="token punctuation">)</span>
    encoder <span class="token operator">=</span> Seq2SeqEncoder<span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>src_vocab<span class="token punctuation">)</span><span class="token punctuation">,</span> embed_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> num_layers<span class="token punctuation">,</span>
                        dropout<span class="token punctuation">)</span>
    decoder <span class="token operator">=</span> Seq2SeqAttentionDecoder<span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>tgt_vocab<span class="token punctuation">)</span><span class="token punctuation">,</span> embed_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> num_layers<span class="token punctuation">,</span>
                            dropout<span class="token punctuation">)</span>
    net <span class="token operator">=</span> EncoderDecoder<span class="token punctuation">(</span>encoder<span class="token punctuation">,</span> decoder<span class="token punctuation">)</span>

    is_train <span class="token operator">=</span> <span class="token boolean">False</span>
    is_show <span class="token operator">=</span> <span class="token boolean">False</span>
    <span class="token keyword">if</span> is_train<span class="token punctuation">:</span>
        train_seq2seq<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> tgt_vocab<span class="token punctuation">,</span> device<span class="token punctuation">)</span>
    <span class="token keyword">elif</span> is_show<span class="token punctuation">:</span>
        state_dict <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'model_h.pt'</span><span class="token punctuation">)</span>
        net<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>state_dict<span class="token punctuation">)</span>
        net<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>

        src_text <span class="token operator">=</span> <span class="token string">"Call us."</span>
        translation<span class="token punctuation">,</span> attention_weight_seq <span class="token operator">=</span> predict_seq2seq<span class="token punctuation">(</span>
                net<span class="token punctuation">,</span> src_text<span class="token punctuation">,</span> src_vocab<span class="token punctuation">,</span> tgt_vocab<span class="token punctuation">,</span> num_steps<span class="token punctuation">,</span> device<span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token comment"># attention_weights = torch.eye(10).reshape((1, 1, 10, 10))</span>
        <span class="token comment"># (num_rows, num_cols, height, width)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'translation=</span><span class="token interpolation"><span class="token punctuation">&#123;</span>translation<span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>attention_weight_seq<span class="token punctuation">)</span>

        stacked_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>attention_weight_seq<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
        stacked_tensor <span class="token operator">=</span> stacked_tensor<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
        show_heatmaps<span class="token punctuation">(</span>
            stacked_tensor<span class="token punctuation">,</span>
            xlabel<span class="token operator">=</span><span class="token string">'Attention weight'</span><span class="token punctuation">,</span> ylabel<span class="token operator">=</span><span class="token string">'Decode Step'</span><span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        state_dict <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'model_h.pt'</span><span class="token punctuation">)</span>
        net<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>state_dict<span class="token punctuation">)</span>
        net<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
        C <span class="token operator">=</span> <span class="token number">0</span>
        C1 <span class="token operator">=</span> <span class="token number">0</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">2000</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># print(source[i])</span>
            <span class="token comment"># print(target[i])</span>
            translation<span class="token punctuation">,</span> attention_weight_seq <span class="token operator">=</span> predict_seq2seq<span class="token punctuation">(</span>
                net<span class="token punctuation">,</span> source<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> src_vocab<span class="token punctuation">,</span> tgt_vocab<span class="token punctuation">,</span> num_steps<span class="token punctuation">,</span> device<span class="token punctuation">)</span>
            
            score <span class="token operator">=</span> bleu<span class="token punctuation">(</span>translation<span class="token punctuation">,</span> target<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> k<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
            <span class="token keyword">if</span> score <span class="token operator">></span> <span class="token number">0.0</span><span class="token punctuation">:</span>
                C <span class="token operator">=</span> C <span class="token operator">+</span> <span class="token number">1</span>
                <span class="token keyword">if</span> score <span class="token operator">></span> <span class="token number">0.8</span><span class="token punctuation">:</span>
                    C1 <span class="token operator">=</span> C1 <span class="token operator">+</span> <span class="token number">1</span>
                <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'</span><span class="token interpolation"><span class="token punctuation">&#123;</span>source<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">&#125;</span></span><span class="token string"> => </span><span class="token interpolation"><span class="token punctuation">&#123;</span>translation<span class="token punctuation">&#125;</span></span><span class="token string">, bleu </span><span class="token interpolation"><span class="token punctuation">&#123;</span>score<span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>

        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Counter(bleu > 0) = </span><span class="token interpolation"><span class="token punctuation">&#123;</span>C<span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Valid-Counter(bleu > 0.8) = </span><span class="token interpolation"><span class="token punctuation">&#123;</span>C1<span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>  下面是encoder过程的简单分析：</p>
<ol>
<li class="lvl-3">
<p>将x通过nn.Embedding得到了(batch_size,num_steps,embed_size)的输入嵌入向量。</p>
</li>
<li class="lvl-3">
<p>将嵌入向量传给nn.GRU，得到了两个输出，并返回：</p>
<ul class="lvl-2">
<li class="lvl-6">output，最后一层rnn的所有时间步的隐藏状态（num_steps,batch_size,num_hiddens）。</li>
<li class="lvl-6">h_n,所有rnn层的，最后一个时间步的隐藏状态（num_layers,batch_size,num_hiddens）。</li>
</ul>
</li>
</ol>
<p>  下面是decoder过程的简单分析：</p>
<ol>
<li class="lvl-3">
<p>将decoder_x通过nn.Embedding得到了(batch_size,num_steps,embed_size)的输入嵌入向量。</p>
</li>
<li class="lvl-3">
<p>将嵌入向量沿着num_steps进行单步运行，每一步经过Attention过程，得到最终的output，以及最后一个时间步的所有rnn层的h_n，每一步执行如下步骤：</p>
<ul class="lvl-2">
<li class="lvl-6">将rnn最后一层的隐藏态作为Q（第一次Q是来自于encoder，后续都是decoder的每一次运行过程产生的隐藏态）</li>
<li class="lvl-6">将encoder的output作为K,V，得到当前动态的上下文 context</li>
<li class="lvl-6">将decoder_x_step 和 context进行组合，得到decoder_x_step_new</li>
<li class="lvl-6">将decoder_x_step_new送入nn.GRU，得到当前时间步的output, h_t</li>
<li class="lvl-6">将每一步的output收集起来作为输出，将h_t作为下一个时间步的Q循环起来</li>
</ul>
</li>
<li class="lvl-3">
<p>将所有的output经过nn.LazyLinear 映射为(num_steps, batch_size, vocab_size)，并和h_t返回</p>
</li>
</ol>
<p>  和原版本的seq2seq进行对比可知：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>在原版中，我们的decoder依赖于一个固定的enc_outputs进行循环解码</p>
</li>
<li class="lvl-2">
<p>在新版中，我们的decoder每次界面，都会有一个Q（第一次是来至于encoder，后续都是decoder的每一次运行过程产生的隐藏态）来计算enc_outputs的权重分数，然后根据权重分数得到一个动态的enc_outputs，这样可以让解码器每一步都关注enc_outputs中的不同的重点。</p>
</li>
</ul>
<p>  attention weight 的解释：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>把Encoder Output（num_steps,1,num_hiddens）作为K,V</p>
</li>
<li class="lvl-2">
<p>将Decoder的隐藏态h_t（1,1,num_hiddens）(初始值来自于Encoder的隐藏态h_t)作为Q，计算出当前step的attention_weight，其是一个softmax概率数据。</p>
</li>
<li class="lvl-2">
<p>然后将attention_weight 与 V进行计算，代表模型当前关注EncoderOutput的那部分数据，得到新的Context</p>
</li>
</ul>
<p>  下面是训练和测试的一些结果</p>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/blog_idx_143/train.png" alt="rep_img"/></center>
    </div>
</div>   
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/blog_idx_143/ret.png" alt="rep_img"/></center>
    </div>
</div>   
<p>  从上面的图可以看到，这个模型有一定的翻译效果，并且，比上一篇文章的模型效果要好一点。</p>
<p>  此外，下面是我们翻译：“Call us.”-&gt; “联 系 我 们 。” 的attention weight的可视化（带mask=3，在不同的decode step中权重变化。）</p>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/blog_idx_143/attention_weight.png" alt="rep_img"/></center>
    </div>
</div>   
<p>  从上面的图可以知道，每一个decode step的注意力权重矩阵值都不一样，意味着，每一步解码的时候，关注的内容也不一样。</p>
<br/>
<br/>
<br/>
<br/>
<h3 id="后记">后记</h3>
<hr>
<p>  本文引入了注意力机制，及注意力机制在seq2seq中，在应用注意力机制后，和原版的seq2seq的结论相比，模型效果有提升。</p>
<h3 id="参考文献">参考文献</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://zh.d2l.ai/chapter_recurrent-modern/encoder-decoder.html">https://zh.d2l.ai/chapter_recurrent-modern/encoder-decoder.html</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://zh.d2l.ai/chapter_recurrent-modern/seq2seq.html">https://zh.d2l.ai/chapter_recurrent-modern/seq2seq.html</a></p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://d2l.ai/chapter_attention-mechanisms-and-transformers/bahdanau-attention.html">https://d2l.ai/chapter_attention-mechanisms-and-transformers/bahdanau-attention.html</a></p>
</li>
</ul>
<br/>
<br/>
<div style="margin:50px auto;">
</div>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <hr/>
        <center><font color = #91e0b0 size = 5>打赏、订阅、收藏、丢香蕉、硬币，请关注公众号（攻城狮的搬砖之路）</font></center>
    </div>
</div>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/qrcode_for_wx_official_account.jpg" alt="qrc_img"/></center>
    </div>
</div>
<!-- ![alt 公众号图片](https://flyinskyin2013.github.io/ImageBed0/blogs/qrcode_for_wx_official_account.jpg "公众号图片") -->
<p><font color="red" size="7">PS: 请尊重原创，不喜勿喷。</font><br/><br>
<font color="red" size="7">PS: 要转载请注明出处，本人版权所有。</font><br/><br>
<font color="red" size="7">PS: 有问题请留言，看到后我会第一时间回复。</font><br/></p>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">Sky</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://e-x.top/2025/11/02/blog_idx_143/">https://e-x.top/2025/11/02/blog_idx_143/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">Sky</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">
                                    <span class="chip bg-color">深度学习</span>
                                </a>
                            
                                <a href="/tags/NLP/">
                                    <span class="chip bg-color">NLP</span>
                                </a>
                            
                                <a href="/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                                <a href="/tags/LM/">
                                    <span class="chip bg-color">LM</span>
                                </a>
                            
                                <a href="/tags/seq2seq/">
                                    <span class="chip bg-color">seq2seq</span>
                                </a>
                            
                                <a href="/tags/attention/">
                                    <span class="chip bg-color">attention</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>



            
                <div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.jpg" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>


</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            


            
                <div class="card" data-aos="fade-up">
    <div id="utteranc-container" class="card-content">
        <script src="https://utteranc.es/client.js"
        repo="flyinskyin2013/blog_issues_rep"
        issue-term="pathname"
        theme="github-light"
        crossorigin="anonymous"
        async>
        </script>
    </div>
</div>

            

        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2025/11/16/blog_idx_144/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/1.jpg" class="responsive-img" alt="大模型基础补全计划(七)---Transformer(多头注意力、自注意力、位置编码)及实例与测试">
                        
                        <span class="card-title">大模型基础补全计划(七)---Transformer(多头注意力、自注意力、位置编码)及实例与测试</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-16
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    深度学习
                                </a>
                            
                            <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/" class="post-category">
                                    NLP
                                </a>
                            
                            <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/LLM/LM/" class="post-category">
                                    LM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">深度学习</span>
                    </a>
                    
                    <a href="/tags/NLP/">
                        <span class="chip bg-color">NLP</span>
                    </a>
                    
                    <a href="/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                    <a href="/tags/LM/">
                        <span class="chip bg-color">LM</span>
                    </a>
                    
                    <a href="/tags/seq2seq/">
                        <span class="chip bg-color">seq2seq</span>
                    </a>
                    
                    <a href="/tags/attention/">
                        <span class="chip bg-color">attention</span>
                    </a>
                    
                    <a href="/tags/transformer/">
                        <span class="chip bg-color">transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2025/10/19/blog_idx_142/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/5.jpg" class="responsive-img" alt="大模型基础补全计划(五)---seq2seq实例与测试(编码器、解码器架构)">
                        
                        <span class="card-title">大模型基础补全计划(五)---seq2seq实例与测试(编码器、解码器架构)</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    深度学习
                                </a>
                            
                            <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/" class="post-category">
                                    NLP
                                </a>
                            
                            <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/LLM/LM/" class="post-category">
                                    LM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">深度学习</span>
                    </a>
                    
                    <a href="/tags/NLP/">
                        <span class="chip bg-color">NLP</span>
                    </a>
                    
                    <a href="/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                    <a href="/tags/LM/">
                        <span class="chip bg-color">LM</span>
                    </a>
                    
                    <a href="/tags/seq2seq/">
                        <span class="chip bg-color">seq2seq</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>


<script>
    $('#articleContent').on('copy', function (e) {
        // IE8 or earlier browser is 'undefined'
        if (typeof window.getSelection === 'undefined') return;

        var selection = window.getSelection();
        // if the selection is short let's not annoy our users.
        if (('' + selection).length < Number.parseInt('20')) {
            return;
        }

        // create a div outside of the visible area and fill it with the selected text.
        var bodyElement = document.getElementsByTagName('body')[0];
        var newdiv = document.createElement('div');
        newdiv.style.position = 'absolute';
        newdiv.style.left = '-99999px';
        bodyElement.appendChild(newdiv);
        newdiv.appendChild(selection.getRangeAt(0).cloneContents());

        // we need a <pre> tag workaround.
        // otherwise the text inside "pre" loses all the line breaks!
        if (selection.getRangeAt(0).commonAncestorContainer.nodeName === 'PRE' || selection.getRangeAt(0).commonAncestorContainer.nodeName === 'CODE') {
            newdiv.innerHTML = "<pre>" + newdiv.innerHTML + "</pre>";
        }

        var url = document.location.href;
        newdiv.innerHTML += '<br />'
            + '来源: Sky&#39;s Blogs<br />'
            + '文章作者: Sky<br />'
            + '文章链接: <a href="' + url + '">' + url + '</a><br />'
            + '本文章著作权归作者所有，任何形式的转载都请注明出处。';

        selection.selectAllChildren(newdiv);
        window.setTimeout(function () {bodyElement.removeChild(newdiv);}, 200);
    });
</script>


<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>


  <!-- 是否加载使用自带的 prismjs. -->
  <script type="text/javascript" src="/libs/prism/prism.min.js"></script>


<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="song"
                   id="26545309"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2014-2025</span>
            
            <a href="/about" target="_blank">Sky</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
            
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2014";
                        var startMonth = "7";
                        var startDate = "3";
                        var startHour = "20";
                        var startMinute = "45";
                        var startSecond = "27";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link ">
    <a href="https://github.com/flyinskyin2013" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>















    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    

    
    
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
     
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/libs/others/star.js"><\/script>');
            }
        </script>
    

    

    <!-- Baidu Analytics -->

<script>
    var _hmt = _hmt || [];
    (function () {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?33f6329a2e264bb97f4f37ee8ddec093";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
    })();
</script>

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"position":"right","width":150,"height":250,"hOffset":50,"vOffset":5},"mobile":{"show":false,"scale":0.5},"react":{"opacityDefault":0.7,"opacityOnHover":0.8},"log":false});</script></body>

</html>
