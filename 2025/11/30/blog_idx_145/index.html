<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/7.0.0/css/all.min.css" integrity="sha256-VHqXKFhhMxcpubYf9xiWdCiojEbY9NexQ4jh8AxbvcM=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"e-x.top","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.27.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":false,"codeblock":{"theme":{"light":"default","dark":"stackoverflow-dark"},"prism":{"light":"prism","dark":"prism-dark"},"copy_button":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"language":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js" defer></script>

    <meta name="description" content="PS：要转载请注明出处，本人版权所有。 PS: 这个只是基于《我自己》的理解，如果和你的原则及想法相冲突，请谅解，勿喷。  环境说明   无 前言     本文是这个系列第八篇，也是本系列的终章，它们是：   《大模型基础补全计划(一)—重温一些深度学习相关的数学知识》 https:&#x2F;&#x2F;www.cnblogs.com&#x2F;Iflyinsky&#x2F;p&#x2F;18717317   《大模型基础补全计划(二)—词">
<meta property="og:type" content="article">
<meta property="og:title" content="大模型基础补全计划(八)---相关知识点回顾与Qwen3-VL-2B-Instruct实例分析(终章)">
<meta property="og:url" content="https://e-x.top/2025/11/30/blog_idx_145/index.html">
<meta property="og:site_name" content="Sky&#39;s Blogs">
<meta property="og:description" content="PS：要转载请注明出处，本人版权所有。 PS: 这个只是基于《我自己》的理解，如果和你的原则及想法相冲突，请谅解，勿喷。  环境说明   无 前言     本文是这个系列第八篇，也是本系列的终章，它们是：   《大模型基础补全计划(一)—重温一些深度学习相关的数学知识》 https:&#x2F;&#x2F;www.cnblogs.com&#x2F;Iflyinsky&#x2F;p&#x2F;18717317   《大模型基础补全计划(二)—词">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://flyinskyin2013.github.io/ImageBed0/blogs/qrcode_for_wx_official_account.jpg">
<meta property="article:published_time" content="2025-11-30T03:07:00.000Z">
<meta property="article:modified_time" content="2025-11-30T03:11:30.801Z">
<meta property="article:author" content="Sky">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="LM">
<meta property="article:tag" content="attention">
<meta property="article:tag" content="transformer">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://flyinskyin2013.github.io/ImageBed0/blogs/qrcode_for_wx_official_account.jpg">


<link rel="canonical" href="https://e-x.top/2025/11/30/blog_idx_145/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://e-x.top/2025/11/30/blog_idx_145/","path":"2025/11/30/blog_idx_145/","title":"大模型基础补全计划(八)---相关知识点回顾与Qwen3-VL-2B-Instruct实例分析(终章)"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>大模型基础补全计划(八)---相关知识点回顾与Qwen3-VL-2B-Instruct实例分析(终章) | Sky's Blogs</title>
  








  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
<script src="/js/utils.js" defer></script><script src="/js/motion.js" defer></script><script src="/js/sidebar.js" defer></script><script src="/js/next-boot.js" defer></script>

  






  




<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8677552300382028"
     crossorigin="anonymous"></script>
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Sky's Blogs</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">A normal star</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>







</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E7%8E%AF%E5%A2%83%E8%AF%B4%E6%98%8E"><span class="nav-number">1.</span> <span class="nav-text">环境说明</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-number"></span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Qwen3-VL-2B-Instruct-%E7%AE%80%E4%BB%8B"><span class="nav-number"></span> <span class="nav-text">Qwen3-VL-2B-Instruct 简介</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%8B%E8%BD%BD%E5%8F%8A%E8%BF%90%E8%A1%8C"><span class="nav-number"></span> <span class="nav-text">下载及运行</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84"><span class="nav-number"></span> <span class="nav-text">模型结构</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Qwen3-VL-2B-Instruct-%E7%9A%84%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E7%AE%80%E5%8D%95%E5%88%86%E6%9E%90-%E5%8F%8A-%E7%9F%A5%E8%AF%86%E5%9B%9E%E9%A1%BE"><span class="nav-number"></span> <span class="nav-text">Qwen3-VL-2B-Instruct 的模型结构简单分析 及 知识回顾</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#visual-%E9%83%A8%E5%88%86%E7%AE%80%E5%8D%95%E5%88%86%E6%9E%90"><span class="nav-number"></span> <span class="nav-text">visual 部分简单分析</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#language-model-%E9%83%A8%E5%88%86%E5%88%86%E6%9E%90"><span class="nav-number"></span> <span class="nav-text">language_model 部分分析</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%8E%E8%AE%B0"><span class="nav-number"></span> <span class="nav-text">后记</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="nav-number"></span> <span class="nav-text">参考文献</span></a></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Sky</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">139</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">92</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">222</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://e-x.top/2025/11/30/blog_idx_145/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Sky">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sky's Blogs">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="大模型基础补全计划(八)---相关知识点回顾与Qwen3-VL-2B-Instruct实例分析(终章) | Sky's Blogs">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          大模型基础补全计划(八)---相关知识点回顾与Qwen3-VL-2B-Instruct实例分析(终章)
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-11-30 11:07:00 / 修改时间：11:11:30" itemprop="dateCreated datePublished" datetime="2025-11-30T11:07:00+08:00">2025-11-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/LLM/" itemprop="url" rel="index"><span itemprop="name">LLM</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/LLM/LM/" itemprop="url" rel="index"><span itemprop="name">LM</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>13 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><script src="\assets\js\APlayer.min.js"> </script><!--
 * @Description: 
 * @Author: Sky
 * @Date: 2020-08-24 16:37:34
 * @LastEditors: Sky
 * @LastEditTime: 2021-06-29 15:04:48
 * @Github: https://github.com/flyinskyin2013/
-->
<p><font color="red" size="7">PS：要转载请注明出处，本人版权所有。</font><br/><br>
<font color="red" size="7">PS: 这个只是基于《我自己》的理解，</font><br/><font color="red" size="7">如果和你的原则及想法相冲突，请谅解，勿喷。</font><br/></p>
<!-- ###### 前置说明
&emsp;&emsp;本文作为本人github blog/cnblog的主站的备份。（BlogID=120） 
&emsp;&emsp;本文发布于 2025-11-30 11:07:00             （BlogID=120） 
-->
<h6 id="环境说明">环境说明</h6>
<p>  无</p>
<h3 id="前言">前言</h3>
<hr>
<p>   本文是这个系列第八篇，也是本系列的终章，它们是：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>《大模型基础补全计划(一)—重温一些深度学习相关的数学知识》 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/Iflyinsky/p/18717317">https://www.cnblogs.com/Iflyinsky/p/18717317</a></p>
</li>
<li class="lvl-2">
<p>《大模型基础补全计划(二)—词嵌入(word embedding) 》 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/Iflyinsky/p/18775451">https://www.cnblogs.com/Iflyinsky/p/18775451</a></p>
</li>
<li class="lvl-2">
<p>《大模型基础补全计划(三)—RNN实例与测试》 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/Iflyinsky/p/18967569">https://www.cnblogs.com/Iflyinsky/p/18967569</a></p>
</li>
<li class="lvl-2">
<p>《大模型基础补全计划(四)—LSTM的实例与测试(RNN的改进)》 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/Iflyinsky/p/19091089">https://www.cnblogs.com/Iflyinsky/p/19091089</a></p>
</li>
<li class="lvl-2">
<p>《大模型基础补全计划(五)—seq2seq实例与测试(编码器、解码器架构)》 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/Iflyinsky/p/19150535">https://www.cnblogs.com/Iflyinsky/p/19150535</a></p>
</li>
<li class="lvl-2">
<p>《大模型基础补全计划(六)—带注意力机制的seq2seq实例与测试(Bahdanau Attention)》 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/Iflyinsky/p/19184558">https://www.cnblogs.com/Iflyinsky/p/19184558</a></p>
</li>
<li class="lvl-2">
<p>《大模型基础补全计划(七)—Transformer(多头注意力、自注意力、位置编码)及实例与测试》<a target="_blank" rel="noopener" href="https://www.cnblogs.com/Iflyinsky/p/19228410">https://www.cnblogs.com/Iflyinsky/p/19228410</a></p>
</li>
</ul>
<p>  本文主要是用一个实际的大模型例子来联系和回顾之前的知识点，让大家能够感受一些，前面文中的一些知识点是真正用到了实际大模型里面的哪些地方。</p>
<p>  由于近期正在学习和应用的Qwen3-VL系列相关模型，因此这里挑了一个Qwen3-VL-2B-Instruct来独立分析，并联系和回顾之前的知识点。</p>
<p>  注意：本文不会详细介绍Qwen3-VL-2B-Instruct的推理过程及原理，如果想学习详细的技术原理，请忽略本文内容，并查看其它相关的文章。</p>
<br/>
<br/>
<br/>
<br/>
<h3 id="Qwen3-VL-2B-Instruct-简介">Qwen3-VL-2B-Instruct 简介</h3>
<hr>
<p>  </p>
<br/>
<br/>
<h5 id="下载及运行">下载及运行</h5>
<p>   首先qwen3-vl的官方工程是 <a target="_blank" rel="noopener" href="https://github.com/QwenLM/Qwen3-VL">https://github.com/QwenLM/Qwen3-VL</a> ，下面的官方示例的下载及变更推理代码（由于国内的原因，从魔塔下载）：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">modelscope download <span class="token parameter variable">--model</span> Qwen/Qwen3-VL-2B-Instruct  <span class="token parameter variable">--local_dir</span> ./cache<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModelForImageTextToText<span class="token punctuation">,</span> AutoProcessor

model_path <span class="token operator">=</span> <span class="token string">"./cache"</span>

<span class="token comment"># default: Load the model on the available device(s)</span>
model <span class="token operator">=</span> AutoModelForImageTextToText<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
    model_path<span class="token punctuation">,</span> cache_dir<span class="token operator">=</span>model_path<span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token string">"auto"</span><span class="token punctuation">,</span> device_map<span class="token operator">=</span><span class="token string">"auto"</span>
<span class="token punctuation">)</span>

<span class="token comment"># We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.</span>
<span class="token comment"># model = AutoModelForImageTextToText.from_pretrained(</span>
<span class="token comment">#     "Qwen/Qwen3-VL-235B-A22B-Instruct",</span>
<span class="token comment">#     dtype=torch.bfloat16,</span>
<span class="token comment">#     attn_implementation="flash_attention_2",</span>
<span class="token comment">#     device_map="auto",</span>
<span class="token comment"># )</span>

processor <span class="token operator">=</span> AutoProcessor<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_path<span class="token punctuation">,</span> cache_dir<span class="token operator">=</span>model_path<span class="token punctuation">)</span>

messages <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token punctuation">&#123;</span>
        <span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"user"</span><span class="token punctuation">,</span>
        <span class="token string">"content"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>
            <span class="token punctuation">&#123;</span>
                <span class="token string">"type"</span><span class="token punctuation">:</span> <span class="token string">"image"</span><span class="token punctuation">,</span>
                <span class="token string">"image"</span><span class="token punctuation">:</span> <span class="token string">"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg"</span><span class="token punctuation">,</span>
            <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>
            <span class="token punctuation">&#123;</span><span class="token string">"type"</span><span class="token punctuation">:</span> <span class="token string">"text"</span><span class="token punctuation">,</span> <span class="token string">"text"</span><span class="token punctuation">:</span> <span class="token string">"Describe this image."</span><span class="token punctuation">&#125;</span><span class="token punctuation">,</span>
        <span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">&#125;</span>
<span class="token punctuation">]</span>

<span class="token comment"># Preparation for inference</span>
inputs <span class="token operator">=</span> processor<span class="token punctuation">.</span>apply_chat_template<span class="token punctuation">(</span>
    messages<span class="token punctuation">,</span>
    tokenize<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    add_generation_prompt<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    return_dict<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    return_tensors<span class="token operator">=</span><span class="token string">"pt"</span>
<span class="token punctuation">)</span>
inputs <span class="token operator">=</span> inputs<span class="token punctuation">.</span>to<span class="token punctuation">(</span>model<span class="token punctuation">.</span>device<span class="token punctuation">)</span>

<span class="token comment"># Inference: Generation of the output</span>
generated_ids <span class="token operator">=</span> model<span class="token punctuation">.</span>generate<span class="token punctuation">(</span><span class="token operator">**</span>inputs<span class="token punctuation">,</span> max_new_tokens<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">)</span>
generated_ids_trimmed <span class="token operator">=</span> <span class="token punctuation">[</span>
    out_ids<span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>in_ids<span class="token punctuation">)</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token keyword">for</span> in_ids<span class="token punctuation">,</span> out_ids <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>inputs<span class="token punctuation">.</span>input_ids<span class="token punctuation">,</span> generated_ids<span class="token punctuation">)</span>
<span class="token punctuation">]</span>
output_text <span class="token operator">=</span> processor<span class="token punctuation">.</span>batch_decode<span class="token punctuation">(</span>
    generated_ids_trimmed<span class="token punctuation">,</span> skip_special_tokens<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> clean_up_tokenization_spaces<span class="token operator">=</span><span class="token boolean">False</span>
<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>output_text<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<br/>
<br/>
<br/>
<br/>
<h5 id="模型结构">模型结构</h5>
<p>   我们在上面的例子基础上，添加如下代码打印其模型结构：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">)</span>
vit_model <span class="token operator">=</span> model<span class="token punctuation">.</span>visual
llm_model <span class="token operator">=</span> model<span class="token punctuation">.</span>language_model <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>  得到的模型结构如下：</p>
<pre class="line-numbers language-none"><code class="language-none">Qwen3VLForConditionalGeneration(
  (model): Qwen3VLModel(
    (visual): Qwen3VLVisionModel(
      (patch_embed): Qwen3VLVisionPatchEmbed(
        (proj): Conv3d(3, 1024, kernel_size&#x3D;(2, 16, 16), stride&#x3D;(2, 16, 16))
      )
      (pos_embed): Embedding(2304, 1024)
      (rotary_pos_emb): Qwen3VLVisionRotaryEmbedding()
      (blocks): ModuleList(
        (0-23): 24 x Qwen3VLVisionBlock(
          (norm1): LayerNorm((1024,), eps&#x3D;1e-06, elementwise_affine&#x3D;True)
          (norm2): LayerNorm((1024,), eps&#x3D;1e-06, elementwise_affine&#x3D;True)
          (attn): Qwen3VLVisionAttention(
            (qkv): Linear(in_features&#x3D;1024, out_features&#x3D;3072, bias&#x3D;True)
            (proj): Linear(in_features&#x3D;1024, out_features&#x3D;1024, bias&#x3D;True)
          )
          (mlp): Qwen3VLVisionMLP(
            (linear_fc1): Linear(in_features&#x3D;1024, out_features&#x3D;4096, bias&#x3D;True)
            (linear_fc2): Linear(in_features&#x3D;4096, out_features&#x3D;1024, bias&#x3D;True)
            (act_fn): GELUTanh()
          )
        )
      )
      (merger): Qwen3VLVisionPatchMerger(
        (norm): LayerNorm((1024,), eps&#x3D;1e-06, elementwise_affine&#x3D;True)
        (linear_fc1): Linear(in_features&#x3D;4096, out_features&#x3D;4096, bias&#x3D;True)
        (act_fn): GELU(approximate&#x3D;&#39;none&#39;)
        (linear_fc2): Linear(in_features&#x3D;4096, out_features&#x3D;2048, bias&#x3D;True)
      )
      (deepstack_merger_list): ModuleList(
        (0-2): 3 x Qwen3VLVisionPatchMerger(
          (norm): LayerNorm((4096,), eps&#x3D;1e-06, elementwise_affine&#x3D;True)
          (linear_fc1): Linear(in_features&#x3D;4096, out_features&#x3D;4096, bias&#x3D;True)
          (act_fn): GELU(approximate&#x3D;&#39;none&#39;)
          (linear_fc2): Linear(in_features&#x3D;4096, out_features&#x3D;2048, bias&#x3D;True)
        )
      )
    )
    (language_model): Qwen3VLTextModel(
      (embed_tokens): Embedding(151936, 2048)
      (layers): ModuleList(
        (0-27): 28 x Qwen3VLTextDecoderLayer(
          (self_attn): Qwen3VLTextAttention(
            (q_proj): Linear(in_features&#x3D;2048, out_features&#x3D;2048, bias&#x3D;False)
            (k_proj): Linear(in_features&#x3D;2048, out_features&#x3D;1024, bias&#x3D;False)
            (v_proj): Linear(in_features&#x3D;2048, out_features&#x3D;1024, bias&#x3D;False)
            (o_proj): Linear(in_features&#x3D;2048, out_features&#x3D;2048, bias&#x3D;False)
            (q_norm): Qwen3VLTextRMSNorm((128,), eps&#x3D;1e-06)
            (k_norm): Qwen3VLTextRMSNorm((128,), eps&#x3D;1e-06)
          )
          (mlp): Qwen3VLTextMLP(
            (gate_proj): Linear(in_features&#x3D;2048, out_features&#x3D;6144, bias&#x3D;False)
            (up_proj): Linear(in_features&#x3D;2048, out_features&#x3D;6144, bias&#x3D;False)
            (down_proj): Linear(in_features&#x3D;6144, out_features&#x3D;2048, bias&#x3D;False)
            (act_fn): SiLUActivation()
          )
          (input_layernorm): Qwen3VLTextRMSNorm((2048,), eps&#x3D;1e-06)
          (post_attention_layernorm): Qwen3VLTextRMSNorm((2048,), eps&#x3D;1e-06)
        )
      )
      (norm): Qwen3VLTextRMSNorm((2048,), eps&#x3D;1e-06)
      (rotary_emb): Qwen3VLTextRotaryEmbedding()
    )
  )
  (lm_head): Linear(in_features&#x3D;2048, out_features&#x3D;151936, bias&#x3D;False)
)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>  从上面的模型结构来看，我们可以知道其分为两个部分，一个是visual，一个是language_model，这也是现在的视觉多模态的常见结构。</p>
<br/>
<br/>
<br/>
<br/>
<h3 id="Qwen3-VL-2B-Instruct-的模型结构简单分析-及-知识回顾">Qwen3-VL-2B-Instruct 的模型结构简单分析 及 知识回顾</h3>
<hr>
<p>  还记得我们前面的模型中的词表这个概念吗？当时的做法是直接将将整个训练用到的文字映射成对应的id，将所有的id组合在一起作为一个词表。在现在的大模型中，其实就有类似的东西，一般放在tokenizer.json文件里面。对于当前这个模型来说，这里有几个特殊的东西说明一下：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>以前文章中的&lt;bos&gt;/&lt;eos&gt;对应的是当前这个模型的&lt;|im_start|&gt;/&lt;|im_end|&gt;</p>
</li>
<li class="lvl-2">
<p>由于是视觉多模态模型，当前这个模型还会有几个本文会用到的特殊token：&lt;|vision_start|&gt;/&lt;|image_pad|&gt;/&lt;|vision_end|&gt;，他们是用来描述一张图怎么被输入到大语言模型中被理解的。</p>
</li>
<li class="lvl-2">
<p>一个token不一定对应一个文字，可能对应多个、或者零点几个字，感兴趣可以私下了解一下，其和文字编码有关系。</p>
</li>
</ul>
<p>  当上文的 processor.apply_chat_template执行后，然后得到的inputs会有如下四个内容：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>input_ids （做完tokenizer之后的输出，已经将输入的文字“Describe this image.”和图片占位符“&lt;|vision_start|&gt;&lt;|image_pad|&gt;*N&lt;|vision_end|&gt;”转换为了对应的token id）</p>
</li>
<li class="lvl-2">
<p>attention_mask （input_ids的掩码，用于屏蔽无效或者pad输入序列）</p>
</li>
<li class="lvl-2">
<p>pixel_values （图片预处理好的矩阵，不仅仅做了归一化，还做了分patch操作，本文不用太关注）</p>
</li>
<li class="lvl-2">
<p>image_grid_thw （本文用不上，别管。）</p>
</li>
</ul>
<p>  对于input_ids来说，我们知道里面有图片的占位符的token_id，这里后面会替换为真实的图像数据，这样才能把图、文字送入到大语言模型，当然，语音等也是一样的。</p>
<p>  我们首先来看看上文model.generate调用之后发生了什么，他会经过一系列变化后，到达如下的Qwen3VLModel的forward的入口：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>
    self<span class="token punctuation">,</span>
    input_ids<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>LongTensor <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    attention_mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    position_ids<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    past_key_values<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Cache<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    inputs_embeds<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    pixel_values<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    pixel_values_videos<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    image_grid_thw<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    video_grid_thw<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    cache_position<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    <span class="token operator">**</span>kwargs<span class="token punctuation">:</span> Unpack<span class="token punctuation">[</span>TransformersKwargs<span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> Union<span class="token punctuation">[</span><span class="token builtin">tuple</span><span class="token punctuation">,</span> Qwen3VLModelOutputWithPast<span class="token punctuation">]</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">r"""
    image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):
        The temporal, height and width of feature shape of each image in LLM.
    video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):
        The temporal, height and width of feature shape of each video in LLM.
    """</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>input_ids <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">)</span> <span class="token operator">^</span> <span class="token punctuation">(</span>inputs_embeds <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"You must specify exactly one of input_ids or inputs_embeds"</span><span class="token punctuation">)</span>

    <span class="token keyword">if</span> inputs_embeds <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        inputs_embeds <span class="token operator">=</span> self<span class="token punctuation">.</span>get_input_embeddings<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span>

    image_mask <span class="token operator">=</span> <span class="token boolean">None</span>
    video_mask <span class="token operator">=</span> <span class="token boolean">None</span>

    <span class="token keyword">if</span> pixel_values <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        image_embeds<span class="token punctuation">,</span> deepstack_image_embeds <span class="token operator">=</span> self<span class="token punctuation">.</span>get_image_features<span class="token punctuation">(</span>pixel_values<span class="token punctuation">,</span> image_grid_thw<span class="token punctuation">)</span>
        image_embeds <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>image_embeds<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>inputs_embeds<span class="token punctuation">.</span>device<span class="token punctuation">,</span> inputs_embeds<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>
        image_mask<span class="token punctuation">,</span> _ <span class="token operator">=</span> self<span class="token punctuation">.</span>get_placeholder_mask<span class="token punctuation">(</span>
            input_ids<span class="token punctuation">,</span> inputs_embeds<span class="token operator">=</span>inputs_embeds<span class="token punctuation">,</span> image_features<span class="token operator">=</span>image_embeds
        <span class="token punctuation">)</span>
        inputs_embeds <span class="token operator">=</span> inputs_embeds<span class="token punctuation">.</span>masked_scatter<span class="token punctuation">(</span>image_mask<span class="token punctuation">,</span> image_embeds<span class="token punctuation">)</span>

    <span class="token keyword">if</span> pixel_values_videos <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        video_embeds<span class="token punctuation">,</span> deepstack_video_embeds <span class="token operator">=</span> self<span class="token punctuation">.</span>get_video_features<span class="token punctuation">(</span>pixel_values_videos<span class="token punctuation">,</span> video_grid_thw<span class="token punctuation">)</span>
        video_embeds <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>video_embeds<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>inputs_embeds<span class="token punctuation">.</span>device<span class="token punctuation">,</span> inputs_embeds<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>
        _<span class="token punctuation">,</span> video_mask <span class="token operator">=</span> self<span class="token punctuation">.</span>get_placeholder_mask<span class="token punctuation">(</span>
            input_ids<span class="token punctuation">,</span> inputs_embeds<span class="token operator">=</span>inputs_embeds<span class="token punctuation">,</span> video_features<span class="token operator">=</span>video_embeds
        <span class="token punctuation">)</span>
        inputs_embeds <span class="token operator">=</span> inputs_embeds<span class="token punctuation">.</span>masked_scatter<span class="token punctuation">(</span>video_mask<span class="token punctuation">,</span> video_embeds<span class="token punctuation">)</span>

    visual_pos_masks <span class="token operator">=</span> <span class="token boolean">None</span>
    deepstack_visual_embeds <span class="token operator">=</span> <span class="token boolean">None</span>
    <span class="token keyword">if</span> image_mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">and</span> video_mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        <span class="token comment"># aggregate visual_pos_masks and deepstack_visual_embeds</span>
        image_mask <span class="token operator">=</span> image_mask<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>
        video_mask <span class="token operator">=</span> video_mask<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>
        visual_pos_masks <span class="token operator">=</span> image_mask <span class="token operator">|</span> video_mask
        deepstack_visual_embeds <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        image_mask_joint <span class="token operator">=</span> image_mask<span class="token punctuation">[</span>visual_pos_masks<span class="token punctuation">]</span>
        video_mask_joint <span class="token operator">=</span> video_mask<span class="token punctuation">[</span>visual_pos_masks<span class="token punctuation">]</span>
        <span class="token keyword">for</span> img_embed<span class="token punctuation">,</span> vid_embed <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>deepstack_image_embeds<span class="token punctuation">,</span> deepstack_video_embeds<span class="token punctuation">)</span><span class="token punctuation">:</span>
            embed_joint <span class="token operator">=</span> img_embed<span class="token punctuation">.</span>new_zeros<span class="token punctuation">(</span>visual_pos_masks<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> img_embed<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>img_embed<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
            embed_joint<span class="token punctuation">[</span>image_mask_joint<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> img_embed
            embed_joint<span class="token punctuation">[</span>video_mask_joint<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> vid_embed
            deepstack_visual_embeds<span class="token punctuation">.</span>append<span class="token punctuation">(</span>embed_joint<span class="token punctuation">)</span>
    <span class="token keyword">elif</span> image_mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        image_mask <span class="token operator">=</span> image_mask<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>
        visual_pos_masks <span class="token operator">=</span> image_mask
        deepstack_visual_embeds <span class="token operator">=</span> deepstack_image_embeds
    <span class="token keyword">elif</span> video_mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        video_mask <span class="token operator">=</span> video_mask<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>
        visual_pos_masks <span class="token operator">=</span> video_mask
        deepstack_visual_embeds <span class="token operator">=</span> deepstack_video_embeds

    <span class="token keyword">if</span> position_ids <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        attention_mask_tensor <span class="token operator">=</span> <span class="token punctuation">(</span>
            attention_mask <span class="token keyword">if</span> <span class="token keyword">not</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>attention_mask<span class="token punctuation">,</span> <span class="token builtin">dict</span><span class="token punctuation">)</span> <span class="token keyword">else</span> attention_mask<span class="token punctuation">[</span><span class="token string">"full_attention"</span><span class="token punctuation">]</span>
        <span class="token punctuation">)</span>
        <span class="token keyword">if</span> attention_mask_tensor <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">and</span> attention_mask_tensor<span class="token punctuation">.</span>ndim <span class="token operator">==</span> <span class="token number">4</span><span class="token punctuation">:</span>
            attention_mask_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>diagonal<span class="token punctuation">(</span>attention_mask_tensor<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dim1<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> dim2<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
            <span class="token comment"># Only apply conversion for floating point tensors (inverted masks)</span>
            <span class="token keyword">if</span> attention_mask_tensor<span class="token punctuation">.</span>dtype<span class="token punctuation">.</span>is_floating_point<span class="token punctuation">:</span>
                attention_mask_tensor <span class="token operator">=</span> attention_mask_tensor <span class="token operator">/</span> torch<span class="token punctuation">.</span>finfo<span class="token punctuation">(</span>attention_mask_tensor<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">min</span>
                attention_mask_tensor <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">-</span> attention_mask_tensor<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">int</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token comment"># Calculate RoPE index once per generation in the pre-fill stage only.</span>
        <span class="token comment"># When compiling, we can't check tensor values thus we check only input length</span>
        <span class="token comment"># It is safe to assume that `length!=1` means we're in pre-fill because compiled</span>
        <span class="token comment"># models currently cannot do asssisted decoding</span>
        prefill_compiled_stage <span class="token operator">=</span> is_torchdynamo_compiling<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">and</span> <span class="token punctuation">(</span>
            <span class="token punctuation">(</span>input_ids <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">and</span> input_ids<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">!=</span> <span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token keyword">or</span> <span class="token punctuation">(</span>inputs_embeds <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">and</span> inputs_embeds<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">!=</span> <span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
        prefill_noncompiled_stage <span class="token operator">=</span> <span class="token keyword">not</span> is_torchdynamo_compiling<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">and</span> <span class="token punctuation">(</span>
            <span class="token punctuation">(</span>cache_position <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">and</span> cache_position<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span>
            <span class="token keyword">or</span> <span class="token punctuation">(</span>past_key_values <span class="token keyword">is</span> <span class="token boolean">None</span> <span class="token keyword">or</span> past_key_values<span class="token punctuation">.</span>get_seq_length<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
        <span class="token keyword">if</span> <span class="token punctuation">(</span>prefill_compiled_stage <span class="token keyword">or</span> prefill_noncompiled_stage<span class="token punctuation">)</span> <span class="token keyword">or</span> self<span class="token punctuation">.</span>rope_deltas <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            position_ids<span class="token punctuation">,</span> rope_deltas <span class="token operator">=</span> self<span class="token punctuation">.</span>get_rope_index<span class="token punctuation">(</span>
                input_ids<span class="token punctuation">,</span>
                image_grid_thw<span class="token punctuation">,</span>
                video_grid_thw<span class="token punctuation">,</span>
                attention_mask<span class="token operator">=</span>attention_mask_tensor<span class="token punctuation">,</span>
            <span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>rope_deltas <span class="token operator">=</span> rope_deltas
        <span class="token comment"># then use the prev pre-calculated rope-deltas to get the correct position ids</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            batch_size<span class="token punctuation">,</span> seq_length<span class="token punctuation">,</span> _ <span class="token operator">=</span> inputs_embeds<span class="token punctuation">.</span>shape
            delta <span class="token operator">=</span> <span class="token punctuation">(</span>
                <span class="token punctuation">(</span>cache_position<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>rope_deltas<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>inputs_embeds<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
                <span class="token keyword">if</span> cache_position <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span>
                <span class="token keyword">else</span> <span class="token number">0</span>
            <span class="token punctuation">)</span>
            position_ids <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>seq_length<span class="token punctuation">,</span> device<span class="token operator">=</span>inputs_embeds<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
            position_ids <span class="token operator">=</span> position_ids<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token keyword">if</span> cache_position <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>  <span class="token comment"># otherwise `deltas` is an int `0`</span>
                delta <span class="token operator">=</span> delta<span class="token punctuation">.</span>repeat_interleave<span class="token punctuation">(</span>batch_size <span class="token operator">//</span> delta<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
            position_ids <span class="token operator">=</span> position_ids<span class="token punctuation">.</span>add<span class="token punctuation">(</span>delta<span class="token punctuation">)</span>
            position_ids <span class="token operator">=</span> position_ids<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

    outputs <span class="token operator">=</span> self<span class="token punctuation">.</span>language_model<span class="token punctuation">(</span>
        input_ids<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        position_ids<span class="token operator">=</span>position_ids<span class="token punctuation">,</span>
        attention_mask<span class="token operator">=</span>attention_mask<span class="token punctuation">,</span>
        past_key_values<span class="token operator">=</span>past_key_values<span class="token punctuation">,</span>
        inputs_embeds<span class="token operator">=</span>inputs_embeds<span class="token punctuation">,</span>
        cache_position<span class="token operator">=</span>cache_position<span class="token punctuation">,</span>
        visual_pos_masks<span class="token operator">=</span>visual_pos_masks<span class="token punctuation">,</span>
        deepstack_visual_embeds<span class="token operator">=</span>deepstack_visual_embeds<span class="token punctuation">,</span>
        <span class="token operator">**</span>kwargs<span class="token punctuation">,</span>
    <span class="token punctuation">)</span>

    <span class="token keyword">return</span> Qwen3VLModelOutputWithPast<span class="token punctuation">(</span>
        last_hidden_state<span class="token operator">=</span>outputs<span class="token punctuation">.</span>last_hidden_state<span class="token punctuation">,</span>
        past_key_values<span class="token operator">=</span>outputs<span class="token punctuation">.</span>past_key_values<span class="token punctuation">,</span>
        rope_deltas<span class="token operator">=</span>self<span class="token punctuation">.</span>rope_deltas<span class="token punctuation">,</span>
    <span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>   看上面的代码，我们来看看 input_ids 中的主要的几个数据分别做了什么：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>input_ids 通过get_input_embeddings获取了input_ids对应的原始inputs_embeds，这一步和我们以前文章中做embedding是一样的。唯一注意的，这里的embedding向量里面包含&lt;|image_pad|&gt;对应的嵌入向量，是占位的，后面要替换为真实的数据。</p>
</li>
<li class="lvl-2">
<p>pixel_values 通过get_image_features获取了图像数据对应的image_embeds，这里对应Qwen3VLVisionModel的推理过程，下面会简单说明一下。</p>
</li>
<li class="lvl-2">
<p>在masked_scatter中，将inputs_embeds中的占位向量替换为image_embeds。</p>
</li>
<li class="lvl-2">
<p>根据输入的inputs_embeds，获取token对应的position_ids，也就是获取位置信息，在前面的文中提到了为什么transformer需要位置信息。</p>
</li>
<li class="lvl-2">
<p>将最终的position_ids，attention_mask，inputs_embeds，past_key_values（此项内容在下文解释）给Qwen3VLTextModel进行推理得到logits序列</p>
</li>
<li class="lvl-2">
<p>然后将logits按采样参数进行采样，得到最终的输出的文字token,然后进行tokenizer解码，得到最终输出的文字。(此部分不在上面所在代码范围内部，但是是大模型的后处理部分的必要逻辑部分。)</p>
</li>
</ul>
<p>   我们从上文已经知道，其模型分为两个部分，下面分别简单介绍这两部分的forward过程，看看我们之前提到的知识点在真实的多模态大模型中是怎么样的存在。</p>
<br/>
<br/>
<h5 id="visual-部分简单分析">visual 部分简单分析</h5>
<p>  本系列文章严格来说是不应该涉及到多模态大模型的，但是现在常见的多模态大模型应用场景已经逐渐扩大，因此这里用视觉多模态大模型为例子，看看视觉多模态大模型和普通的大模型有什么区别，首先visual部分的forward代码如下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> hidden_states<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> grid_thw<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Args:
        hidden_states (`torch.Tensor` of shape `(seq_len, hidden_size)`):
            The final hidden states of the model.
        grid_thw (`torch.Tensor` of shape `(num_images_or_videos, 3)`):
            The temporal, height and width of feature shape of each image in LLM.

    Returns:
        `torch.Tensor`: hidden_states.
    """</span>
    hidden_states <span class="token operator">=</span> self<span class="token punctuation">.</span>patch_embed<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>

    pos_embeds <span class="token operator">=</span> self<span class="token punctuation">.</span>fast_pos_embed_interpolate<span class="token punctuation">(</span>grid_thw<span class="token punctuation">)</span>
    hidden_states <span class="token operator">=</span> hidden_states <span class="token operator">+</span> pos_embeds

    rotary_pos_emb <span class="token operator">=</span> self<span class="token punctuation">.</span>rot_pos_emb<span class="token punctuation">(</span>grid_thw<span class="token punctuation">)</span>

    seq_len<span class="token punctuation">,</span> _ <span class="token operator">=</span> hidden_states<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>
    hidden_states <span class="token operator">=</span> hidden_states<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>seq_len<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    rotary_pos_emb <span class="token operator">=</span> rotary_pos_emb<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>seq_len<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    emb <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>rotary_pos_emb<span class="token punctuation">,</span> rotary_pos_emb<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    position_embeddings <span class="token operator">=</span> <span class="token punctuation">(</span>emb<span class="token punctuation">.</span>cos<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> emb<span class="token punctuation">.</span>sin<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    cu_seqlens <span class="token operator">=</span> torch<span class="token punctuation">.</span>repeat_interleave<span class="token punctuation">(</span>grid_thw<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">*</span> grid_thw<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> grid_thw<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cumsum<span class="token punctuation">(</span>
        dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>
        <span class="token comment"># Select dtype based on the following factors:</span>
        <span class="token comment">#  - FA2 requires that cu_seqlens_q must have dtype int32</span>
        <span class="token comment">#  - torch.onnx.export requires that cu_seqlens_q must have same dtype as grid_thw</span>
        <span class="token comment"># See https://github.com/huggingface/transformers/pull/34852 for more information</span>
        dtype<span class="token operator">=</span>grid_thw<span class="token punctuation">.</span>dtype <span class="token keyword">if</span> torch<span class="token punctuation">.</span>jit<span class="token punctuation">.</span>is_tracing<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> torch<span class="token punctuation">.</span>int32<span class="token punctuation">,</span>
    <span class="token punctuation">)</span>
    cu_seqlens <span class="token operator">=</span> F<span class="token punctuation">.</span>pad<span class="token punctuation">(</span>cu_seqlens<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> value<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>

    deepstack_feature_lists <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> layer_num<span class="token punctuation">,</span> blk <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>blocks<span class="token punctuation">)</span><span class="token punctuation">:</span>
        hidden_states <span class="token operator">=</span> blk<span class="token punctuation">(</span>
            hidden_states<span class="token punctuation">,</span>
            cu_seqlens<span class="token operator">=</span>cu_seqlens<span class="token punctuation">,</span>
            position_embeddings<span class="token operator">=</span>position_embeddings<span class="token punctuation">,</span>
            <span class="token operator">**</span>kwargs<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
        <span class="token keyword">if</span> layer_num <span class="token keyword">in</span> self<span class="token punctuation">.</span>deepstack_visual_indexes<span class="token punctuation">:</span>
            deepstack_feature <span class="token operator">=</span> self<span class="token punctuation">.</span>deepstack_merger_list<span class="token punctuation">[</span>self<span class="token punctuation">.</span>deepstack_visual_indexes<span class="token punctuation">.</span>index<span class="token punctuation">(</span>layer_num<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">(</span>
                hidden_states
            <span class="token punctuation">)</span>
            deepstack_feature_lists<span class="token punctuation">.</span>append<span class="token punctuation">(</span>deepstack_feature<span class="token punctuation">)</span>

    hidden_states <span class="token operator">=</span> self<span class="token punctuation">.</span>merger<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>

    <span class="token keyword">return</span> hidden_states<span class="token punctuation">,</span> deepstack_feature_lists<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>  由于本文并不是要细节介绍这个模型的结构，因此这里我们只需要知道其输入是：预处理好的图片数据+grid_thw，其输出是：hidden_states+deepstack_feature_lists。其中最重要的就是输出的hidden_states，它含义是图片token的ebedding向量矩阵，在上面已经提到了其作用。</p>
<br/>
<br/>
<br/>
<br/>
<h5 id="language-model-部分分析">language_model 部分分析</h5>
<p>  对于语言模型部分来说，这个部分才是和我们前面训练的模型比较像的，下面我们先来看看其forward过程：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>
    self<span class="token punctuation">,</span>
    input_ids<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    attention_mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    position_ids<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    past_key_values<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Cache<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    inputs_embeds<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    use_cache<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    cache_position<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    <span class="token comment"># args for deepstack</span>
    visual_pos_masks<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    deepstack_visual_embeds<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">list</span><span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    <span class="token operator">**</span>kwargs<span class="token punctuation">:</span> Unpack<span class="token punctuation">[</span>FlashAttentionKwargs<span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> Union<span class="token punctuation">[</span><span class="token builtin">tuple</span><span class="token punctuation">,</span> BaseModelOutputWithPast<span class="token punctuation">]</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">r"""
    visual_pos_masks (`torch.Tensor` of shape `(batch_size, seqlen)`, *optional*):
        The mask of the visual positions.
    deepstack_visual_embeds (`list[torch.Tensor]`, *optional*):
        The deepstack visual embeddings. The shape is (num_layers, visual_seqlen, embed_dim).
        The feature is extracted from the different visual encoder layers, and fed to the decoder
        hidden states. It's from the paper DeepStack(https://arxiv.org/abs/2406.04334).
    """</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>input_ids <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">)</span> <span class="token operator">^</span> <span class="token punctuation">(</span>inputs_embeds <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"You must specify exactly one of input_ids or inputs_embeds"</span><span class="token punctuation">)</span>

    <span class="token comment"># torch.jit.trace() doesn't support cache objects in the output</span>
    <span class="token keyword">if</span> use_cache <span class="token keyword">and</span> past_key_values <span class="token keyword">is</span> <span class="token boolean">None</span> <span class="token keyword">and</span> <span class="token keyword">not</span> torch<span class="token punctuation">.</span>jit<span class="token punctuation">.</span>is_tracing<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        past_key_values <span class="token operator">=</span> DynamicCache<span class="token punctuation">(</span>config<span class="token operator">=</span>self<span class="token punctuation">.</span>config<span class="token punctuation">)</span>

    <span class="token keyword">if</span> inputs_embeds <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        inputs_embeds <span class="token operator">=</span> self<span class="token punctuation">.</span>embed_tokens<span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span>

    <span class="token keyword">if</span> cache_position <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        past_seen_tokens <span class="token operator">=</span> past_key_values<span class="token punctuation">.</span>get_seq_length<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> past_key_values <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">else</span> <span class="token number">0</span>
        cache_position <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>
            past_seen_tokens<span class="token punctuation">,</span> past_seen_tokens <span class="token operator">+</span> inputs_embeds<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span>inputs_embeds<span class="token punctuation">.</span>device
        <span class="token punctuation">)</span>

    <span class="token comment"># the hard coded `3` is for temporal, height and width.</span>
    <span class="token keyword">if</span> position_ids <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        position_ids <span class="token operator">=</span> cache_position<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> inputs_embeds<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token keyword">elif</span> position_ids<span class="token punctuation">.</span>ndim <span class="token operator">==</span> <span class="token number">2</span><span class="token punctuation">:</span>
        position_ids <span class="token operator">=</span> position_ids<span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> position_ids<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token keyword">if</span> position_ids<span class="token punctuation">.</span>ndim <span class="token operator">==</span> <span class="token number">3</span> <span class="token keyword">and</span> position_ids<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">4</span><span class="token punctuation">:</span>
        text_position_ids <span class="token operator">=</span> position_ids<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        position_ids <span class="token operator">=</span> position_ids<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        text_position_ids <span class="token operator">=</span> position_ids<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>

    attention_mask <span class="token operator">=</span> create_causal_mask<span class="token punctuation">(</span>
        config<span class="token operator">=</span>self<span class="token punctuation">.</span>config<span class="token punctuation">,</span>
        input_embeds<span class="token operator">=</span>inputs_embeds<span class="token punctuation">,</span>
        attention_mask<span class="token operator">=</span>attention_mask<span class="token punctuation">,</span>
        cache_position<span class="token operator">=</span>cache_position<span class="token punctuation">,</span>
        past_key_values<span class="token operator">=</span>past_key_values<span class="token punctuation">,</span>
        position_ids<span class="token operator">=</span>text_position_ids<span class="token punctuation">,</span>
    <span class="token punctuation">)</span>

    hidden_states <span class="token operator">=</span> inputs_embeds

    <span class="token comment"># create position embeddings to be shared across the decoder layers</span>
    position_embeddings <span class="token operator">=</span> self<span class="token punctuation">.</span>rotary_emb<span class="token punctuation">(</span>hidden_states<span class="token punctuation">,</span> position_ids<span class="token punctuation">)</span>

    <span class="token comment"># decoder layers</span>
    <span class="token keyword">for</span> layer_idx<span class="token punctuation">,</span> decoder_layer <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>layers<span class="token punctuation">)</span><span class="token punctuation">:</span>
        layer_outputs <span class="token operator">=</span> decoder_layer<span class="token punctuation">(</span>
            hidden_states<span class="token punctuation">,</span>
            attention_mask<span class="token operator">=</span>attention_mask<span class="token punctuation">,</span>
            position_ids<span class="token operator">=</span>text_position_ids<span class="token punctuation">,</span>
            past_key_values<span class="token operator">=</span>past_key_values<span class="token punctuation">,</span>
            cache_position<span class="token operator">=</span>cache_position<span class="token punctuation">,</span>
            position_embeddings<span class="token operator">=</span>position_embeddings<span class="token punctuation">,</span>
            <span class="token operator">**</span>kwargs<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
        hidden_states <span class="token operator">=</span> layer_outputs

        <span class="token comment"># add visual features to the hidden states of first several layers</span>
        <span class="token keyword">if</span> deepstack_visual_embeds <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">and</span> layer_idx <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>deepstack_visual_embeds<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            hidden_states <span class="token operator">=</span> self<span class="token punctuation">.</span>_deepstack_process<span class="token punctuation">(</span>
                hidden_states<span class="token punctuation">,</span>
                visual_pos_masks<span class="token punctuation">,</span>
                deepstack_visual_embeds<span class="token punctuation">[</span>layer_idx<span class="token punctuation">]</span><span class="token punctuation">,</span>
            <span class="token punctuation">)</span>

    hidden_states <span class="token operator">=</span> self<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>

    <span class="token keyword">return</span> BaseModelOutputWithPast<span class="token punctuation">(</span>
        last_hidden_state<span class="token operator">=</span>hidden_states<span class="token punctuation">,</span>
        past_key_values<span class="token operator">=</span>past_key_values<span class="token punctuation">,</span>
    <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>  我们看到了将position_ids，attention_mask，inputs_embeds，past_key_values传入推理过程后，得到了两个重要的内容，一个logits，一个past_key_values，下面重点介绍一下这两个是什么：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>logits 输出的是一次推理后，词表大小的一个概率矩阵，然后根据我们的采样相关参数（例如我们常见的：Temperature/Top P/Frequency Penalty等就是在这一阶段生效），选择对应的token_id，然后转换为文字。</p>
</li>
<li class="lvl-2">
<p>past_key_values 保存的是每一层decoder layer的注意力机制里面的K/V内容，也就是我们常见的KV Cache一词的存在的地方。</p>
</li>
</ul>
<p>  最后我们来看看现在常见的KV cache（缓存命中、缓存未命中）到底意味着什么?我们举一个简单直观的例子：我们保存了“你好”的KV cache，那我们再一次推理“你好世界。”，那么我们可以直接使用“你好”的KV cache，不用重复计算前面部分，可以直接计算新的部分，加快推理速度、减少了计算资源使用。</p>
<br/>
<br/>
<br/>
<br/>
<h3 id="后记">后记</h3>
<hr>
<p>  本文基于Qwen3-VL-2B-Instruct，回顾了之前的一些知识，从这里我们可以看到，当前大模型里面用到的好多知识点，其实都来自于以前的某个地方。</p>
<p>  本系列到此，完结散花。</p>
<h3 id="参考文献">参考文献</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/QwenLM/Qwen3-VL">https://github.com/QwenLM/Qwen3-VL</a></p>
</li>
</ul>
<br/>
<br/>
<div style="margin:50px auto;">
</div>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <hr/>
        <center><font color = #91e0b0 size = 5>打赏、订阅、收藏、丢香蕉、硬币，请关注公众号（攻城狮的搬砖之路）</font></center>
    </div>
</div>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/qrcode_for_wx_official_account.jpg" alt="qrc_img"/></center>
    </div>
</div>
<!-- ![alt 公众号图片](https://flyinskyin2013.github.io/ImageBed0/blogs/qrcode_for_wx_official_account.jpg "公众号图片") -->
<p><font color="red" size="7">PS: 请尊重原创，不喜勿喷。</font><br/><br>
<font color="red" size="7">PS: 要转载请注明出处，本人版权所有。</font><br/><br>
<font color="red" size="7">PS: 有问题请留言，看到后我会第一时间回复。</font><br/></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
              <a href="/tags/NLP/" rel="tag"># NLP</a>
              <a href="/tags/LLM/" rel="tag"># LLM</a>
              <a href="/tags/LM/" rel="tag"># LM</a>
              <a href="/tags/attention/" rel="tag"># attention</a>
              <a href="/tags/transformer/" rel="tag"># transformer</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/11/16/blog_idx_144/" rel="prev" title="大模型基础补全计划(七)---Transformer(多头注意力、自注意力、位置编码)及实例与测试">
                  <i class="fa fa-angle-left"></i> 大模型基础补全计划(七)---Transformer(多头注意力、自注意力、位置编码)及实例与测试
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/12/28/blog_idx_146/" rel="next" title="一次酣畅淋漓的问题排查（c++标准库异常实现原理）">
                  一次酣畅淋漓的问题排查（c++标准库异常实现原理） <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2026</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Sky</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">286k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">17:19</span>
  </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"position":"right","width":150,"height":250,"hOffset":50,"vOffset":5},"mobile":{"show":false,"scale":0.5},"react":{"opacityDefault":0.7,"opacityOnHover":0.8},"log":false});</script></body>
</html>
