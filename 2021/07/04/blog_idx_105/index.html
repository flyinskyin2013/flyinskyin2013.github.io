<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/7.0.0/css/all.min.css" integrity="sha256-VHqXKFhhMxcpubYf9xiWdCiojEbY9NexQ4jh8AxbvcM=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"e-x.top","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.27.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":false,"codeblock":{"theme":{"light":"default","dark":"stackoverflow-dark"},"prism":{"light":"prism","dark":"prism-dark"},"copy_button":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"language":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js" defer></script>

    <meta name="description" content="PS：要转载请注明出处，本人版权所有。 PS: 这个只是基于《我自己》的理解，如果和你的原则及想法相冲突，请谅解，勿喷。  环境说明   Windows 10   VSCode   Python 3.8.10   Pytorch 1.8.1   Cuda 10.2   前言    从我2017毕业到现在为止，我的工作一直都是AI在边缘端部署落地等相关内容。所以我的工作基本都集中在嵌入式+Lin">
<meta property="og:type" content="article">
<meta property="og:title" content="DL基础补全计划(一)---线性回归及示例（Pytorch，平方损失）">
<meta property="og:url" content="https://e-x.top/2021/07/04/blog_idx_105/index.html">
<meta property="og:site_name" content="Sky&#39;s Blogs">
<meta property="og:description" content="PS：要转载请注明出处，本人版权所有。 PS: 这个只是基于《我自己》的理解，如果和你的原则及想法相冲突，请谅解，勿喷。  环境说明   Windows 10   VSCode   Python 3.8.10   Pytorch 1.8.1   Cuda 10.2   前言    从我2017毕业到现在为止，我的工作一直都是AI在边缘端部署落地等相关内容。所以我的工作基本都集中在嵌入式+Lin">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://flyinskyin2013.github.io/ImageBed0/blogs/blog_idx_105/feature_label.png">
<meta property="og:image" content="https://flyinskyin2013.github.io/ImageBed0/blogs/blog_idx_105/loss_surface.png">
<meta property="og:image" content="https://flyinskyin2013.github.io/ImageBed0/blogs/blog_idx_105/train_log.png">
<meta property="og:image" content="https://flyinskyin2013.github.io/ImageBed0/blogs/qrcode_for_wx_official_account.jpg">
<meta property="article:published_time" content="2021-07-04T08:15:16.000Z">
<meta property="article:modified_time" content="2024-05-18T11:06:02.646Z">
<meta property="article:author" content="Sky">
<meta property="article:tag" content="平方损失">
<meta property="article:tag" content="Pytorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://flyinskyin2013.github.io/ImageBed0/blogs/blog_idx_105/feature_label.png">


<link rel="canonical" href="https://e-x.top/2021/07/04/blog_idx_105/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://e-x.top/2021/07/04/blog_idx_105/","path":"2021/07/04/blog_idx_105/","title":"DL基础补全计划(一)---线性回归及示例（Pytorch，平方损失）"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>DL基础补全计划(一)---线性回归及示例（Pytorch，平方损失） | Sky's Blogs</title>
  








  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
<script src="/js/utils.js" defer></script><script src="/js/motion.js" defer></script><script src="/js/sidebar.js" defer></script><script src="/js/next-boot.js" defer></script>

  






  




<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8677552300382028"
     crossorigin="anonymous"></script>
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Sky's Blogs</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">A normal star</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>







</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E7%8E%AF%E5%A2%83%E8%AF%B4%E6%98%8E"><span class="nav-number">1.</span> <span class="nav-text">环境说明</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-number"></span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%9E%E5%BD%92%E6%A6%82%E5%BF%B5"><span class="nav-number"></span> <span class="nav-text">回归概念</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-number"></span> <span class="nav-text">线性回归</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-number"></span> <span class="nav-text">非线性回归</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8Ey-aX12-bX12-cX2-dX2-e%E7%9A%84%E5%9B%9E%E5%BD%92Pytorch%E5%AE%9E%E4%BE%8B"><span class="nav-number"></span> <span class="nav-text">基于y&#x3D;aX12+bX12+cX2+dX2+e的回归Pytorch实例</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%B8%A6%E5%99%AA%E5%A3%B0%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E5%8F%8A%E5%A4%84%E7%90%86"><span class="nav-number"></span> <span class="nav-text">带噪声数据采集及处理</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E6%90%AD%E5%BB%BA%E5%8F%8A%E8%AE%AD%E7%BB%83"><span class="nav-number"></span> <span class="nav-text">模型搭建及训练</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%8E%E8%AE%B0"><span class="nav-number"></span> <span class="nav-text">后记</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="nav-number"></span> <span class="nav-text">参考文献</span></a></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Sky</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">139</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">92</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">222</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://e-x.top/2021/07/04/blog_idx_105/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Sky">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sky's Blogs">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="DL基础补全计划(一)---线性回归及示例（Pytorch，平方损失） | Sky's Blogs">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          DL基础补全计划(一)---线性回归及示例（Pytorch，平方损失）
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-07-04 16:15:16" itemprop="dateCreated datePublished" datetime="2021-07-04T16:15:16+08:00">2021-07-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-05-18 19:06:02" itemprop="dateModified" datetime="2024-05-18T19:06:02+08:00">2024-05-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/DL/" itemprop="url" rel="index"><span itemprop="name">DL</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/DL/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" itemprop="url" rel="index"><span itemprop="name">计算机视觉</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.4k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>16 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><script src="\assets\js\APlayer.min.js"> </script><!--
 * @Description: 
 * @Author: Sky
 * @Date: 2020-08-24 16:37:34
 * @LastEditors: Sky
 * @LastEditTime: 2021-06-29 15:04:05
 * @Github: https://github.com/flyinskyin2013/
-->
<p><font color="red" size="7">PS：要转载请注明出处，本人版权所有。</font><br/><br>
<font color="red" size="7">PS: 这个只是基于《我自己》的理解，</font><br/><font color="red" size="7">如果和你的原则及想法相冲突，请谅解，勿喷。</font><br/></p>
<!-- ###### 前置说明
&emsp;&emsp;本文作为本人csdn blog的主站的备份。（BlogID=105） 
&emsp;&emsp;本文发布于 2021-07-04 16:15:16    （BlogID=105）       
-->
<h6 id="环境说明">环境说明</h6>
<ul class="lvl-0">
<li class="lvl-2">
<p>Windows 10</p>
</li>
<li class="lvl-2">
<p>VSCode</p>
</li>
<li class="lvl-2">
<p>Python 3.8.10</p>
</li>
<li class="lvl-2">
<p>Pytorch 1.8.1</p>
</li>
<li class="lvl-2">
<p>Cuda 10.2</p>
</li>
</ul>
<h3 id="前言">前言</h3>
<hr>
<p>  从我2017毕业到现在为止，我的工作一直都是AI在边缘端部署落地等相关内容。所以我的工作基本都集中在嵌入式+Linux+DL三者之内的范围，由于个人兴趣和一些工作安排，就会去做一些模型移植的工作，所以我会经常接触模型基本结构，前处理、后处理等等基本的知识，但是其实我很少去接触模型怎么来的这个问题。虽然以前也硬啃过Lenet5和BP算法，也按照别人弄好的脚本训练过一些简单的模型，但是从来没有认真仔细的看过这些脚本，这些脚本为什么这样写。</p>
<p>  在2019年下半年，随着我移植模型的工作深入，接触的各种硬件平台越来越多，经常遇见一些层在此平台无法移植，需要拆出来特殊处理。这让我产生了为啥这些层在这些特定平台不能够移植的疑问？为啥替换为别的层就能正常工作？为啥此平台不提供这个层？于是我去请教我们的算法小伙伴们，他们建议我如果要解决这个问题，建议我学习一下DL的基本知识，至少要简单了解从数据采集及处理、模型搭建及训练、模型部署等知识，其中模型部署可能是我最了解的内容了。利用一些闲暇时间和工作中的一些机会，我对以上需要了解的知识有了一个大概的认知。随着了解的深入，可能我也大概知道我比较缺一些基础知识。经过小伙伴的推荐和自己的搜索，我选择了《动⼿学深度学习.pdf》作为我的基础补全资料。</p>
<p>  本文是以‘补全资料’的Chapter3中线性规划为基础来编写的，主要是对‘补全资料’之前的基础知识的一个简单汇总，包含了深度学习中一些基本的知识点，同时会对这些基本知识点进行一些解释。</p>
<p>  由于我也是一个初学者，文中的解释是基于我的知识水平结构做的‘特色适配’，如果解释有误，请及时提出，我这里及时更正。写本文的原因也是记录我的学习历程，算是我的学习笔记。</p>
<br/>
<br/>
<br/>
<br/>
<h3 id="回归概念">回归概念</h3>
<hr>
<p>  回归是一种建模方法，得到的模型表示了自变量和因变量的关系。因此回归还可以解释为一种事与事之间的联系。对我们来说最常见的例子就是我们学习过的函数。例如函数Y=aX+b，这里的Y=aX+b就是我们模型， X代表自变量，Y代表因变量。</p>
<p>  这里顺便多说一句，深度学习是机器学习的子集。建模方法很多，回归只是其中的一种。</p>
<br/>
<br/>
<h5 id="线性回归">线性回归</h5>
<p>  线性回归就是自变量和因变量是线性关系，感觉跟废话一样，换个方式表达就是自变量是一次。例如：Y=aX+b, Y=aX1+bX2+c, 这里的X、X1、X2都是一次的，不是二次或者更高的。</p>
<br/>
<br/>
<h5 id="非线性回归">非线性回归</h5>
<p>  非线性回归就是自变量和因变量不是是线性关系，同样感觉跟废话一样，换个方式表达就是自变量是二次及以上的。这里和线性回归对比一下就行。</p>
<br/>
<br/>
<br/>
<br/>
<h3 id="基于y-aX12-bX12-cX2-dX2-e的回归Pytorch实例">基于y=aX1<sup>2+bX1</sup>2+cX2+dX2+e的回归Pytorch实例</h3>
<hr>
<p>  此实例是《动⼿学深度学习.pdf》中线性回归的从零实现的变种。从零实现，可以了解到许多的基本知识。<br>
  此小节基本按照数据采集及处理、模型搭建及训练和模型部署来描述。</p>
<br/>
<br/>
<h5 id="带噪声数据采集及处理">带噪声数据采集及处理</h5>
<p>  我们知道，在我们准备数据时，肯定由于各种各样的原因，会有各种干扰，导致我们根据实际场景采集到的数据，其实不是精准的，但是这并不影响我们建模，因为我们的模型是尽量去拟合真实场景的情况。</p>
<p>  生成我们的实际模型的噪声数据，也就是我们常见的数据集的说法。如下是代码：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">def</span> <span class="token function">synthetic_data</span><span class="token punctuation">(</span>w1<span class="token punctuation">,</span> w2<span class="token punctuation">,</span> b<span class="token punctuation">,</span> num_examples<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""⽣成y = X1^2w1 + X2w2 + b + 噪声。"""</span>
    <span class="token comment"># 根据正太分布，随机生成我们的X1和X2</span>
    <span class="token comment"># X1和X2都是(1000, 2)的矩阵</span>
    X1 <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>num_examples<span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>w1<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    X2 <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>num_examples<span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>w2<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token comment"># 基于X1,X2,true_w1, true_w2, true_b， 通过向量内积、广播等方法计算模型的真实结果</span>
    y <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>X1<span class="token operator">**</span><span class="token number">2</span><span class="token punctuation">,</span> w1<span class="token punctuation">)</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>X2<span class="token punctuation">,</span> w2<span class="token punctuation">)</span> <span class="token operator">+</span> b

    <span class="token comment"># 通过随机噪声加上真实结果，生成我们的数据集。</span>
    <span class="token comment"># y是(1000, 1)的矩阵</span>
    y <span class="token operator">+=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">,</span> y<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
    
    <span class="token keyword">return</span> X1<span class="token punctuation">,</span> X2<span class="token punctuation">,</span> y<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    
true_w1 <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">5.7</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">3.4</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
true_w2 <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4.8</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">3.4</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
true_b <span class="token operator">=</span> <span class="token number">4.2</span>

<span class="token comment"># 这里我们得到了我们的数据集，包含了特征和标签</span>
features1<span class="token punctuation">,</span> features2<span class="token punctuation">,</span> labels <span class="token operator">=</span> synthetic_data<span class="token punctuation">(</span>true_w1<span class="token punctuation">,</span> true_w2<span class="token punctuation">,</span> true_b<span class="token punctuation">,</span> <span class="token number">1000</span><span class="token punctuation">)</span>

<span class="token comment"># 因为我们知道我们的模型是y=aX1^2+bX1^2+cX2+dX2+e，于是我们知道a的数据分布是类似y=ax^2+b的这种形状。于是我们知道c的数据分布是类似y=ax+b的这种形状。</span>
<span class="token comment"># 我们可以通过如下代码验证一下</span>
plt<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>features1<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> labels<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> c<span class="token operator">=</span><span class="token string">'r'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>features2<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> labels<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> c<span class="token operator">=</span><span class="token string">'b'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/blog_idx_105/feature_label.png" alt="rep_img"/></center>
    </div>
</div>   
其实从图中可以看到，红色的是类似y=ax^2+b的这种形状，蓝色是类似y=ax+b这种形状，但是他们都不是在一条线，说明我们的设置的噪声项是有效的。
<p>  同时这里我们还要准备一个函数用来随机抽取特征和标签，一批一批的进行训练。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">data_iter</span><span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> features1<span class="token punctuation">,</span> features2<span class="token punctuation">,</span> labels<span class="token punctuation">)</span><span class="token punctuation">:</span>
    num_examples <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>features1<span class="token punctuation">)</span>
    indices <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span>num_examples<span class="token punctuation">)</span><span class="token punctuation">)</span>
    np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>indices<span class="token punctuation">)</span> <span class="token comment"># 样本的读取顺序是随机的</span>
    
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> num_examples<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
        j <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>indices<span class="token punctuation">[</span>i<span class="token punctuation">:</span> <span class="token builtin">min</span><span class="token punctuation">(</span>i <span class="token operator">+</span> batch_size<span class="token punctuation">,</span> num_examples<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token comment"># print(features1.take(j, axis=0).shape)</span>
        <span class="token keyword">yield</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>features1<span class="token punctuation">.</span>take<span class="token punctuation">(</span>j<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>features2<span class="token punctuation">.</span>take<span class="token punctuation">(</span>j<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>labels<span class="token punctuation">.</span>take<span class="token punctuation">(</span>j<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># take函数根据索引返回对应元素</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<br/>
<br/>
<h5 id="模型搭建及训练">模型搭建及训练</h5>
<p>  对于我们这个实例来说，模型就是一个二元二次函数，此外这里的模型也叫作目标函数。所以，下面我们用torch来实现它就行。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">our_func</span><span class="token punctuation">(</span>X1<span class="token punctuation">,</span> X2<span class="token punctuation">,</span> W1<span class="token punctuation">,</span> W2<span class="token punctuation">,</span> B<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># print(X1.shape) (100, 2)</span>
    <span class="token comment"># print(W1.shape) (2, 1)</span>
    net_out <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>X1<span class="token operator">**</span><span class="token number">2</span><span class="token punctuation">,</span> W1<span class="token punctuation">)</span> <span class="token operator">+</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>X2<span class="token punctuation">,</span> W2<span class="token punctuation">)</span> <span class="token operator">+</span> B
    <span class="token comment"># print(net_out.shape) (100, 1)</span>
    <span class="token keyword">return</span> net_out<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>注意哟，这里的X1,X2,W1,W2,B都是torch的tensor格式。</p>
<p>  由数据采集及处理可知，在我们设定的真实true_w1,true_w2和true_b的情况下，我们得到了许许多多的X1,X2和y。我们要做的事情是求出W1、W2和b，这里看起是不是很矛盾？我们已知了true_w1,true_w2,true_b然后去求w1,w2,b。这里其实是一个错觉，由于在实际情况中，我们可能会得到许许多多的X1,X2和y，这些数据不是我们模拟生成的，而是某种关系实际产生的数据，只是这些数据被我们收集到了。在实际情况下，而且我们仅仅只能够得到X1,X2和y，我们通过观察X1,X2和y的关系，发现他们有一元二次和一元一次关系，所以我们建立的模型为y=aX1<sup>2+bX1</sup>2+cX2+dX2+e，这个过程称为建模。</p>
<p>  通过上面的说明，我们知道这个模型我们要求a,b,c,d,e这些参数的值，我们求这些参数的过程叫做训练。对于这个实例来说，这个过程也叫作求解方程，这里其实我们可以通过解方程的方法把这5个参数解出来，但是在实际情况中，我们建立的模型可能参数较多，可能手动解不出来，于是我们要通过训练的方式，去拟合这些参数。所以这里一个重要的问题就是怎么拟合这些参数？</p>
<p>  如果学习过《数值分析》这门课程的话，其实就比较好理解了，如果要拟合这些参数，我们有许多的方法可以使用，但是基本分为两类，一类是针对误差进行分析，一类是针对模型进行分析。那么在深度学习中，我们一般是对误差进行分析，也就是我们常说的梯度下降法，我们需要随机生成这些参数初始值(w1,w2,b)，然后根据我们得到的X1,X2和y，通过梯度下降方法可以得到新的w1’,w2’,b’，且w1’,w2’,b’更加接近true_w1,true_w2和true_b。这就是一种优化过程，经过多次优化，我们就有可能求出跟接近与true_w1,true_w2和true_b的值。</p>
<p>  上面啰嗦了一大堆之后，我这里正式引入损失函数这个概念，然后我们根据损失函数去优化我们的w1,w2,b。我们定义这个实例的损失函数如下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">loss_func</span><span class="token punctuation">(</span>y_train<span class="token punctuation">,</span> y_label<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># print(y_train.shape)</span>
    <span class="token comment"># print(y_label.shape)</span>
    <span class="token keyword">return</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>y_train <span class="token operator">-</span> y_label<span class="token punctuation">)</span><span class="token operator">**</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">2</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>  这里我们y_train就是我们得到的训练值，y_label就是我们采集到数值，这里的损失函数就是描述训练值和标签的差多少，那么我们只需要让这个损失函数的值越来越小就行，那么可能问题就变成了求损失函数的极小值问题，我们在这里还是用梯度下降的方法来求损失函数的极小值。</p>
<p>  这里要回忆起来一个概念，梯度是一个函数在这个方向增长最快的方向。我们求损失函数的极小值的话，就减去梯度就行了。</p>
<p>  这里还要说一句，损失函数有很多(L1,MSE,交叉熵等等)，大家以后可以自己选一个合适的即可，这里的合适需要大家去学习每种损失函数的适用场景。这里的平方损失的合理性我个人认为有两种：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>1 直观法，平方损失函数描述的是在数据集上，训练值和真实值的误差趋势，我要想得到的参数最准确，就要求误差最小，误差最小就要求我去求解损失函数的极小值，这是我所了解的数学知识的直观反映。 （直觉大法）</p>
</li>
<li class="lvl-2">
<p>2 数学证明如下：我们生成数据或者采样数据的时候，他们的误差服从正态分布（ $P(x) = 1/\sqrt{2\pi\sigma<sup>2}*exp((-1/2\sigma</sup>2)(x-\mu)^2)$ ），于是我们根据条件概率得到特定feature得到特定y的概率为: $P(y|X) = 1/\sqrt{2\pi\sigma<sup>2}*exp((-1/2\sigma</sup>2)(y-w1X1<sup>2-w2X2-b)</sup>2)$,根据最大似然估计$P(y|X) = \prod\limits_{i=0}<sup>{n-1}P(y</sup>{i}|X^{i})$，此时最大似然估计最大，w1,w2,b就是最佳的参数，但是乘积函数的最大值不好求，我们用对数转换一下，变为各项求和，只需要保证含义一致就行。由于一般的优化一般是说最小化，所以我们要取负的对数和$-\log(P(y|X))$，此时我们把最大似然估计函数转换为对数形式：$-\log(P(y|X)) = \sum\limits_{i=0}<sup>{n-1}1/2\log(2\pi\sigma</sup>2)+1/2\sigma<sup>2(y-w1X1</sup>2-w2X2-b)^2)$,我们可以看到要求此函数的最小值，其实就是后半部分的平方是最小值就行，因为其余的都是常数项，而后面部分恰好就是我们的平方损失函数。（Copy书上大法）</p>
</li>
</ul>
<p>  这里的梯度下降法就是(w1,w2,b)-lr*(w1,w2,b).grad/batch_size,代码如下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">sgd</span><span class="token punctuation">(</span>params<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""⼩批量随机梯度下降。"""</span>
    <span class="token keyword">for</span> param <span class="token keyword">in</span> params<span class="token punctuation">:</span>
        <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            param<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> param <span class="token operator">-</span> lr <span class="token operator">*</span> param<span class="token punctuation">.</span>grad <span class="token operator">/</span> batch_size<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>  这里我画出我们的损失函数的三维图像,我们把损失函数简化为Z=aX^2+bY+c的形式。其图像如下图：</p>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/blog_idx_105/loss_surface.png" alt="rep_img"/></center>
    </div>
</div>   
<p>我们可以看到这个曲面有很多极小值，当我们随机初始化a,b,c的时候，我们会根据X和Y得到部分损失点，这时我们求出a,b,c的梯度，当我们对a,b,c减去偏导数时，就会更快的靠近损失函数的谷底，我们的当我们的损失函数到极小值时，我们就认为此时的a,b,c是我们要找的参数。这就是我们的梯度下降法的意义所在。</p>
<p>  下面就直接写训练代码就行。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">w1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
w2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token comment"># print(w1.grad)</span>
<span class="token comment"># print(w1.grad_fn)</span>
<span class="token comment"># </span>
lr <span class="token operator">=</span> <span class="token number">0.001</span>
num_epochs <span class="token operator">=</span> <span class="token number">10000</span>
net <span class="token operator">=</span> our_func
loss <span class="token operator">=</span> loss_func

batch_size <span class="token operator">=</span> <span class="token number">200</span>

<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> X1<span class="token punctuation">,</span> X2<span class="token punctuation">,</span> y <span class="token keyword">in</span> data_iter<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> features1<span class="token punctuation">,</span> features2<span class="token punctuation">,</span> labels<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># print(X1.shape) (100, 2)</span>
        <span class="token comment"># print(y.shape) (100, 1)</span>
        l <span class="token operator">=</span> loss<span class="token punctuation">(</span>net<span class="token punctuation">(</span>X1<span class="token punctuation">,</span> X2<span class="token punctuation">,</span> w1<span class="token punctuation">,</span> w2<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># `X`和`y`的⼩批量损失</span>
        <span class="token comment"># print(f'epoch &#123;epoch + 1&#125;, before sdg loss &#123;float(l.mean()):f&#125;')</span>
        <span class="token comment"># 计算l关于[`w`, `b`]的梯度</span>
        <span class="token comment"># print(l.shape)</span>
        
        <span class="token comment"># l.backward() default call, loss.backward(torch.Tensor(1.0))</span>
        w1<span class="token punctuation">.</span>grad <span class="token operator">=</span> <span class="token boolean">None</span>
        w2<span class="token punctuation">.</span>grad <span class="token operator">=</span> <span class="token boolean">None</span>
        b<span class="token punctuation">.</span>grad <span class="token operator">=</span> <span class="token boolean">None</span>

        l<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span>y<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

        sgd<span class="token punctuation">(</span><span class="token punctuation">[</span>w1<span class="token punctuation">,</span> w2<span class="token punctuation">,</span> b<span class="token punctuation">]</span><span class="token punctuation">,</span> lr<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span> <span class="token comment"># 使⽤参数的梯度更新参数</span>
        
        l1 <span class="token operator">=</span> loss<span class="token punctuation">(</span>net<span class="token punctuation">(</span>X1<span class="token punctuation">,</span> X2<span class="token punctuation">,</span> w1<span class="token punctuation">,</span> w2<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># `X`和`y`的⼩批量损失</span>
        <span class="token comment"># print(f'epoch &#123;epoch + 1&#125;, after sdg loss &#123;float(l1.mean()):f&#125;')</span>
    
    train_l <span class="token operator">=</span> loss<span class="token punctuation">(</span>net<span class="token punctuation">(</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>features1<span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>features2<span class="token punctuation">)</span><span class="token punctuation">,</span> w1<span class="token punctuation">,</span> w2<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>labels<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token comment"># print(train_l.sum())</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>epoch <span class="token operator">%</span> <span class="token number">1000</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'epoch </span><span class="token interpolation"><span class="token punctuation">&#123;</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">&#125;</span></span><span class="token string">, loss </span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">float</span><span class="token punctuation">(</span>train_l<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">f</span><span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'train_w1 '</span><span class="token punctuation">,</span>w1<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'train_w2 '</span><span class="token punctuation">,</span>w2<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'train_b '</span><span class="token punctuation">,</span>b<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>  这里做的事情就是首先设定训了次数，学习率，批次数量参数，然后随机生成了w1,w2,b，然后通过data_iter取一批数据，算出loss，清空w1,w2,b的梯度，对loss求w1,w2,b的偏导数，调用sdg求新的w1,w2,b。最后计算新参数在整个数据集上的损失。</p>
<p>  这里尤其需要注意的是在多次迭代过程中，一定要清空w1,w2,b的梯度，因为它的梯度是累加的，不会覆盖。这里用的Pytorch的自动求导模块，其实底层就是调用的bp算法，利用了链式法则，反向从loss开始，能够算出w1,w2,b的偏导数。</p>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/blog_idx_105/train_log.png" alt="rep_img"/></center>
    </div>
</div>   
这里我们看到最终的平均误差到了一定值后就下降不了，这和我们的数据有关系，我们只需要关心这个误差我们能够接受吗？能接受，我们就训练得到了成功的模型，如果不能接受，我们就改参数重新来，这是一个多次试验尝试的过程。
<p>  我们从上文可知，true_w1 = np.array([5.7, -3.4])，true_w2 = np.array([4.8, -3.4])，true_b = 4.2。我们训练出来的值是w1=(5.7012, -3.3955),w2=(4.8042, -3.4071),b=4.2000。可以看到其实训练得到的值还是非常接近的，但是这个任务其实太简单了。</p>
<br/>
<br/>
<br/>
<br/>
<h3 id="后记">后记</h3>
<hr>
<p>  这里主要用到了许多基本的概念，一个是数据集的收集和处理，模型搭建，模型训练，优化方法、损失函数等等。至少通过本文，我们知道了得到一个模型的基本过程，其实普遍性的深度学习就是使用新的损失函数、搭建新的模型、使用新的优化方法等等。</p>
<p>  从文中特例我们可以发现，我们只利用了Pytorch自带的自动求导模块，其他的一些常见的深度学习概念都是我们手动实现了，其实这些好多内容都是Pytorch这些框架封装好的，我们没有必要自己手写，但是如果是初次学习的话，建议手动来写，这样认知更加深刻。</p>
<h3 id="参考文献">参考文献</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/d2l-ai/d2l-zh/releases">https://github.com/d2l-ai/d2l-zh/releases</a> (V1.0.0)</p>
</li>
<li class="lvl-2">
<p><a target="_blank" rel="noopener" href="https://github.com/d2l-ai/d2l-zh/releases">https://github.com/d2l-ai/d2l-zh/releases</a> (V2.0.0 alpha1)</p>
</li>
</ul>
<br/>
<br/>
<div style="margin:50px auto;">
</div>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <hr/>
        <center><font color = #91e0b0 size = 5>打赏、订阅、收藏、丢香蕉、硬币，请关注公众号（攻城狮的搬砖之路）</font></center>
    </div>
</div>
<div style="text-align:center">
    <div style="margin:0 auto;">
        <center><img src="https://flyinskyin2013.github.io/ImageBed0/blogs/qrcode_for_wx_official_account.jpg" alt="qrc_img"/></center>
    </div>
</div>
<!-- ![alt 公众号图片](https://flyinskyin2013.github.io/ImageBed0/blogs/qrcode_for_wx_official_account.jpg "公众号图片") -->
<p><font color="red" size="7">PS: 请尊重原创，不喜勿喷。</font><br/><br>
<font color="red" size="7">PS: 要转载请注明出处，本人版权所有。</font><br/><br>
<font color="red" size="7">PS: 有问题请留言，看到后我会第一时间回复。</font><br/></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E5%B9%B3%E6%96%B9%E6%8D%9F%E5%A4%B1/" rel="tag"># 平方损失</a>
              <a href="/tags/Pytorch/" rel="tag"># Pytorch</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/04/24/blog_idx_104/" rel="prev" title="Mediapipe 在RK3399PRO上的初探（二）(自定义Calculator)">
                  <i class="fa fa-angle-left"></i> Mediapipe 在RK3399PRO上的初探（二）(自定义Calculator)
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/07/11/blog_idx_106/" rel="next" title="DL基础补全计划(二)---Softmax回归及示例（Pytorch，交叉熵损失）">
                  DL基础补全计划(二)---Softmax回归及示例（Pytorch，交叉熵损失） <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2026</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Sky</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">286k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">17:19</span>
  </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"position":"right","width":150,"height":250,"hOffset":50,"vOffset":5},"mobile":{"show":false,"scale":0.5},"react":{"opacityDefault":0.7,"opacityOnHover":0.8},"log":false});</script></body>
</html>
